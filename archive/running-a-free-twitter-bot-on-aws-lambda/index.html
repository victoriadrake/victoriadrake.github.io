<!doctype html><html><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><link href=/images/fav/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link href=/images/fav/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/images/fav/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/images/fav/site.webmanifest rel=manifest><title>Running a free Twitter bot on AWS Lambda | victoria.dev</title>
<link href=/css/styles.css rel=stylesheet><link href=/css/custom.css rel=stylesheet><link href=/css/tags.css rel=stylesheet></head><body><header><div class="logo h-card"><a class=u-url href=https://victoria.dev/><img alt="Site Logo" class="site-logo u-photo" src=/images/vd.svg></a></div><nav class=menu><ul><li><a href=/>hello</a></li><li><a href=/about/>about</a></li><li><a href=/posts/>blog</a></li></ul></nav></header><div class=archived-banner><p>This post has been archived. Information may not be up to date.</p></div><section class="hero container" id=heroSection onload=adjustTextColor(this)><div><h1 class=p-name id=textOverlay>Running a free Twitter bot on AWS Lambda</h1></div></section><main class="post h-entry container row"><div class=post><div class="post-data row"><time class=dt-published datetime="2018-03-05 10:29:15 -0500 -0500"><a class=u-url href=https://victoria.dev/archive/running-a-free-twitter-bot-on-aws-lambda/>Mar 5, 2018</a>
</time><span rel=author class="p-author h-card"><img class="u-photo hidden" src=/images/profile.jpg>
<span class="p-name hidden" rel=me>Victoria Drake</span></span></div><div class=e-content><p>If you read <a href=/blog/about-time/>About time</a>, you&rsquo;ll know that I&rsquo;m a big believer in spending time now on building things that save time in the future. To this end I built a simple Twitter bot in Go that would occasionally post links to my articles and keep my account interesting even when I&rsquo;m too busy to use it. The tweets help drive traffic to my sites, and I don&rsquo;t have to lift a finger.</p><p>I ran the bot on an Amazon EC2 instance for about a month. My AWS usage has historically been pretty inexpensive (less than the price of a coffee in most of North America), so I was surprised when the little instance I was using racked up a bill 90% bigger than the month before. I don&rsquo;t think AWS is expensive, to be clear, but still&mldr; I&rsquo;m cheap. I want my Twitter bot, and I want it for less.</p><p>I&rsquo;d been meaning to explore AWS Lamda, and figured this was a good opportunity. Unlike an EC2 instance that is constantly running (and charging you for it), Lambda charges you per request and according to the duration of time your function takes to run. There&rsquo;s a free tier, too, and the first 1 million requests, plus a certain amount of compute time, are free. Roughly translated to running a Twitter bot that posts for you, say, twice a day, your monthly cost for using Lambda would total&mldr; carry the one&mldr; nothing. I&rsquo;ve been running my Lambda function for a couple weeks now, completely free.</p><p>When recently it came to me to take the reigns of the <a href=https://twitter.com/freeCodeCampTO>@freeCodeCampTO</a> Twitter, I decided to employ a similar strategy, and also use this opportunity to document the process for you, dear reader.</p><p>So if you&rsquo;re currently using a full-time running instance for a task that could be served by a cron job, this is the article for you. I&rsquo;ll cover how to write your function for Lambda, how to get it set up to run automatically, and as a sweet little bonus, a handy bash script that updates your function from the command line whenever you need to make a change. Let&rsquo;s do it!</p><h2 id=is-lambda-right-for-you>Is Lambda right for you</h2><p>When I wrote the code for my Twitter bot in Go, I intended to have it run on an AWS instance and borrowed heavily from <a href=https://github.com/campoy/justforfunc/tree/master/14-twitterbot>Francesc&rsquo;s awesome Just for Func episode</a>. Some time later I modified it to randomly choose an article from my RSS feeds and tweet the link, twice a day. I wanted to do something similar for the @freeCodeCampTO bot, and have it tweet an inspiring quote about programming every morning.</p><p>This is a good use case for Lambda because:</p><ul><li>The program should execute once</li><li>It runs on a regular schedule, using time as a trigger</li><li>It doesn&rsquo;t need to run constantly</li></ul><p>The important thing to keep in mind is that Lambda runs a function once in response to an event that you define. The most widely applicable trigger is a simple cron expression, but there are many other trigger events you can hook up. You can get an overview <a href=https://aws.amazon.com/lambda/>here</a>.</p><h2 id=write-a-lambda-function>Write a Lambda function</h2><p>I found this really straightforward to do in Go. First, grab the <a href=https://github.com/aws/aws-lambda-go>aws-lambda-go</a> library:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>go get github.com/aws/aws-lambda-go/lambda
</span></span></code></pre></div><p>Then make this your <code>func main()</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span> <span style=color:#a6e22e>lambda</span>.<span style=color:#a6e22e>Start</span>(<span style=color:#a6e22e>tweetFeed</span>)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Where <code>tweetFeed</code> is the name of the function that makes everything happen. While I won&rsquo;t go into writing the whole Twitter bot here, you can view my code <a href=https://gist.github.com/victoriadrake/7859dab68df87e28f40d6715d08383c7>on GitHub</a>.</p><h2 id=setting-up-aws-lambda>Setting up AWS Lambda</h2><p>I&rsquo;m assuming you already have an AWS account. If not, first things first here: <a href=https://aws.amazon.com/free>https://aws.amazon.com/free</a></p><h3 id=1-create-your-function>1. Create your function</h3><p>Find AWS Lambda in the list of services, then look for this shiny button:</p><p><img src=lambda-01.png#screenshot alt="Create function"></p><p>We&rsquo;re going to author a function from scratch. Name your function, then under <strong>Runtime</strong> choose &ldquo;Go 1.x&rdquo;.</p><p>Under <strong>Role name</strong> write any name you like. It&rsquo;s a required field but irrelevant for this use case.</p><p>Click <strong>Create function.</strong></p><p><img src=lambda-02.png#screenshot alt="Author from scratch"></p><h3 id=2-configure-your-function>2. Configure your function</h3><p>You&rsquo;ll see a screen for configuring your new function. Under <strong>Handler</strong> enter the name of your Go program.</p><p><img src=lambda-03.png#screenshot alt="Configure your function"></p><p>If you scroll down, you&rsquo;ll see a spot to enter environment variables. This is a great place to enter the Twitter API tokens and secrets, using the variable names that your program expects. The AWS Lambda function will create the environment for you using the variables you provide here.</p><p><img src=lambda-04.png#screenshot alt="Environment variables"></p><p>No further settings are necessary for this use case. Click <strong>Save</strong> at the top of the page.</p><h3 id=3-upload-your-code>3. Upload your code</h3><p>You can upload your function code as a zip file on the configuration screen. Since we&rsquo;re using Go, you&rsquo;ll want to <code>go build</code> first, then zip the resulting executable before uploading that to Lambda.</p><p>&mldr;Of course I&rsquo;m not going to do that manually every time I want to tweak my function. That&rsquo;s what <code>awscli</code> and this bash script is for!</p><p><code>update.sh</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>go build <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>zip fcc-tweet.zip fcc-tweet <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>rm fcc-tweet <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>aws lambda update-function-code --function-name fcc-tweet --zip-file fileb://fcc-tweet.zip <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>rm fcc-tweet.zip
</span></span></code></pre></div><p>Now whenever I make a tweak, I just run <code>bash update.sh</code>.</p><p>If you&rsquo;re not already using <a href=https://aws.amazon.com/cli/>AWS Command Line Interface</a>, do <code>pip install awscli</code> and thank me later. Find instructions for getting set up and configured in a few minutes <a href=https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html>here</a> under <strong>Quick Configuration</strong>.</p><h3 id=4-test-your-function>4. Test your function</h3><p>Wanna see it go? Of course you do! Click &ldquo;Configure test events&rdquo; in the dropdown at the top.</p><p><img src=lambda-05.png#screenshot alt="Configure test events"></p><p>Since you&rsquo;ll use a time-based trigger for this function, you don&rsquo;t need to enter any code to define test events in the popup window. Simply write any name under <strong>Event name</strong> and empty the JSON in the field below. Then click <strong>Create</strong>.</p><p><img src=lambda-06.png#screenshot alt="Configuring an empty test event"></p><p>Click <strong>Test</strong> at the top of the page, and if everything is working correctly you should see&mldr;</p><p><img src=lambda-07.png#screenshot alt="Test success notification"></p><h3 id=5-set-up-cloudwatch-events>5. Set up CloudWatch Events</h3><p>To run our function as we would a cron job - as a regularly scheduled time-based event - we&rsquo;ll use CloudWatch. Click <strong>CloudWatch Events</strong> in the <strong>Designer</strong> sidebar.</p><p><img src=lambda-08.png#screenshot alt="CloudWatch Events trigger"></p><p>Under <strong>Configure triggers</strong>, you&rsquo;ll create a new rule. Choose a descriptive name for your rule without spaces or punctuation, and ensure <strong>Schedule expression</strong> is selected. Then input the time you want your program to run as a <em>rate expression</em>, or cron expression.</p><p>A cron expression looks like this: <code>cron(0 12 * * ? *)</code></p><table><thead><tr><th>Minutes</th><th>Hours</th><th>Month</th><th>Day of week</th><th>Year</th><th>In English</th></tr></thead><tbody><tr><td>0</td><td>12</td><td><code>*</code></td><td>?</td><td><code>*</code></td><td>Run at noon (UTC) every day</td></tr></tbody></table><p>For more on how to write your cron expressions, read <a href=https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html>this.</a></p><p>If you want your program to run twice a day, say once at 10am and again at 3pm, you&rsquo;ll need to set two separate CloudWatch Events triggers and cron expression rules.</p><p>Click <strong>Add</strong>.</p><p><img src=lambda-09.png#screenshot alt="Set cron expression rule"></p><h2 id=watch-it-go>Watch it go</h2><p>That&rsquo;s all you need to get your Lambda function up and running! Now you can sit back, relax, and do more important things than share your RSS links on Twitter.</p><span class=tag-container><a class="p-category tag" href=/tags/aws/>aws
</a>&nbsp;
<a class="p-category tag" href=/tags/go/>go
</a>&nbsp;</span></div></div><aside class=tableOfContentContainer id=tableOfContentContainer><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a href=#is-lambda-right-for-you>Is Lambda right for you</a></li><li><a href=#write-a-lambda-function>Write a Lambda function</a></li><li><a href=#setting-up-aws-lambda>Setting up AWS Lambda</a><ul><li><a href=#1-create-your-function>1. Create your function</a></li><li><a href=#2-configure-your-function>2. Configure your function</a></li><li><a href=#3-upload-your-code>3. Upload your code</a></li><li><a href=#4-test-your-function>4. Test your function</a></li><li><a href=#5-set-up-cloudwatch-events>5. Set up CloudWatch Events</a></li></ul></li><li><a href=#watch-it-go>Watch it go</a></li></ul></nav></aside></main><script>function adjustTextColor(e){const n=document.getElementById(e),s=window.getComputedStyle(n),o=s.backgroundImage.slice(4,-1).replace(/["']/g,""),t=new Image;t.src=o,t.onload=function(){const s=document.createElement("canvas"),r=s.getContext("2d");s.width=t.width,s.height=t.height,r.drawImage(t,0,0,t.width,t.height);const d=r.getImageData(0,0,s.width,s.height),n=d.data;let o=0,i=0,a=0;for(let e=0;e<n.length;e+=4)o+=n[e],i+=n[e+1],a+=n[e+2];o/=n.length/4,i/=n.length/4,a/=n.length/4;const c=Math.sqrt(.299*o*o+.587*i*i+.114*a*a),u=c>128?"black":"white",e=c>128?"rgba(255, 255, 255, 0.7)":"rgba(0, 0, 0, 0.7)",l=document.getElementById("textOverlay");l.style.color=u,l.style.textShadow=`0 0 5px ${e}, 0 0 10px ${e}, 0 0 20px ${e}, 0 0 40px ${e}, 0 0 80px ${e}, 0 0 90px ${e}, 0 0 100px ${e}, 0 0 150px ${e}`}}adjustTextColor("heroSection")</script><script>window.addEventListener("DOMContentLoaded",()=>{const e=new IntersectionObserver(e=>{e.forEach(e=>{const t=e.target.getAttribute("id");e.intersectionRatio>0&&(clearActiveStatesInTableOfContents(),document.querySelector(`nav li a[href="#${t}"]`).parentElement.classList.add("active"))})});document.querySelectorAll("h1[id],h2[id],h3[id],h4[id]").forEach(t=>{e.observe(t)})});function clearActiveStatesInTableOfContents(){document.querySelectorAll("nav li").forEach(e=>{e.classList.remove("active")})}</script><footer><div class=social-media><a href=https://x.com/victoriadotdev><img alt=X src=/social/x.svg></a>
<a href=https://github.com/victoriadrake/><img alt=Github src=/social/github.svg></a>
<a href=mailto:hello@victoria.dev><img alt=Email src=/social/email.png></a></div><div class="logo h-card"><a class=u-url href=https://victoria.dev/><img alt="Site Logo" class="site-logo u-photo" src=/images/vd.svg></a></div><nav class=menu><ul><li><a href=/>hello</a></li><li><a href=/about/>about</a></li><li><a href=/posts/>blog</a></li></ul></nav><script>window.store={"https://victoria.dev/tags/ai/":{title:"Ai",tags:[],content:"",url:"https://victoria.dev/tags/ai/"},"https://victoria.dev/tags/docs/":{title:"Docs",tags:[],content:"",url:"https://victoria.dev/tags/docs/"},"https://victoria.dev/posts/how-to-create-technical-documentation-using-chatgpt/":{title:"How to Create Technical Documentation Using ChatGPT",tags:["ai","docs"],content:` No need to print out stacks of code when conversational AI can summarize it for you. (Photo is Margaret Hamilton, a computer scientist who played a crucial role in developing the software for the Apollo space program at NASA, standing next to a towering stack of printed code that she and her team wrote for the Apollo Guidance Computer.)
I&rsquo;ve maintained for years that good code isn&rsquo;t that great without good documentation. It&rsquo;s a shame that this area is so often overlooked. Good docs don&rsquo;t have to be hard to write and often help you organize your thoughts &ndash; which makes the code better as well.
Of course, I understand that some people much prefer writing code to writing docs. Here, conversational AI like ChatGPT can do most of the heavy lifting for you. Here are a few strategies for getting ChatGPT to write your docs.
Quality Code Basics If your code is ever going to be seen by people other than yourself, there&rsquo;s no excuse to not have a README and simple Getting Started guide. (Even if you&rsquo;re the only one to ever look at it, it&rsquo;s a near-guarantee that you&rsquo;ll forget how something works.)
The ChatGPT web interface allows you to upload files. You can provide your code files and ask ChatGPT to create your docs with a prompt such as:
Write a straightforward README for the repository that contains these files. Include a beginner-friendly Getting Started section that starts with cloning the repository and installing any necessary dependencies.
If you want ChatGPT to get as close as it can to a complete document that you can copy and paste into your git repository, you can also provide details such as:
The repo URL
The license type, if any
Instructions for potential contributors
Ways to support the project financially
Links to find you or your project on social media
Now your job is as simple as giving the results a read-through and run-through to ensure it makes sense and that instructions are correct. For simple projects or established frameworks, I&rsquo;ve found ChatGPT to be accurate and complete.
What if your project isn&rsquo;t simple? In this case, you might benefit from some more in-depth documentation.
In-Depth Documentation More complicated projects or ones that don&rsquo;t use an established framework can benefit from in-depth documentation to guide new contributors or refresh the memories of their builders.
ChatGPT can help you write in-depth documentation with a prompt such as:
Explain this code to me. Start with an overview of what it does in simple language, then detail how all the parts and functions connect in an in-depth technical manner. Describe anything I might need to know about adding to or modifying this code in the future.
Prompts like this are a great practice, whether or not you need the resulting docs. It&rsquo;s like having an always-available programming partner to look over your work and see if it makes sense to someone other than yourself.
Reading the results will help ensure they&rsquo;re correct, that the code you wrote does what you intended it to, and can even help highlight areas for further improvement.
Inline Documentation Over years of working with many different teams, developers, and technical leaders, I&rsquo;ve come to see that some folks just don&rsquo;t read anything outside of the code files. It&rsquo;s an unfortunate habit that I encourage changing &ndash; reading your team&rsquo;s docs, forum threads, blog posts, etc. can save you time and energy and help inspire your own work.
In the meantime, ChatGPT can still help by creating inline documentation. This can be language-specific (like pydoc docstrings, which have the added benefit of generating standalone documentation) or it could take the form of simple comments in the code file.
To generate inline docs, upload your files to ChatGPT and use a prompt such as:
Add inline documentation to this code file without changing any of the code. Add short explanations to each function that document what it does and its input and output.
A Few Tips If you find your results lacking, try providing more context about what the code does in plain language. When communicating (with software developers or ChatGPT) it helps to start by giving context about the general intent, goal, or desired end result. If you find it&rsquo;s difficult to explain, you may have highlighted some opportunities for improvement that would benefit both your docs and your code.
A note on execution: for very long code files, it can be cumbersome to copy and paste the results. ChatGPT can give you a zip of your updated file(s) if you add an instruction to do so, i.e. &ldquo;&hellip;and give me the updated file to download.&rdquo;
I hope I&rsquo;ve given you some inspiration of how using ChatGPT for documentation can streamline the process and improve clarity. Whether it&rsquo;s in a README, detailed explanations, or found in inline comments, good documentation helps make your code more accessible. Now you can focus on coding while ensuring your project is well-documented and easier for others (and your future self) to understand and contribute to.
`,url:"https://victoria.dev/posts/how-to-create-technical-documentation-using-chatgpt/"},"https://victoria.dev/tags/leadership/":{title:"Leadership",tags:[],content:"",url:"https://victoria.dev/tags/leadership/"},"https://victoria.dev/tags/":{title:"Tags",tags:[],content:"",url:"https://victoria.dev/tags/"},"https://victoria.dev/posts/top-5-tips-for-effective-software-leadership/":{title:"Top 5 Tips for Effective Software Leadership",tags:["leadership"],content:`The more you advance towards leadership in your tech career, the less technical your focus becomes.
The most helpful things you can do become less about any particular technological problem and more about how to enable your leaders and team members to solve it.
Here are the most critical things technical leaders can do for their teams.
1. Delegate Responsibility Trust your team members. Instead of fixing bugs or implementing features yourself, assign these tasks to the right people. This empowers your team and frees you to focus on larger challenges.
Concrete Suggestions:
Assign Ownership: For major initiatives, assign clear ownership to team members. For example, if your team is migrating to a new framework, delegate the project lead role to someone who shows interest and capability. Encourage them to make decisions and involve them in strategic discussions.
Set Expectations: When delegating tasks, clearly outline the expected outcomes and deadlines, but give them the autonomy to decide how to achieve those goals.
Provide Support: Check in periodically to see if they need guidance or resources, but avoid micromanaging. This balance helps them grow while ensuring project success.
2. Foster a Learning Culture Encourage continuous learning. Provide access to online courses, workshops, or conferences. If a team member is interested in a new programming language, support them. This could lead to innovative solutions in the future.
Concrete Suggestions:
Learning Stipends: Offer a yearly learning stipend for your team members to spend on courses, books, or conferences. This demonstrates your commitment to their growth.
Internal Knowledge Sharing: Organize regular “lunch and learn” sessions where team members can share what they’ve learned or are passionate about. This promotes peer learning and creates an open, curious environment.
Hackathons: Host internal hackathons or innovation days where team members can work on projects outside their regular responsibilities. This can be a fun way to explore new technologies and foster collaboration.
3. Focus on Communication Clear communication is crucial. Regularly share the team’s goals and how each person’s work contributes. This can be done through weekly team meetings or one-on-one check-ins.
Concrete Suggestions:
Set Up Communication Channels: Use tools like Slack or Microsoft Teams to keep communication flowing. Create channels for project updates, general discussions, and specific technical topics.
Regular Updates: Send a weekly email or Slack message summarizing key achievements, upcoming deadlines, and any changes in direction. Sharing context like this keeps everyone aligned and reduces misunderstandings.
One-on-One Meetings: Schedule regular one-on-ones with your team members. Use these meetings to discuss their progress, any concerns they have, and how they feel about their workload and career development.
4. Develop Leadership in Others Identify potential leaders in your team and mentor them. Pair them with experienced members on challenging projects or let them lead meetings. This helps build a strong pipeline of future leaders.
Concrete Suggestions:
Mentorship Program: Create a formal or informal mentorship program within your team. Pair junior engineers with senior ones to help them navigate challenges and develop leadership skills.
Leadership Opportunities: Offer opportunities for potential leaders to take charge, whether it’s leading a small project, facilitating a team meeting, or representing the team in cross-department discussions.
Feedback Loops: Provide constructive feedback to help them refine their leadership abilities. Highlight what they’re doing well and offer guidance on areas for improvement.
5. Remove Roadblocks Look for and eliminate obstacles that slow your team down. Be your team&rsquo;s best advocate with senior management. This could involve negotiating for more resources, improving tools, or streamlining processes.
Concrete Suggestions:
Identify Bottlenecks: Regularly review your team’s workflow to spot inefficiencies. For example, if your deployment process is highly manual and takes up a lot of your team&rsquo;s time, consider investing in improving your CI/CD workflow.
Cross-Department Collaboration: If your team is waiting on another department (e.g., for approvals or resources), facilitate conversations between teams to speed up the process.
Empower Problem Solvers: Encourage team members to let you know about any blockers they encounter. Create an environment where people feel comfortable raising issues without fear of blame, and prioritize fixing these roadblocks swiftly.
By implementing these strategies, you can empower your team to succeed and foster a culture of growth and collaboration. As a technical leader, your role shifts from solving problems yourself to enabling others to do their best work.
`,url:"https://victoria.dev/posts/top-5-tips-for-effective-software-leadership/"},"https://victoria.dev/":{title:"Victoria Drake",tags:[],content:`Hello, I&rsquo;m Victoria Drake. I love leading tech teams, building software, and empowering our next generation of engineers.
`,url:"https://victoria.dev/"},"https://victoria.dev/tags/career/":{title:"Career",tags:[],content:"",url:"https://victoria.dev/tags/career/"},"https://victoria.dev/posts/how-to-future-proof-your-software-engineering-career-for-the-age-of-agi/":{title:"How to Future-Proof Your Software Engineering Career for the Age of AGI",tags:["ai","career"],content:`In the viral essay The Decade Ahead, Leopold Aschenbrenner predicts that Artificial General Intelligence (AGI) will be a reality in only a few years. But what exactly is AGI, and how does it differ from the AI we have today?
AGI refers to a type of artificial intelligence that has the ability to understand, learn, and apply knowledge across a wide range of tasks at a level comparable to, or even beyond, human intelligence. Unlike narrow AI, which excels at specific tasks (like image recognition or playing chess), AGI would be able to perform any cognitive task that a human can, adapt to new situations, and improve its capabilities over time without human intervention.
The emergence of AGI would fundamentally change how we think about and interact with technology. For engineers, it means preparing for a world where intelligent systems can perform tasks autonomously, requiring new skills and approaches to software development.
Here are some key areas of work to focus on now to prepare you for the AGI era.
1. Mastering Machine Learning and Deep Learning Engineers with expertise in these fields will be at the forefront of AGI development. Machine learning and deep learning are the building blocks of AGI, as they enable systems to learn from data, identify patterns, and make decisions.
To prepare for AGI, you need to go beyond the basics of supervised learning and explore more advanced areas like reinforcement learning, where agents learn by interacting with their environment, and unsupervised learning, which allows systems to find hidden patterns in data without explicit guidance. Neural networks, particularly deep neural networks, will play a critical role in enabling AGI to generalize across tasks.
Why these skills? AGI will require systems that can adapt and improve autonomously. Mastering these skills will help you understand how to create models that can handle complex, unstructured data and make decisions in real-time, which is essential for AGI.
How to learn Courses: Consider taking advanced courses on platforms like freeCodeCamp, Coursera, edX, or Udacity that focus on reinforcement learning, neural networks, and deep learning. Projects: Build your own machine learning models and experiment with different types of data. Participating in Kaggle competitions can also help you hone your skills. Job titles to watch: Machine Learning Engineer, AI Research Scientist.
2. Software Engineering with a Focus on AI Integration Traditional software engineering roles will evolve to integrate AI components seamlessly. This means developing frameworks that allow AGI to be incorporated into existing systems or creating entirely new systems designed around AGI capabilities.
What does this look like? Engineers might develop APIs that allow AGI to communicate with other software, create microservices that enable modular AGI deployment, or design platforms that facilitate continuous learning for AGI systems. For example, integrating AGI into a customer service platform could involve building an interface where AGI handles complex queries while human agents focus on more nuanced tasks.
How to learn Study: Learn how to design and implement AI components in software through courses and hands-on experience. Understanding cloud-based AI services like AWS SageMaker or Google AI Platform will also be beneficial. Practice: Work on projects where you integrate AI models into existing applications, such as adding a chatbot to a web service or incorporating predictive analytics into a mobile app. Job titles to watch: Full Stack Developer with AI specialization, AI Software Engineer.
3. Navigating Ethics and AI Governance As AGI could pose significant ethical and governance challenges, roles focusing on the ethical implications, policy-making, and regulatory compliance will be crucial. This includes ensuring AGI systems operate within legal and ethical frameworks. Public as well as private sector experience will be valuable.
Key ethical concerns include issues like accuracy, accountability, and transparency. You can benefit from developing skills in critical thinking and understanding how to interpret data and statistics. These skills can be helpful when collaborating with policymakers.
How to learn Read: Explore literature on how policy is formed at the institutional and governmental level. Courses: Consider taking courses on statistics and ethics to grow a deeper understanding of model results. Job titles to watch: AI Ethics Analyst, Policy Advisor for AI, Compliance Officer for AI Systems.
4. Evolving Human-Computer Interaction (HCI) HCI will quickly transform into Human-AI Interaction Design. As AGI systems become more prevalent, they will need to interact with humans in intuitive and seamless ways. Companies will need interfaces where humans can interact with AGI systems effectively, built by engineers who understand cognitive psychology and UX/UI design for AI systems.
Engineers will need to design interfaces where AGI can explain its decisions, ask for clarification when needed, and understand human emotions and context. For example, AGI in healthcare might need to provide doctors with explanations of its diagnoses while considering the doctor&rsquo;s expertise and the patient&rsquo;s emotions. Building skills in designing intuitive interfaces and interactions between humans and intelligent systems will help you to be highly successful in AGI integration.
How to learn Courses: Study HCI and UX design with a focus on AI systems. Platforms like Interaction Design Foundation and Coursera offer relevant courses. Projects: Experiment with designing user interfaces for AI-powered applications. This could include developing conversational agents or creating dashboards that visualize AI decision-making processes. Job titles to watch: Interaction Designer for AI, User Experience Researcher for AI Systems.
5. Enhancing Autonomous Systems and Robotics If AGI leads to more autonomous robots, engineers who can design, build, and program robots with AGI capabilities will be in demand. This includes understanding how AGI can enhance robotic functionality.
AGI has the potential to revolutionize autonomous systems and robotics by enabling machines to learn and adapt in real-time. This could lead to more advanced self-driving cars, drones, and robots that can perform complex tasks without human intervention. AGI could allow robots to understand and navigate unstructured environments, learn from experience, and collaborate with humans more effectively. For example, an AGI-powered robot could assist in disaster relief by autonomously adapting to changing conditions and coordinating with human teams.
Working on autonomous systems, whether in robotics, self-driving vehicles, or drones, can provide practical experience with highly independent systems. These skills will be transferrable to managing and optimizing AGI-based autonomous agents.
How to learn Courses: Take robotics courses that cover autonomous systems, computer vision, and AI integration. Projects: Work on robotics projects, such as building an autonomous vehicle or programming a robot to perform complex tasks. Job titles to watch: Robotics Engineer, Automation Specialist.
6. Pioneering Hardware Development for AGI We&rsquo;ll need engineers working on specialized hardware that can support AGI. Technologies like neuromorphic computing chips or quantum computing might be necessary for the computational power AGI would require.
Neuromorphic computing involves designing chips that replicate the structure and function of the human brain&rsquo;s neurons and synapses. These chips could enable more efficient and powerful AI systems by processing information in ways that are closer to how the human brain works. Additionally, quantum computing could provide the processing power needed for AGI&rsquo;s complex calculations.
How to learn Study: Learn about neuromorphic computing and quantum computing through specialized courses and research papers. Staying updated with developments from companies like IBM and Intel, which are working on neuromorphic chips, can also be helpful. Projects: Experiment with hardware design, such as working with FPGAs (Field Programmable Gate Arrays) or exploring quantum computing platforms like IBM Q. Job titles to watch: Hardware Engineer for AI, Quantum Computing Engineer.
7. Securing the Future: Cybersecurity for AGI AGI systems will introduce new security challenges. Engineers with expertise in cybersecurity will be in high demand to protect AGI systems from national security threats, ensure data privacy, and secure AI-driven decision-making processes against manipulation. There are also concerns about data privacy, as AGI systems will likely handle sensitive information across various domains.
How to learn Courses: Take cybersecurity courses focused on AI and machine learning security. Platforms like freeCodeCamp, Cybrary, and Coursera offer relevant courses. Practice: Engage in cybersecurity challenges, such as Capture the Flag (CTF) competitions, to develop hands-on skills in securing computing systems. Job titles to watch: AI Security Specialist, Cybersecurity Analyst for AI.
8. Data Engineering: Fueling AGI with Information Handling large-scale data systems will be critical for AGI, as it will require vast amounts of data to learn and operate effectively. Data engineers will play a crucial role in building and maintaining the infrastructure that feeds AGI with the information it needs.
Data engineers will need expertise in big data technologies, such as Hadoop and Spark, and real-time data processing systems, like Apache Kafka. They will also need to ensure data quality and integrity, as AGI systems will rely heavily on accurate and comprehensive data to function effectively.
How will data engineers work with AGI systems? Data engineers will design data pipelines that can handle the immense scale of data AGI requires. This includes everything from data ingestion, storage, processing, and ensuring that the data is of high quality and usable for AGI models. Additionally, they will need to implement systems for continuous data updates, enabling AGI to learn and adapt in real-time.
How to learn Courses: Take courses on big data technologies, data pipeline architecture, and real-time data processing. Platforms like Udemy and Coursera offer courses on tools like Apache Kafka, Spark, and Hadoop. Projects: Work on projects that involve large-scale data processing and pipeline development. Contributing to open-source big data projects can also be a great way to gain experience. Job titles to watch: Data Engineer, Big Data Architect.
9. Building Infrastructure for AGI AGI will require robust and scalable infrastructure on a never-before-seen scale. Engineers with experience in cloud computing, distributed systems, and infrastructure as code (IaC) will be crucial in building the systems that support AGI.
What kind of infrastructure will AGI need? AGI systems will likely operate on a global scale, requiring vast amounts of computational power and data storage. Engineers will need to design cloud-based infrastructure that can scale dynamically, handle high volumes of data, and ensure low latency for real-time processing. They will also need to consider the security and reliability of these systems.
How to learn Courses: Study cloud computing platforms like AWS, Google Cloud, or Microsoft Azure. Learning about distributed systems and IaC tools like Terraform and Ansible will also be beneficial. Certifications: Earning certifications in cloud architecture (e.g., AWS Certified Solutions Architect) can help solidify your knowledge. Projects: Work on setting up and managing cloud infrastructure for applications, experimenting with scalability and load balancing. Job titles to watch: Cloud Infrastructure Engineer, Systems Architect for AI.
10. Cross-Disciplinary Collaboration in the AGI Era Working in roles that involve cross-disciplinary collaboration, such as roles in research or innovation labs, can provide engineers with the ability to think broadly and integrate knowledge from various fields. Knowledge in other fields can give you the ability to engineer products that help people in a niche you care about.
Combining skills from fields such as biology (for bioinformatics or synthetic biology with AGI) and psychology (for understanding human-AI interaction) will be vital in the AGI era. Engineers who can think broadly and collaborate across disciplines will be better equipped to tackle complex problems that require diverse perspectives. For instance, combining AGI with neuroscience could advance brain-computer interfaces.
How to learn Networking: Engage with professionals from different fields by attending interdisciplinary conferences and joining relevant online communities. Courses: Take courses or workshops in complementary fields, such as biology, psychology, or environmental science, to broaden your understanding. Projects: Collaborate on interdisciplinary projects, such as developing AI models that incorporate insights from other domains. Job titles to watch: Bioinformatics Engineer, Environmental Data Scientist.
11. Education and Training for an AGI-Ready Workforce As AGI transforms industries, there will be a growing need for educational programs that teach engineers how to work with AGI systems.
What should these programs focus on? Education and training programs should cover a range of topics, from yet-undeveloped AGI techniques, safety protocols, policy-making, to interdisciplinary collaboration. Preparation for creating or leading education in AGI means becoming a continuous learner yourself. Additionally, training should emphasize lifelong learning, as AGI technology will continue to evolve rapidly.
How to learn Create content: If you&rsquo;re an educator, consider developing courses or workshops focused on AGI-related topics. Collaborate with industry experts to ensure the content is relevant and up-to-date. Enroll in programs: Participate in advanced AI or AGI training programs, either through universities or industry-led initiatives. Stay updated with emerging trends by attending seminars and conferences. Job titles to watch: AI Curriculum Developer, Training Specialist for AI Technologies.
12. Shaping Regulations in an AGI-Driven World Engineers working on regulatory technology (RegTech) will gain insight into compliance and governance, which will be critical as AGI evolves within legal frameworks. Understanding how to navigate and shape regulations will be vital.
Regulations could cover areas such as data privacy, transparency, accountability, and the use of AGI in various industries. Engineers working in this field will need to collaborate with policymakers, legal experts, and industry leaders to develop guidelines that balance innovation with responsibility.
How to learn Study: Stay informed about current AI regulations and legal frameworks. If you&rsquo;re interested, consider pursuing a certification or degree in law or public policy with a focus on AI governance. Networking: Join industry groups like IEEE or think tanks that focus on AI policy and ethics. Engaging in discussions with policymakers can provide valuable insights into the regulatory landscape. Job titles to watch: Regulatory Engineer, Compliance Specialist for AI.
13. Research and Development (R&amp;D) in AGI-related Areas Finally, engineers who are involved in cutting-edge research in AGI, cognitive computing, or advanced AI labs will be directly contributing to and understanding the frontiers of AGI technology, giving them a head start in a world where AGI is a reality. These roles offer the opportunity to shape the future of AGI and explore new possibilities in artificial intelligence.
Get involved in R&amp;D by joining research institutions, universities, or tech companies that focus on AGI development. Contributing to open-source AI projects or publishing papers on AGI-related topics can also help you establish yourself in the field.
How to learn Research: Stay updated with the latest advancements in AGI by reading academic papers, attending conferences, and following thought leaders in the field. Collaborate: Work with academic or industry researchers on AGI projects. Participating in hackathons or research competitions can also provide hands-on experience. Job titles to watch: AGI Research Scientist, Cognitive Computing Engineer.
Future Job Titles The transition to an AGI world will likely see a blend of these roles, where engineers might need to be polymaths, understanding not just one but multiple areas of technology and science. It&rsquo;s important to grow your technical skills, but also practice adaptability and continuous learning.
Be on the lookout for roles that might not directly mention AGI but are foundational in AI, machine learning, and related technologies. As you choose your next role, think ahead to how you can tailor your focus and future-proof your work for the AGI era.
Actionable steps Learn continuously: Make lifelong learning a priority by regularly updating your skills and knowledge through courses, certifications, and hands-on projects. Network: Build relationships with professionals in various fields to stay informed about emerging trends and opportunities. Adapt: Stay flexible and open to new challenges, as the AGI era will require engineers to adapt to rapidly changing technologies and environments. `,url:"https://victoria.dev/posts/how-to-future-proof-your-software-engineering-career-for-the-age-of-agi/"},"https://victoria.dev/categories/article/":{title:"Article",tags:[],content:"",url:"https://victoria.dev/categories/article/"},"https://victoria.dev/categories/":{title:"Categories",tags:[],content:"",url:"https://victoria.dev/categories/"},"https://victoria.dev/posts/post-to-your-static-website-from-your-iphone/":{title:"Post to your static website from your iPhone",tags:["websites"],content:`I love websites. I love static sites in particular. But I know that sometimes it&rsquo;s just not practical to write and post only from your computer. With my hands full raising a family, I do a lot more development in stops and starts from my phone these days than I thought I ever would.
So I brought together everything that&rsquo;s great about Hugo plus everything that&rsquo;s great about sharing your 3AM thoughts with the world from your phone, thanks to Collected Notes. I put it in a new Hugo site template with a fancy new theme I call Quint.
You can deploy the Quint site template with one button (this button):
The Quint template can use the Collected Notes app as a CMS and also saves your posts to the site repository, for redundancy. It fetches new posts each time you build, and if you&rsquo;re deploying via Netlify or GitHub Actions, you can use a webhook to deploy the site whenever you make a new post with Collected Notes.
To set up your own site:
Deploy the Quint template to Netlify with the button above, or clone the repo if you plan to use another deployment solution. Sign up for Collected Notes if you haven&rsquo;t already (there&rsquo;s a free plan) and download the Collected Notes app on your iPhone. Update the utils/fetch-posts.js file to use your Collected Notes site name. Allow the GitHub Action to push changes back to your repository to save your posts. Under Settings &gt; Actions &gt; General &gt; Workflow permissions, choose Read and write permissions. Netlify will trigger a new build each time you push to your site repo, or, if you have a Collected Notes Premium subscription, you can set a Netlify Build Hook URL in your Collected Notes site settings to automatically redeploy the site when you make a post or update an existing post.
I hope this template helps out busy people like you! I&rsquo;m using this solution myself, of course, to write the next chapter of my one-bag era &ndash; with my phone in one hand and a coffee in the other.
`,url:"https://victoria.dev/posts/post-to-your-static-website-from-your-iphone/"},"https://victoria.dev/tags/websites/":{title:"Websites",tags:[],content:"",url:"https://victoria.dev/tags/websites/"},"https://victoria.dev/tags/algorithms/":{title:"Algorithms",tags:[],content:"",url:"https://victoria.dev/tags/algorithms/"},"https://victoria.dev/tags/api/":{title:"Api",tags:[],content:"",url:"https://victoria.dev/tags/api/"},"https://victoria.dev/tags/chatgpt/":{title:"Chatgpt",tags:[],content:"",url:"https://victoria.dev/tags/chatgpt/"},"https://victoria.dev/tags/coding/":{title:"Coding",tags:[],content:"",url:"https://victoria.dev/tags/coding/"},"https://victoria.dev/tags/computing/":{title:"Computing",tags:[],content:"",url:"https://victoria.dev/tags/computing/"},"https://victoria.dev/tags/data/":{title:"Data",tags:[],content:"",url:"https://victoria.dev/tags/data/"},"https://victoria.dev/posts/how-to-send-long-text-input-to-chatgpt-using-the-openai-api/":{title:"How to send long text input to ChatGPT using the OpenAI API",tags:["chatgpt","nlp","api","algorithms","coding","computing","data","python"],content:`In a previous post, I showed how you can apply text preprocessing techniques to shorten your input length for ChatGPT. Today in the web interface (chat.openai.com), ChatGPT allows you to send a message with a maximum token length of 4,096.
There are bound to be situations in which this isn&rsquo;t enough, such as when you want to read in a large amount of text from a file. Using the OpenAI API allows you to send many more tokens in a messages array, with the maximum number depending on your chosen model. This lets you provide large amounts of text to ChatGPT using chunking. Here&rsquo;s how.
Chunking your input The gpt-4 model currently has a maximum content length token limit of 8,192 tokens. (Here are the docs containing current limits for all the models.) Remember that you can first apply text preprocessing techniques to reduce your input size &ndash; in my previous post I achieved a 28% size reduction without losing meaning with just a little tokenization and pruning.
When this isn&rsquo;t enough to fit your message within the maximum message token limit, you can take a general programmatic approach that sends your input in message chunks. The goal is to divide your text into sections that each fit within the model&rsquo;s token limit. The general idea is to:
Tokenize and split text into chunks based on the model&rsquo;s token limit. It&rsquo;s better to keep message chunks slightly below the token limit since the token limit is shared between your message and ChatGPT&rsquo;s response. Maintain context between chunks, e.g. avoid splitting a sentence in the middle. Each chunk is sent as a separate message in the conversation thread.
Handling responses You send your chunks to ChatGPT using the OpenAI library&rsquo;s ChatCompletion. ChatGPT returns individual responses for each message, so you may want to process these by:
Concatenating responses in the order you sent them to get a coherent answer. Manage conversation flow by keeping track of which response refers to which chunk. Formatting the response to suit your desired output, e.g. replacing \\n with line breaks. Putting it all together Using the OpenAI API, you can send multiple messages to ChatGPT and ask it to wait for you to provide all of the data before answering your prompt. Being a language model, you can provide these instructions to ChatGPT in plain language. Here&rsquo;s a suggested script:
Prompt: Summarize the following text for me
To provide the context for the above prompt, I will send you text in parts. When I am finished, I will tell you &ldquo;ALL PARTS SENT&rdquo;. Do not answer until you have received all the parts.
I created a Python module, chatgptmax, that puts all this together. It breaks up a large amount of text by a given maximum token length and sends it in chunks to ChatGPT.
You can install it with pip install chatgptmax, but here&rsquo;s the juicy part:
import os import openai import tiktoken # Set up your OpenAI API key # Load your API key from an environment variable or secret management service openai.api_key = os.getenv(&#34;OPENAI_API_KEY&#34;) def send( prompt=None, text_data=None, chat_model=&#34;gpt-3.5-turbo&#34;, model_token_limit=8192, max_tokens=2500, ): &#34;&#34;&#34; Send the prompt at the start of the conversation and then send chunks of text_data to ChatGPT via the OpenAI API. If the text_data is too long, it splits it into chunks and sends each chunk separately. Args: - prompt (str, optional): The prompt to guide the model&#39;s response. - text_data (str, optional): Additional text data to be included. - max_tokens (int, optional): Maximum tokens for each API call. Default is 2500. Returns: - list or str: A list of model&#39;s responses for each chunk or an error message. &#34;&#34;&#34; # Check if the necessary arguments are provided if not prompt: return &#34;Error: Prompt is missing. Please provide a prompt.&#34; if not text_data: return &#34;Error: Text data is missing. Please provide some text data.&#34; # Initialize the tokenizer tokenizer = tiktoken.encoding_for_model(chat_model) # Encode the text_data into token integers token_integers = tokenizer.encode(text_data) # Split the token integers into chunks based on max_tokens chunk_size = max_tokens - len(tokenizer.encode(prompt)) chunks = [ token_integers[i : i + chunk_size] for i in range(0, len(token_integers), chunk_size) ] # Decode token chunks back to strings chunks = [tokenizer.decode(chunk) for chunk in chunks] responses = [] messages = [ {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}, { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;To provide the context for the above prompt, I will send you text in parts. When I am finished, I will tell you &#39;ALL PARTS SENT&#39;. Do not answer until you have received all the parts.&#34;, }, ] for chunk in chunks: messages.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: chunk}) # Check if total tokens exceed the model&#39;s limit and remove oldest chunks if necessary while ( sum(len(tokenizer.encode(msg[&#34;content&#34;])) for msg in messages) &gt; model_token_limit ): messages.pop(1) # Remove the oldest chunk response = openai.ChatCompletion.create(model=chat_model, messages=messages) chatgpt_response = response.choices[0].message[&#34;content&#34;].strip() responses.append(chatgpt_response) # Add the final &#34;ALL PARTS SENT&#34; message messages.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;ALL PARTS SENT&#34;}) response = openai.ChatCompletion.create(model=chat_model, messages=messages) final_response = response.choices[0].message[&#34;content&#34;].strip() responses.append(final_response) return responses Here&rsquo;s an example of how you can use this module with text data read from a file. (chatgptmax also provides a convenience method for getting text from a file.)
# First, import the necessary modules and the function import os from chatgptmax import send # Define a function to read the content of a file def read_file_content(file_path): with open(file_path, &#39;r&#39;, encoding=&#39;utf-8&#39;) as file: return file.read() # Use the function if __name__ == &#34;__main__&#34;: # Specify the path to your file file_path = &#34;path_to_your_file.txt&#34; # Read the content of the file file_content = read_file_content(file_path) # Define your prompt prompt_text = &#34;Summarize the following text for me:&#34; # Send the file content to ChatGPT responses = send(prompt=prompt_text, text_data=file_content) # Print the responses for response in responses: print(response) Error handling While the module is designed to handle most standard use cases, there are potential pitfalls to be aware of:
Incomplete sentences: If a chunk ends in the middle of a sentence, it might alter the meaning or context. To mitigate this, consider ensuring that chunks end at full stops or natural breaks in the text. You could do this by separating the text-chunking task into a separate function that: Splits the text into sentences. Iterates over the sentences and adds them to a chunk until the chunk reaches the maximum size. Starts a new chunk when the current chunk reaches the maximum size or when adding another sentence would exceed the maximum size. API connectivity issues: There&rsquo;s always a possibility of timeouts or connectivity problems during API calls. If this is a significant issue for your application, you can include retry logic in your code. If an API call fails, the script could wait for a few seconds and then try again, ensuring that all chunks are processed. Rate limits: Be mindful of OpenAI API&rsquo;s rate limits. If you&rsquo;re sending many chunks in rapid succession, you might hit these limits. Introducing a slight delay between calls or spreading out requests can help avoid this. Optimization As with any process, there&rsquo;s always room for improvement. Here are a couple of ways you might optimize the module&rsquo;s chunking and sending process further:
Parallelizing API calls: If OpenAI API&rsquo;s rate limits and your infrastructure allow, you could send multiple chunks simultaneously. This parallel processing can speed up the overall time it takes to get responses for all chunks. Unless you have access to OpenAI&rsquo;s 32k models or need to use small chunk sizes, however, parallelism gains are likely to be minimal. Caching mechanisms: If you find yourself sending the same or similar chunks frequently, consider implementing a caching system. By storing ChatGPT&rsquo;s responses for specific chunks, you can retrieve them instantly from the cache the next time, saving both time and API calls. Now what If you found your way here via search, you probably already have a use case in mind. Here are some other (startup) ideas:
You&rsquo;re a researcher who wants to save time by getting short summaries of many lengthy articles. You&rsquo;re a legal professional who wants to analyze long contracts by extracting key points or clauses. You&rsquo;re a financial analyst who wants to pull a quick overview of trends from a long report. You&rsquo;re a writer who wants feedback on a new article or chapter&hellip; without having to actually show it to anyone yet. Do you have a use case I didn&rsquo;t list? Let me know about it! In the meantime, have fun sending lots of text to ChatGPT.
`,url:"https://victoria.dev/posts/how-to-send-long-text-input-to-chatgpt-using-the-openai-api/"},"https://victoria.dev/tags/nlp/":{title:"Nlp",tags:[],content:"",url:"https://victoria.dev/tags/nlp/"},"https://victoria.dev/tags/python/":{title:"Python",tags:[],content:"",url:"https://victoria.dev/tags/python/"},"https://victoria.dev/posts/optimizing-text-for-chatgpt-nlp-and-text-pre-processing-techniques/":{title:"Optimizing text for ChatGPT: NLP and text pre-processing techniques",tags:["nlp","chatgpt","api","algorithms","coding","computing","data","python"],content:`In order for chatbots and voice assistants to be helpful, they need to be able to take in and understand our instructions in plain language using Natural Language Processing (NLP). ChatGPT relies on a blend of advanced algorithms and text preprocessing methods to make sense of our words. But just throwing a wall of text at it can be inefficient &ndash; you might be dumping in a lot of noise with that signal and hitting the text input limit.
Text preprocessing can help shorten and refine your input, ensuring that ChatGPT can grasp the essence without getting overwhelmed. In this article, we&rsquo;ll explore these techniques, understand their importance, and see how they make your interactions with tools like ChatGPT more reliable and productive.
Text preprocessing Text preprocessing prepares raw text data for analysis by NLP models. Generally, it distills everyday text (like full sentences) to make it more manageable or concise and meaningful. Techniques include:
Tokenization: splitting up text by sentences or paragraphs. For example, you could break down a lengthy legal document into individual clauses or sentences. Extractive summarization: selecting key sentences from the text and discarding the rest. Instead of reading an entire 10-page document, extractive summarization could pinpoint the most crucial sentences and give you a concise overview without delving into the details. Abstractive summarization: generating a concise representation of the text content, for example, turning a 10-page document into a brief paragraph that captures the document&rsquo;s essence in new wording. Pruning: removing redundant or less relevant parts. For example, in a verbose email thread, pruning can help remove all the greetings, sign-offs, and other repetitive elements, leaving only the core content for analysis. While all these techniques can help reduce the size of raw text data, some of these techniques are easier to apply to general use cases than others. Let&rsquo;s examine how text preprocessing can help us send a large amount of text to ChatGPT.
Tokenization and ChatGPT input limits In the realm of Natural Language Processing (NLP), a token is the basic unit of text that a system reads. At its simplest, you can think of a token as a word, but depending on the language and the specific tokenization method used, a token can represent a word, part of a word, or even multiple words.
While in English we often equate tokens with words, in NLP, the concept is broader. A token can be as short as a single character or as long as a word. For example, with word tokenization, the sentence &ldquo;Unicode characters such as emojis are not indivisible. ✂️&rdquo; can be broken down into tokens like this: [&ldquo;Unicode&rdquo;, &ldquo;characters&rdquo;, &ldquo;such&rdquo;, &ldquo;as&rdquo;, &ldquo;emojis&rdquo;, &ldquo;are&rdquo;, &ldquo;not&rdquo;, &ldquo;indivisible&rdquo;, &ldquo;.&rdquo;, &ldquo;✂️&rdquo;]
In another form called Byte-Pair Encoding (BPE), the same sentence is tokenized as: [&ldquo;Un&rdquo;, &ldquo;ic&rdquo;, &ldquo;ode&rdquo;, &quot; characters&quot;, &quot; such&quot;, &quot; as&quot;, &quot; em, &ldquo;oj&rdquo;, &ldquo;is&rdquo;, &quot; are&quot;, &quot; not&quot;, &quot; ind&quot;, &ldquo;iv&rdquo;, &ldquo;isible&rdquo;, &ldquo;.&rdquo;, &quot; �&quot;, &ldquo;�️&rdquo;]. The emoji itself is split into tokens containing its underlying bytes.
Depending on the ChatGPT model chosen, your text input size is restricted by tokens. Here are the docs containing current limits. BPE is used by ChatGPT to determine token count, and we&rsquo;ll discuss it more thoroughly later. First, we can programmatically apply some preprocessing techniques to reduce our text input size and use fewer tokens.
A general programmatic approach For a general approach that can be applied programmatically, pruning is a suitable preprocessing technique. One form is stop word removal, or removing common words that might not add significant meaning in certain contexts. For example, consider the sentence:
&ldquo;I always enjoy having pizza with my friends on weekends.&rdquo;
Stop words are often words that don&rsquo;t carry significant meaning on their own in a given context. In this sentence, words like &ldquo;I&rdquo;, &ldquo;always&rdquo;, &ldquo;enjoy&rdquo;, &ldquo;having&rdquo;, &ldquo;with&rdquo;, &ldquo;my&rdquo;, &ldquo;on&rdquo; are considered stop words.
After removing the stop words, the sentence becomes:
&ldquo;pizza friends weekends.&rdquo;
Now, the sentence is distilled to its key components, highlighting the main subject (pizza) and the associated context (friends and weekends). If you find yourself wishing you could convince people to do this in real life (coughmeetingscough)&hellip; you aren&rsquo;t alone.
Stop word removal is straightforward to apply programmatically: given a list of stop words, examine some text input to see if it contains any of the stop words on your list. If it does, remove them, then return the altered text.
def clean_stopwords(text: str) -&gt; str: stopwords = [&#34;a&#34;, &#34;an&#34;, &#34;and&#34;, &#34;at&#34;, &#34;but&#34;, &#34;how&#34;, &#34;in&#34;, &#34;is&#34;, &#34;on&#34;, &#34;or&#34;, &#34;the&#34;, &#34;to&#34;, &#34;what&#34;, &#34;will&#34;] tokens = text.split() clean_tokens = [t for t in tokens if not t in stopwords] return &#34; &#34;.join(clean_tokens) To see how effective stop word removal can be, I took the entire text of my Tech Leader Docs newsletter (17,230 words consisting of 104,892 characters) and processed it using the above function. How effective was it? The resulting text contained 89,337 characters, which is about a 15% reduction in size.
Other pruning techniques can also be applied programmatically. Removing punctuation, numbers, HTML tags, URLs and email addresses, or non-alphabetical characters are all valid pruning techniques that can be straightforward to apply. Here is a function that does just that:
import re def clean_text(text): # Remove URLs text = re.sub(r&#39;http\\S+&#39;, &#39;&#39;, text) # Remove email addresses text = re.sub(r&#39;\\S+@\\S+&#39;, &#39;&#39;, text) # Remove everything that&#39;s not a letter (a-z, A-Z) text = re.sub(r&#39;[^a-zA-Z\\s]&#39;, &#39;&#39;, text) # Remove whitespace, tabs, and new lines text = &#39;&#39;.join(text.split()) return text What measure of length reduction might we be able to get from this additional processing? Applying these techniques to the remaining characters of Tech Leader Docs results in just 75,217 characters; an overall reduction of about 28% from the original text.
More opinionated pruning, such as removing short words or specific words or phrases, can be tailored to a specific use case. These don&rsquo;t lend themselves well to general functions, however.
Now that you have some text processing techniques in your toolkit, let&rsquo;s look at how a reduction in characters translates to fewer tokens used when it comes to ChatGPT. To understand this, we&rsquo;ll examine Byte-Pair Encoding.
Byte-Pair Encoding (BPE) Byte-Pair Encoding (BPE) is a subword tokenization method. It was originally introduced for data compression but has since been adapted for tokenization in NLP tasks. It allows representing common words as tokens and splits more rare words into subword units. This enables a balance between character-level and word-level tokenization.
Let&rsquo;s make that more concrete. Imagine you have a big box of LEGO bricks, and each brick represents a single letter or character. You&rsquo;re tasked with building words using these LEGO bricks. At first, you might start by connecting individual bricks to form words. But over time, you notice that certain combinations of bricks (or characters) keep appearing together frequently, like &ldquo;th&rdquo; in &ldquo;the&rdquo; or &ldquo;ing&rdquo; in &ldquo;running.&rdquo;
BPE is like a smart LEGO-building buddy who suggests, &ldquo;Hey, since &rsquo;th&rsquo; and &lsquo;ing&rsquo; keep appearing together a lot, why don&rsquo;t we glue them together and treat them as a single piece?&rdquo; This way, the next time you want to build a word with &ldquo;the&rdquo; or &ldquo;running,&rdquo; you can use these glued-together pieces, making the process faster and more efficient.
Colloquially, the BPE algorithm looks like this:
Start with single characters. Observe which pairs of characters frequently appear together. Merge those frequent pairs together to treat them as one unit. Repeat this process until you have a mix of single characters and frequently occurring character combinations. BPE is a particularly powerful tokenization method, especially when dealing with diverse and extensive vocabularies. Here&rsquo;s why:
Handling rare words: Traditional tokenization methods might stumble upon rare or out-of-vocabulary words. BPE, with its ability to break words down into frequent subword units, can represent these words without needing to have seen them before. Efficiency: By representing frequent word parts as single tokens, BPE can compress text more effectively. This is especially useful for models like ChatGPT, where token limits apply. Adaptability: BPE is language-agnostic. It doesn&rsquo;t rely on predefined dictionaries or vocabularies. Instead, it learns from the data, making it adaptable to various languages and contexts. In essence, BPE strikes a balance, offering the granularity of character-level tokenization and the context-awareness of word-level tokenization. This hybrid approach ensures that NLP models like ChatGPT can understand a wide range of texts while maintaining computational efficiency.
Sending lots of text to ChatGPT At time of writing, a message to ChatGPT via its web interface has a maximum token length of 4,096 tokens. If we assume the prior mentioned percent reduction as an average, this means you could reduce text of up to 5,712 tokens down to the appropriate size with just text preprocessing.
What about when this isn&rsquo;t enough? Beyond text preprocessing, larger input can be sent in chunks using the OpenAI API. In my next post, I&rsquo;ll show you how to build a Python module that does exactly that.
`,url:"https://victoria.dev/posts/optimizing-text-for-chatgpt-nlp-and-text-pre-processing-techniques/"},"https://victoria.dev/tags/ci/cd/":{title:"Ci/Cd",tags:[],content:"",url:"https://victoria.dev/tags/ci/cd/"},"https://victoria.dev/tags/git/":{title:"Git",tags:[],content:"",url:"https://victoria.dev/tags/git/"},"https://victoria.dev/posts/git-branching-for-small-teams/":{title:"Git branching for small teams",tags:["git","ci/cd","leadership"],content:`Here&rsquo;s a practice I use personally and encourage within my open source projects and any small teams I run for work. I&rsquo;ve seen major elements of it presented under a few different names: Short-Lived Feature Branch flow, GitHub flow (not to be confused with GitFlow), and Feature Branch Workflow are some. Having implemented features I like from all of these with different teams over the years, I&rsquo;ll describe the resulting process that I&rsquo;ve found works best for small teams of about 5-12 people.
A protected main branch To support continuous delivery, no human should have direct push permissions on your master branch. If you develop on GitHub, the latest tag of this branch gets deployed when you create a release &ndash; which is hopefully very often, and very automated.
One issue, one branch, one PR You&rsquo;re already doing a great job of tracking future features and current bugs as issues (right?). To take a quick aside, an issue is a well-defined piece of work that can be merged to the main branch and deployed without breaking anything. It could be a new piece of functionality, a button component update, or a bug fix.
Author&#39;s illustration of issue branches and releases from master. A short-lived branch-per-issue helps ensure that its resulting pull request doesn&rsquo;t get too large, making it unwieldy and hard to review carefully. The definition of &ldquo;short&rdquo; varies depending on the team or project&rsquo;s development velocity: for a small team producing a commercial app (like a startup), the time from issue branch creation to PR probably won&rsquo;t exceed a week. For open source projects like the OWASP WSTG that depends on volunteers working around busy schedules, branches may live for a few weeks to a few months, depending on the contributor. Generally, strive to iterate in as little time as possible.
Here&rsquo;s what this looks like practically. For an issue named (#28) Add user settings page, check out a new branch from master:
# Get all the latest work locally git checkout master git pull # Start your new branch from master git checkout -b 28/add-settings-page Work on the issue, and periodically merge master to fix and avoid other conflicts:
# Commit to your issue branch git commit ... # Get the latest work on master git checkout master git pull # Return to your issue branch and merge in master git checkout 28/add-settings-page git merge master You may prefer to use rebasing instead of merging in master. This happens to be my personal preference as well, however, I&rsquo;ve found that people generally seem to have a harder time wrapping their heads around how rebasing works than they do with merging. Interactive rebasing can easily introduce confusing errors, and rewriting history can be confusing to begin with. Since I&rsquo;m all about reducing cognitive load in developers&rsquo; processes in general, I recommend using a merge strategy.
When the issue work is ready to PR, open the request against master. Automated tests run. Teammates review the work (using inline comments and suggestions if you&rsquo;re on GitHub). Depending on the project, you may deploy a preview version as well.
Once everything checks out, the PR is merged, the issue is closed, and the branch is deleted.
Keep it clean Some common pitfalls I&rsquo;ve seen that can undermine this flow are:
Creating feature branches off of other feature/issue branches. This is a result of poor organization and prioritization. To avoid confusing conflicts and dependencies, always branch off the most up-to-date master. Letting the issue branch live just a little longer. This results in scope creep and huge, confusing PRs that take a lot of time and mental effort to review. Keep branches tightly scoped to the one issue they&rsquo;re meant to close. Not deleting merged branches. There&rsquo;s no reason to leave them about &ndash; all the work is in master. Not removing branches that are stale or have already been merged can cause confusion and make it more difficult than necessary to differentiate new ones. If this sounds like a process you&rsquo;d use, or if you have anything to add, let me know via Webmention!
`,url:"https://victoria.dev/posts/git-branching-for-small-teams/"},"https://victoria.dev/archive/introducing-the-tech-leader-docs/":{title:"Introducing The Tech Leader Docs",tags:["leadership"],content:`I&rsquo;m launching a brand new paid newsletter on Substack focused on building, growing, and leading your technology teams to success. It’s a short, no-time-wasted bi-weekly newsletter that will give you immediately applicable skills and strategies you can take to work that day.
Here are a few things past colleagues have said about my work in software engineering leadership:
&quot;&hellip;the level of organization and process we had was amazing and I personally want to duplicate as much of it as possible!&quot;
&quot;&hellip;remember how excited I was in my interview with you Victoria about how well-organized [your previous company] was and how well-thought-out your processes were? I dare say 99.99% of startups, and many larger companies aren&rsquo;t very organized and it becomes a pain point as they grow. Between the docs and the other processes you had in place, I think you could write a really good book that would be a &lsquo;Blueprint&rsquo; for forming a technical team.&quot;
&quot;@Victoria Drake you publish a book and I will buy at least a dozen copies to hand out.&quot;
Instead of waiting to collect all this information in book form, the first post goes out the first week of January. Here&rsquo;s a preview of some of the skills you’ll learn in future editions:
How to set up remote and asynchronous work to be super-efficient How to remove the right restrictions to make your processes more productive Setting up safeguards to ensure progress doesn&rsquo;t slow down when you take time off Hiring for the culture you want Creating the number one secret weapon that makes everyone on your team more productive These insights are for senior engineers and engineering managers, as well as anyone who wants to start establishing yourself as a leader on your team right away.
You can subscribe monthly, or lock in an early-bird New Year Special subscription for 30% off before Jan 1.
My website Victoria.dev isn’t going anywhere, I&rsquo;ve just decided to put more effort into this new resource. The paid newsletter format strikes a good balance between the immediate delivery of information, skin-in-the-game for you to implement these ideas, and motivation for me to keep sharing how I acquired the skills and knowledge that live in my head, just as I’ve done for years on my blog.
I&rsquo;ve held all kinds of roles on engineering teams, from contract developer to Director of Engineering. Over the years I&rsquo;ve heard from past colleagues and peers about confusing processes, time-wasting meetings, and poor leadership in companies of all sizes. With The Tech Leader Docs, I hope to help those in positions to create positive change in their organization (including you!) to turn that feedback around.
Subscribe today and lock in 30% off. You’ll start 2022 with the practical skills it takes to build a successful engineering team: smarter strategies, less toil, and happier and more productive developers.
`,url:"https://victoria.dev/archive/introducing-the-tech-leader-docs/"},"https://victoria.dev/tags/life/":{title:"Life",tags:[],content:"",url:"https://victoria.dev/tags/life/"},"https://victoria.dev/posts/my-paper-to-do-strategy/":{title:"My paper to-do strategy",tags:["docs","life"],content:`Coding up a to-do app may be the Hello, World of every framework, but when it comes to actually tracking tasks effectively (knock &rsquo;em out not stack &rsquo;em up) there&rsquo;s no app that keeps things front of mind better than an open notebook on your desk.
Here&rsquo;s my stupid-simple strategy for tracking and checking off my to-do list.
One page at a time Plenty of methodologies recommend using sections or different pages of your book for monthly, weekly, and daily views; others advocate for creating sections for each category, such as &ldquo;Home Tasks&rdquo; and &ldquo;Work Tasks&rdquo; and other such time-wasters. All of this is unnecessary.
A to-do list works because it&rsquo;s in your face and hard to miss. When you write things down on different pages, they become easy to miss. Don&rsquo;t do that.
Use one page at a time. Write down one task under another. Don&rsquo;t sort them, prioritize them (yet), or categorize anything. Just write them down on the current page, where you&rsquo;re guaranteed to look when you lay eyes on your notebook next.
Intuitive notation I use my notebook for two things: short notes (just a bit of information &ndash; nothing to do) and tasks (something to do). This translates to a notation system of three possible states:
It&rsquo;s a note, indicated with a bullet point It&rsquo;s a new task, indicated with a checkbox It&rsquo;s a completed task, with the checkbox checked and the line struck out (because strike-throughs are satisfying) I use a checkbox to distinguish tasks from notes because I&rsquo;m an old-school HTML fan, but you do you.
You may like to add your own embellishments to this: I sometimes denote an urgent item with an asterisk. You might like to use a color pen or highlighter (avoid the bullet journal rabbit hole &ndash; another time-waster). Just keep it simple, repeatable, and intuitive.
When it&rsquo;s time to turn the page When life gets busy, you might fill up a page pretty quickly. If one or two tasks haven&rsquo;t yet been crossed off, they&rsquo;re liable to be forgotten. You can avoid this by carrying tasks over to the next page.
It&rsquo;s straightforward: cross out the task on the page that&rsquo;s filled up. Turn the page and write it down there again.
That&rsquo;s silly, you might say, that&rsquo;s a waste of energy! By the time I write it down all over again, I could&rsquo;ve done half of it already.
&hellip;
I&rsquo;ll wait.
&hellip;
The clever bit about carrying a task over is taking the opportunity to evaluate it. If the task is really a five-minute thing, more often than not, I go ahead and take care of it right there and then. If it&rsquo;s a longer endeavor, the friction of writing it down again gives me the chance to answer the question of whether it&rsquo;s something I feel strongly about doing (and hence whether it&rsquo;s really important that I do it at all). It might not be, and that&rsquo;s fine. I cross it out and don&rsquo;t do it. If it is an important task, carrying it over means it remains front of mind until I can make the time to get it done.
Time well spent doing I&rsquo;ve explored a myriad of task list apps, pre-printed to-do lists and journals, and all kinds of digital notes for tracking work. I consistently keep returning to the feel of pen on paper and an open notebook on my desk. Why? Minimal cognitive load.
No time spent categorizing and labeling tasks in a complicated system. No time spent remembering how to open that app, where you stored that todo.txt file, or deciding whether to write something down under your weekly or daily plan. No tasks lost in an invisible backlog that grows over the years, becoming more and more infeasible.
Just pen and paper, one page at a time, and the satisfaction of getting things done.
`,url:"https://victoria.dev/posts/my-paper-to-do-strategy/"},"https://victoria.dev/tags/aws/":{title:"Aws",tags:[],content:"",url:"https://victoria.dev/tags/aws/"},"https://victoria.dev/tags/cybersecurity/":{title:"Cybersecurity",tags:[],content:"",url:"https://victoria.dev/tags/cybersecurity/"},"https://victoria.dev/tags/open-source/":{title:"Open-Source",tags:[],content:"",url:"https://victoria.dev/tags/open-source/"},"https://victoria.dev/tags/privacy/":{title:"Privacy",tags:[],content:"",url:"https://victoria.dev/tags/privacy/"},"https://victoria.dev/archive/set-up-a-pi-hole-vpn-on-an-aws-lightsail-instance/":{title:"Set up a Pi-hole VPN on an AWS Lightsail instance",tags:["privacy","aws","cybersecurity","open-source"],content:`I&rsquo;ve written a fair bit in the past about the whys of online privacy, and a lot about staying safe online. Chances are, if a search brought you here, you&rsquo;re well-past why. Let&rsquo;s go straight on to how.
This guide will walk you through setting up Pi-hole on an AWS Lightsail instance that acts as your VPN thanks to OpenVPN. It&rsquo;s a more succinct version of the official Pi-hole docs for OpenVPN, made specifically for Lightsail with a few tips and tricks added in, because you deserve it.
Create and connect to a Lightsail instance Log in or sign up to AWS and create a Lightsail Instance.
Under Select a platform, choose Linux/Unix.
Under Select a blueprint, choose the OS Only button.
Select the latest officially supported Ubuntu server.
You can save a tidbit of effort by putting the following into the Launch script box:
# Update installed packages sudo apt-get update sudo apt-get upgrade -y Create a new SSH key for this server and ensure you download the .pem.
Choose your plan. The $3.50 USD instance is sufficient.
Give it a name then click Create instance.
Stare eagerly at the page until the instance status is Running, then go to the Networking tab.
Create a Static IP and attach it to your new instance. Remember that static IP addresses are free only while attached to an instance.
Click on your instance name to return to its dashboard. Go back to the Networking tab. It&rsquo;ll look a bit different now.
Under IPv6 networking, click the toggle to turn it off (unless you know what you are doing and you want IPv6 for some reason. Most of y&rsquo;all don&rsquo;t need it).
Under IPv4 Firewall, delete the rule for HTTP.
Click Add rule. In the Application dropdown, choose Custom.
For Protocol, choose UDP. In the Port or range input, enter a UDP port for the OpenVPN server to run on. (It&rsquo;s typically 1194, which you can choose to use, but you might like a different number for security purposes. Port range is 0-65535.) Connect using SSH and your new key pair, either in your terminal or on the Connect tab with the browser-based client.
Install OpenVPN on your server After connecting to your server using SSH, install OpenVPN on your server.
# Download OpenVPN wget https://git.io/vpn -O openvpn-install.sh chmod 755 openvpn-install.sh sudo ./openvpn-install.sh You&rsquo;ll see:
Welcome to this OpenVPN road warrior installer! This server is behind NAT. What is the public IPv4 address or hostname? Public IPv4 address / hostname [x.xx.xxx.xxx]: &hellip;where the default option is your static IP that you set up earlier. Hit return to accept this. Then:
Which protocol should OpenVPN use? 1) UDP (recommended) 2) TCP Protocol [1]: 1 Choose 1 or hit return. Then:
What port should OpenVPN listen to? Port [1194]: ##### Enter the UDP port number you chose earlier. Then:
Select a DNS server for the clients: 1) Current system resolvers 2) Google 3) 1.1.1.1 4) OpenDNS 5) Quad9 6) AdGuard DNS server [1]: 1 Choose 1 or hit return. Then:
Enter a name for the first client: Name [client]: pihole The Pi-hole will be the client. Name it as you like then Press any key to continue...
OpenVPN will set itself up. Confirm that tun0 has the interface address 10.8.0.1/24 with the following command:
ip addr show tun0 This ensures that the Pi-hole will be set up properly. Now, about that:
Install and configure Pi-hole On your Lightsail instance, install Pi-hole.
# Download and install Pi-hole curl -sSL https://install.pi-hole.net | bash This runs the Pi-hole automated installer. You&rsquo;ll see some prompts which you can answer using the enter key, arrow keys, tab, and space bar for selecting an option.
The important things:
When you see Choose An Interface, ensure you pick tun0. It isn&rsquo;t the default selection. You&rsquo;ll need to set the IPv4 address to the interface address you viewed previously using the ip addr command: 10.8.0.1/24. This ensures the Pi-hole uses the VPN. At time of writing, the second item above wasn&rsquo;t presented as an option in the automated installer. After the Pi-hole installer finishes, manually change the IP address by editing the configuration file:
&gt; sudo vim /etc/pihole/setupVars.conf
Change the IPV4_ADDRESS to 10.8.0.1/24 and save the file. Restart the Pi-hole with: pihole restartdns.
If you mess up, you can redo the configuration with pihole reconfigure.
Finally, you&rsquo;ll configure the VPN to use the Pi-hole.
Configure OpenVPN Confirm the address of the tun0 interface with:
ip a | grep -C 1 &#39;tun0&#39; You should see: inet 10.8.0.1/24 in there.
Edit the OpenVPN config file with:
sudo vim /etc/openvpn/server/server.conf Change the line that starts with push &quot;dhcp-option&hellip; to use the Pi-hole&rsquo;s IP address that you confirmed above:
push &#34;dhcp-option DNS 10.8.0.1&#34; If any other lines start with push &quot;dhcp-option&hellip;, comment those out.
If you want to log OpenVPN traffic, add these lines to the end of the file:
log /var/log/openvpn.log verb 3 Save the config. If you forgot to open Vim with sudo, use the tee trick: :w !sudo tee %, then O, then :q!.
Restart OpenVPN with sudo systemctl restart openvpn-server@server.
Configure firewall Run the following to control traffic to the server as described here.
sudo iptables -I INPUT -i tun0 -j ACCEPT sudo iptables -A INPUT -i tun0 -p tcp --destination-port 53 -j ACCEPT sudo iptables -A INPUT -i tun0 -p udp --destination-port 53 -j ACCEPT sudo iptables -A INPUT -i tun0 -p tcp --destination-port 80 -j ACCEPT sudo iptables -A INPUT -p tcp --destination-port 22 -j ACCEPT sudo iptables -A INPUT -p tcp --destination-port 1194 -j ACCEPT sudo iptables -A INPUT -p udp --destination-port 1194 -j ACCEPT sudo iptables -I INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT sudo iptables -I INPUT -i lo -j ACCEPT sudo iptables -P INPUT DROP # Optionally, also block HTTPS advertisements while you&#39;re here. sudo iptables -A INPUT -p udp --dport 80 -j REJECT --reject-with icmp-port-unreachable sudo iptables -A INPUT -p tcp --dport 443 -j REJECT --reject-with tcp-reset sudo iptables -A INPUT -p udp --dport 443 -j REJECT --reject-with icmp-port-unreachable You can review the results with sudo iptables -L --line-numbers.
These are only stored in memory before you save them, so test out your set up on your client now to see if it all works as expected.
Test your client connection To test your configuration, try adding a client (the phone or computer that will connect to the VPN).
Run the OpenVPN script again: sudo ./openvpn-install.sh and choose 1) Add a new client. Give it a name; you may find it helps to name it by the device, e.g. &ldquo;phone&rdquo;. This creates a file that ends in .ovpn. You need to place this file on your client to use it. Install the appropriate OpenVPN app for your device. Transfer the .ovpn file you just obtained to the device if you haven&rsquo;t already. (See future tasks for a way to copy the file to your host machine.) Follow instructions in your app (try under FAQ) for importing the .ovpn file and activating the VPN. Ensure it seems to connect properly. If you go to DuckDuckGo.com and search for &ldquo;What&rsquo;s my IP&rdquo;, you should see the location of your Lightsail instance. For a more in-depth test, check for DNS leaks at BrowserLeaks.com. Try browsing for a while. You can also view the Pi-hole dashboard by visiting http://pi.hole/admin/ on this device.
If everything seems all right, go on to saving the configuration on your instance.
Save iptables Save the iptables you created earlier using the tee command to achieve the second permission.
sudo iptables-save | sudo tee /etc/pihole/rules.v4 You&rsquo;re finished with configuration on your Lightsail instance. If you wish to disconnect now, you can just type exit.
Future tasks You&rsquo;re done with the set up! You now have your very own personal VPN with a Pi-hole keeping you safe from nasty trackers. Here are some references for operations you might like to come back to in the future:
Reconnect to your Lightsail instance with SSH: ssh -i /path/to/private-key.pem ubuntu@public-ip-address Set a password for the web interface dashboard: pihole -a -p Access the web interface dashboard: Connect to the VPN, then visit http://pi.hole/admin/ Update the Pi-hole: pihole -up Add a new client (for iOS, Linux, or Windows, or for Android) Copy the .ovpn file for a client to your host machine (run on the host machine): ssh -i /path/to/private-key.pem ubuntu@public-ip-address 'sudo cat /path/on/lightsail/client.ovpn' &gt; /path/on/host/client.ovpn Beef up that block list! Here&rsquo;s my favorite resource for updating your Pi-hole adlist table: The Big Blocklist Collection Enjoy your new, more secure and peaceful Internet! If you found this guide helpful, please share it with someone else.
`,url:"https://victoria.dev/archive/set-up-a-pi-hole-vpn-on-an-aws-lightsail-instance/"},"https://victoria.dev/posts/measuring-productivity-with-github-issues/":{title:"Measuring productivity with GitHub issues",tags:["data","leadership","python","api"],content:`How long does it take for a bug to get squashed, or for a pull request to be merged? What kind of issues take the longest to close?
Most organizations want to improve productivity and output, but few technical teams seem to take a data-driven approach to discovering productivity bottlenecks. If you&rsquo;re looking to improve development velocity, a couple key metrics could help your team get unblocked. Here&rsquo;s how you can apply a smidge of data science to visualize how your repository is doing, and where improvements can be made.
Getting quality data The first and most difficult part, as any data scientist would likely tell you, is ensuring the quality of your data. It&rsquo;s especially important to consider consistency: are dates throughout the dataset presented in a consistent format? Have tags or labels been applied under consistent rules? Does the dataset contain repeated values, empty values, or unmatched types?
If your repository has previously changed up processes or standards, consider the timeframe of the data you collect. If labeling issues is done arbitrarily, those may not be a useful feature. While cleaning data is outside the scope of this article, I can, at least, help you painlessly collect it.
I wrote a straightforward Python utility that uses the GitHub API to pull data for any repository. You can use this on the command line and output the data to a file. It uses the list repository issues endpoint (docs), which, perhaps confusingly, includes both issues and pull requests (PRs) for the repository. I get my data like this:
$ python fetch.py -h usage: fetch.py [-h] [--token TOKEN] repository months $ python fetch.py OWASP/wstg 24 &gt; data.json Using the GitHub API means less worry about standardization, for example, all the dates are expressed as ISO 8601. Now that you have some data to process, it&rsquo;s time to play with Pandas.
Plotting with Pandas You can use a Jupyter Notebook to do some simple calculations and data visualization.
First, create the Notebook file:
touch stats.ipynb Open the file in your favorite IDE, or in your browser by running jupyter notebook.
In the first code cell, import Pandas and load your data:
import pandas as pd data = pd.read_json(&#34;data.json&#34;) data You can then run that cell to see a preview of the data you collected.
Pandas is a well-documented data analysis library. With a little imagination and a few keyword searches, you can begin to measure all kinds of repository metrics. For this walk-through, here&rsquo;s how you can calculate and create a graph that shows the number of days an issue or PR remains open in your repository.
Create a new code cell and, for each item in your Series, subtract the date it was closed from the date it was created:
duration = pd.Series(data.closed_at - data.created_at) duration.describe() Series.describe() will give you some summary statistics that look something like these (from mypy on GitHub):
count 514 mean 5 days 08:04:17.239299610 std 14 days 12:04:22.979308668 min 0 days 00:00:09 25% 0 days 00:47:46.250000 50% 0 days 06:18:47 75% 2 days 20:22:49.250000 max 102 days 20:56:30 Series.plot() uses a specified plotting backend (matplotlib by default) to visualize your data. A histogram can be a helpful way to examine issue duration:
duration.apply(lambda x: x.days).plot(kind=&#34;hist&#34;) This will plot a histogram that represents the frequency distribution of issues over days, which is one way you can tell how long most issues take to close. For example, mypy seems to handle the majority of issues and PRs within 10 days, with some outliers taking more than three months:
It would be interesting to visualize other repository data, such as its most frequent contributors, or most often used labels. Does a relationship exist between the author or reviewers of an issue and how quickly it is resolved? Does the presence of particular labels predict anything about the duration of the issue?
You aim for what you measure Now that you have some data-driven superpowers, remember that it comes with great responsibility. Deciding what to measure is just as, if not more, important than measuring it.
Consider how to translate the numbers you gather into productivity improvements. For example, if your metric is closing issues and PRs faster, what actions can you take to encourage the right behavior in your teams? I&rsquo;d suggest encouraging issues to be clearly defined, and pull requests to be small and have a well-contained scope, making them easier to understand and review.
To prepare to accurately take measurements for your repository, establish consistent standards for labels, tags, milestones, and other features you might want to examine. Remember that meaningful results are more easily gleaned from higher quality data.
Finally, have fun exercising your data science skills. Who knows what you can discover and improve upon next!
`,url:"https://victoria.dev/posts/measuring-productivity-with-github-issues/"},"https://victoria.dev/tags/hardware/":{title:"Hardware",tags:[],content:"",url:"https://victoria.dev/tags/hardware/"},"https://victoria.dev/tags/linux/":{title:"Linux",tags:[],content:"",url:"https://victoria.dev/tags/linux/"},"https://victoria.dev/archive/there-are-better-options-for-a-privacy-respecting-phone/":{title:"There are better options for a privacy-respecting phone",tags:["privacy","linux","open-source","hardware"],content:`Whether you think the news of Apple scanning your private devices was a big deal, run-of-the-mill, or something we all should have seen coming, you might be wondering, &ldquo;What now?&rdquo; We know full well that Google is looking at the stuff on your phone too (and Gmail, and&hellip; well, everywhere else) so it’s not like there are other options after Apple&hellip; right?
If a move towards privacy is what we’re after, we know a new off-the-shelf Google phone isn&rsquo;t a better answer &ndash; but there are more options.
If you don&rsquo;t want the details, jump straight to The TL;DR at the end.
Linux phones (sort of) Unless you’re a rather tolerant tech-savvy tinkerer, a Linux phone isn’t one of these options&hellip; yet. I’ve personally been very excited about the bevy of emerging options in this space, from freedom-oriented hardware to fully open source, crowd-developed operating systems.
The current state of these efforts is that this magical mashup just isn’t ready yet. Most Linux phone OS such as Ubuntu Touch, Mobian, Pure OS, etc, are in a &ldquo;mostly working&rdquo; state, with the missing features ranging from &ldquo;lack of reliable push notifications&rdquo; to &ldquo;intermittent Bluetooth connectivity&rdquo; to &ldquo;camera.&rdquo;
If all you need is text messaging and a web browser, yes, you can probably go this route. For most users however, this isn’t going to make daily-driver status.
If a Linux phone would suit you, I recommend getting your hands on a PinePhone and running Arch Linux ARM (releases on GitHub) with Plasma Mobile.
De-googled Android For a daily-driver, &ldquo;de-googled&rdquo; Android is your best bet. Android itself (specifically, the Android Open Source Project source code) is based on a modified Linux kernel and is free and open source software. When we typically think of &ldquo;Android phones,&rdquo; we refer to Android devices with Google&rsquo;s proprietary software added to the mix, including Google Play Services. A &ldquo;de-googled&rdquo; Android phone is essentially the Android OS without Google&rsquo;s spyware services included by default.
Keep in mind that this route still involves some DIY. You&rsquo;ll need to install an OS on a device yourself. Don&rsquo;t worry, there are step-by-step guides available &ndash; the most technical thing you&rsquo;ll likely have to do is copy and paste some commands into your terminal.
Free and open source Android OS comes in multiple flavors, and the choice isn&rsquo;t arbitrary. Your selection of a &ldquo;de-googled&rdquo; phone is going to be determined by a couple factors: the hardware device you have or that you want to use, and the apps (software) you want to run on it.
Hardware The phone you may already have (or the one you’re willing to purchase) will influence your choice of operating system (OS).
LineageOS At the time I’m writing this, if you have an older Pixel or another model of Android phone, your best bet for a hassle-free OS with A-class support will be Lineage. Here’s a link to the LineageOS list of supported devices. Clicking on your device here will get you to some installation instructions for your phone.
GrapheneOS If you have a newer Pixel (generation 3 up to the newer 5) then GrapheneOS could be the way to go. Here are the devices officially supported by GrapheneOS. They also have easy-to-follow installation instructions and help via chat. It is possible to run GrapheneOS on other phones, but not without substantial DIY for which technical knowledge would help.
Generally speaking, GrapheneOS is intended to be a security-hardened operating system targeted at individuals who won&rsquo;t be miffed if there are tradeoffs for mitigating vulnerabilities. If you don&rsquo;t have those requirements or intend to use Google Apps on your phone (see Software), then LineageOS will likely suit you better.
New phone, who dis? If you&rsquo;re looking to purchase a new phone, you have some flexibility. My general recommendation is to pick up last-season&rsquo;s version of the model you want. Not only will this likely be cheaper (and often a great deal if you buy refurbished) but the open source community that develops these operating systems will have had more time to work with the device itself, which could help ensure better compatibility and a smoother set up.
Consider buying a refurbished phone (sometimes called &ldquo;renewed&rdquo;) locally when you can. This can help fund the small businesses that offer them.
Software What do you need to do on your phone? Privacy and convenience are typically at odds (a far larger topic I won’t dig into right now) so it can help to narrow down the functionality you need. If your needs look something like:
Calls and texts Web browser Web-based email via browser Then you&rsquo;re good to go, right out of the box, with either LineageOS or GrapheneOS. They&rsquo;ll both include free and open source apps that let you do all these things.
If you want a particular application that doesn&rsquo;t come pre-installed, here&rsquo;s where we get into some nuance. Your choices depend on the level of privacy you&rsquo;d like to maintain. Here are your avenues for installing apps, listed in order of preference.
1. Official APKs Some particularly privacy-focused applications offer an Android Package Kit (APK) that you can download directly in order to install the app. You should only download these when you&rsquo;ve navigated directly to a domain that the organization owns. Here are my favorites:
Signal ProtonMail You can download and install APKs whether you choose LineageOS or GrapheneOS.
2. Use F-Droid If you can&rsquo;t find an APK for something you want, search for it on F-Droid.
The F-Droid software repository allows you to download and install apps in much the same way that the Google Play store does, with a couple notable differences. All the apps here are free and open source, and no account or profile is required to download them. The F-Droid APK itself can be downloaded and installed from f-droid.org directly on either LineageOS or GrapheneOS.
Just like any open source software, it&rsquo;s up to the user (you) to ensure that you&rsquo;re downloading and installing software you trust. If you want help or advice, F-Droid has a healthy community that you can interact with in lots of ways, including via IRC, Matrix, and the Fediverse.
You can find an app for pretty much anything here: from your general-store type functions such as to-do lists, music players, and maps; to specific niche security applications, and even a tea timer. Here are some well-known choices I can easily recommend:
Standard Notes (https://standardnotes.org/) Element for Matrix (https://element.io/) Tutanota (https://tutanota.com) 3. Aurora Store If you need an app that isn&rsquo;t available on F-Droid, your next stop is the Aurora Store. This is an unofficial client for the Google Play Store that lets you download free applications anonymously, without signing into a Google account. Most applications found in the larger stores can be downloaded this way, without requiring Google&rsquo;s proprietary stuff on your phone.
When loading Aurora Store for the first time, be sure to choose the &ldquo;Anonymous&rdquo; option instead of signing in.
The Aurora Store itself can be installed via F-Droid or auroraoss.com. It works on either LineageOS or GrapheneOS &ndash; however, apps that require less private permissions or access will probably work better on LineageOS.
Keep in mind that your phone OS in no way supports these apps directly, or knows what&rsquo;s in them, or what sort of tracking and information exchange they may be up to. It&rsquo;s a slight privacy downgrade, but still better than a fully Google-ified OS.
4. If you need Google Apps If this will be your only phone and you simply must have Google Apps on it (think Google Play Store, Gmail, Calendar, Photos, etc) then go with LineageOS. You can choose to try emulating Google Play Services using LineageOS for microG, or install the Google Apps add-on when you install LineageOS.
The TL;DR Here&rsquo;s the &ldquo;Internet personality quiz&rdquo; version of everything above. You are&hellip;
Knowledgeable about Linux; mostly use a phone for text, calls, and web browser; and potentially want to help develop Linux phone software. Try a Linux phone such as the PinePhone, but consider one of the other options as a back up for when you just need stuff to work. Security or privacy inclined, happy to use FOSS apps, or do most things via web browser anyway. Get your hands on a Pixel 3{XL, a, a XL}, Pixel 4{XL, a, a 5G}, or Pixel 5, and use GrapheneOS. Installation instructions here. Optionally, download the F-Droid or Aurora Store APKs for apps. Someone who needs Google Apps to work, or you want a phone that isn&rsquo;t a Pixel, or you&rsquo;re setting up a device for someone who&rsquo;s fine using Android but needs it to look familiar. Use LineageOS with any of its supported devices. Click on the device name for installation instructions. If you must have Google Apps and need Google Play Services to work, install the add-on at the same time you install LineageOS. Optionally, download the F-Droid or Aurora Store for installing apps. Whichever route you choose, my advice is to treat this like a learning experiment. You&rsquo;re sort of building your own phone, after all, and gaining all the technological independence that comes with that knowledge. If possible, don&rsquo;t ditch your current phone until you try out one (two?) of these paths. The one you end up liking most could surprise you! It&rsquo;s great to have options.
`,url:"https://victoria.dev/archive/there-are-better-options-for-a-privacy-respecting-phone/"},"https://victoria.dev/posts/building-in-context/":{title:"Building in context",tags:["leadership","computing"],content:`It&rsquo;s a comedy classic &ndash; you&rsquo;ve got a grand idea. Maybe you want to build a beautiful new dining room table. You spend hours researching woodcraft, learn about types of wood and varnish, explore different styles of construction, and now you have a solid plan. You buy the wood and other materials. You set up in the garage. For months you measure and saw, sand, hammer and paint. Finally, the effort is worthwhile. The table is finished, and it&rsquo;s fantastic.
In a frenzy of accomplishment you drag it into the house &ndash; only to discover that your dining room doorway is several inches too small. It doesn&rsquo;t fit.
Art only imitates life, so you may say that this comedic example is unrealistic. Of course an experienced DIY-er would have measured the doorway first. In real life, however, unforeseen hindrances rarely come in ones: once you get the table in the door, you discover the floor&rsquo;s uneven. Perhaps the chairs you&rsquo;ve chosen are a few inches too short&hellip; and so on.
Far from attempting to persuade you away from your next DIY project, I&rsquo;d like to help make those and any other projects you take on go even smoother. The same patterns are found in furniture-building as in software development: it&rsquo;s always better to build in context.
The planning fallacy Few software developers are accurate when it comes to time and cost estimates. This isn&rsquo;t a failing of software engineers, but a human tendency to towards optimism when it comes to predicting your own future. First proposed by Daniel Kahneman and Amos Tversky in 1979, the planning fallacy is no new concept.
In one study, students were asked to estimate how long they would take to finish their senior theses. The estimates, an average 27.4 days at the optimistic end and 48.6 days at the pessimistic end, came up predictably short. The average actual completion time was 55.5 days.
The study proposed two main hypotheses as to why this happens: first, that people tend to focus on their future plans rather than their past experiences; and second, people don&rsquo;t tend to think that past experiences matter all that much to the future anyway.
You can probably find examples of this in your own life without trying too hard, perhaps couched in the infamous &ldquo;only because&rdquo; envelope. Sure, that last &ldquo;weekend project&rdquo; turned into a two-week affair, but that was only because you had to go run some unexpected errands. Or maybe you didn&rsquo;t finish that blog post when you mean to, but that&rsquo;s only because your siblings dropped in to visit. You&rsquo;re absolutely, positively, definitely certain that next time would be different.
In reality, people are just plain poor at factoring in the unexpected daily demands of life. This makes sense from a certain perspective: if we were good at it, we&rsquo;d probably have a lot more to fret about on a daily basis. Some measure of ignorance can make life a little more blissful.
That said, some measure of accurate planning is also necessary for success. One way we can improve accuracy is to work in context as much as possible.
Context Let&rsquo;s consider the dining room table story again. Instead of spending months out in the garage, what would you do differently to build in context?
You might say, &ldquo;Build it in the dining room!&rdquo; While that would certainly be ideal for context, both in homes and in software development, it&rsquo;s rarely possible (or palatable). Instead, you can do the next best thing: start building, and make frequent visits to context.
Having decided you&rsquo;d like to build a table, one of the first questions is likely, &ldquo;How big will it be?&rdquo; You&rsquo;ll undoubtedly have some requirements to fulfill (must seat six, must match other furniture, must hold the weight of your annual twenty-eight-course Christmas feast, etc.) that will lead you to a rough decision.
With a size in mind, you can then build a mock up. At this point, the specific materials, style, and color don&rsquo;t matter &ndash; only its three dimensions. Once you have your mock table, you now have the ability to make your first trip to the context in which you hope it will ultimately live. Attempting to carry your foam/wood/cardboard/balloon animal mock up into the dining room is highly likely to reveal a number of issues, and possibly new opportunities as well. Perhaps, though you&rsquo;d never have thought it, a modern abstractly-shaped dining table would better compliment the space and requirements. (It worked for the Jetsons.) You can then take this into account in your next higher-fidelity iteration.
This process translates directly to software development, minus the Christmas feast. You may have already recognized the MVP approach; however, even here, putting the MVP in context is a step that&rsquo;s frequently omitted.
Where will your product ultimately live? How will it be accessed? Building your MVP and attempting to deploy it is sure to help uncover lots of little hiccups at an early stage.
Even when teams have prior experience with stacks or technologies, remember the planning fallacy. People have a natural tendency to discount past evidence to the point of forgetting (memory bias). It&rsquo;s also highly unlikely that the same exact team is building the same exact product as the last time. The language, technology, framework, and infrastructure have likely changed in at least some small way &ndash; as have the capabilities and appetites of the engineers on your team. Frequent visits to context can help you run into any issues early on, adapt to them, and create a short feedback loop.
Go for good enough The specific meaning of putting something in context is going to vary from one software project to another. It may mean deployment to cloud infrastructure, running a new bare metal server, or attempting to find out if your office across the ocean can access the same resources you use. In all cases, keep those short iterations going. Don&rsquo;t wait and attempt to get a version to 100% before you find out if it works in context. Send it at 80%, see how close you got, then iterate again.
The concept of building in context can be applied at any stage &ndash; of course, the sooner the better! Try applying this idea to your project guidance today. I&rsquo;d love to hear how it goes.
`,url:"https://victoria.dev/posts/building-in-context/"},"https://victoria.dev/posts/leading-with-a-cybersecurity-mindset/":{title:"Leading with a cybersecurity mindset",tags:["cybersecurity","coding","open-source","ci/cd"],content:`Times and technologies change, but a few good ideas are still the same. With consistent application, a handful of wise practices can help deter a slew of cybersecurity attacks. While implementation differs across applications, learning to lead development teams with a cybersecurity mindset boils down to a few fundamental concepts:
Be a bad guy Fail secure Practice software minimalism A slight change in thinking can create a sea change in security. Let&rsquo;s examine how.
Let&rsquo;s be bad guys When it comes to cybersecurity, I take a pragmatic approach. There aren&rsquo;t enough sheaves of NIST recommendations in the world to help you if you aren&rsquo;t accustomed to thinking like the bad guy. To best lead your team to defend against hacking, first know how to hack yourself.
A perusal of the resources linked at the end of this article can help you with a starting point, as will general consideration of your application through the lens of an outsider. Are there potentially vulnerable forms or endpoints you might examine first? Is there someone at your company you could call on the phone and surreptitiously get helpful information from? Defense is a difficult position to hold in any battle. If you aren&rsquo;t the first person to consider how your application might be attacked, you&rsquo;ve already lost.
Develop your sense of how to be the bad guy. Every component of software, every interaction, every bit of data, can be useful to the bad guy. The more you hone your ability to consider how a thing can be used for ill, the better able you&rsquo;ll be to protect it.
When looking at information, ask, &ldquo;How can I use this information to gain access to more important information?&rdquo; When considering a user story, ask, &ldquo;What happens if I do something unexpected?&rdquo;
In all things, channel your inner four-year-old. Push all the buttons.
Playing offense on your own application lets you fix vulnerabilities before they happen. That&rsquo;s a luxury you won&rsquo;t get from the real bad guys.
Fail secure Every part of a system will fail with 100% certainty on a long enough timescale. Thinking a step ahead can help to ensure that when it does, the one failure doesn&rsquo;t leave your application wide open to others.
To fail secure means that when a system or code fails to perform or does something unexpected, any follow-on effects are halted rather than permitted. This likely takes many forms in many areas of your application, so here are the more common ones I see.
Permissions When gating access, deny by default. This most often takes the form of whitelisting, or colloquially, &ldquo;no one is allowed, except for the people on this list.&rdquo; In terms of code flow, everything should be denied first. Only allow any particular action after proper credentials are verified.
Automation For automated workflows such as deployments, ensure each step is dependent on the last. Don&rsquo;t make the (rather common) mistake of connecting actions to triggers that can kick off a workflow before all the necessary pieces are in place. With the smorgasbord of cloud and CI tools available, failure events may not be obvious or noisy.
Be careful to avoid running flows on timed triggers unless they are completely self-contained. Workflows that unpredictably run faster or slower than expected can throw a whole series of events into disarray, leaving processes half-run and states insecure.
Exception handling Errors are a frequent gold mine for attackers. Ensure your team&rsquo;s code returns &ldquo;pretty&rdquo; errors with content that you can control. &ldquo;Ugly&rdquo; errors, returned by default by databases, frameworks, etc, try to be helpful by providing lots of debugging information that can be extremely helpful to a hacker.
Software minimalism If your development team doesn&rsquo;t currently have one central source of information when it comes to keeping track of all your application components, here&rsquo;s a tip you really need. In software security, less is more (secure).
The more modular an application is, the better its various components can be isolated, protected, or changed out. With a central source of truth for what all those components are (and preferably one that doesn&rsquo;t rely on manual updates), it&rsquo;s easier to ensure that your application is appropriately minimalist. Dependency managers, such as Pipenv, are a great example.
Few industries besides technology seem to have produced as many acronyms. Philosophies like Don&rsquo;t Repeat Yourself (DRY), Keep It Simple Stupid (KISS), You Aren&rsquo;t Going to Need It (YAGNI), and countless other methodologies all build upon one very basic principle: minimalism. It&rsquo;s a principle that warrants incorporation in every aspect of an application.
There&rsquo;s a reason it takes little skill to shoot the broad side of a barn: barns are rather large, and there&rsquo;s quite a lot of one to hit. Applications bloated by excessive third-party components, repeated code, and unnecessary assets make similarly large targets. The more there is to maintain and protect, the easier it is to hit.
Like Marie Kondo&rsquo;s method for dispatching the inevitable creep of household clutter, you can reduce your application&rsquo;s attack surface by considering each component and asking whether it brings you joy. Do all of this component&rsquo;s functions benefit your application? Is there unnecessary redundancy here? Assess each component and decide how integral it is to the application. Every component is a risk; your job is to decide if it&rsquo;s a worthwhile risk.
Bonus: your personal Yodas With the basic principles of learning to think like the bad guy, failing securely, and practicing software minimalism, you&rsquo;re now ready to steep in the specifics. Keeping the fundamentals in mind can help you lead your team to focus your cybersecurity efforts where it matters most.
No Jedi succeeds without a little help from friends. Whether you&rsquo;re a beginner in the battle against the dark side or a twice-returned-home Jedi Master, these resources provide continuing training and guidance.
Open Web Application Security Project National Institute of Standards and Technology (NIST): Cybersecurity OWASP Proactive Controls OWASP Secure Coding Practices OWASP Web Security Testing Guide NIST Special Publication 800-30: Guide for conducting risk assessments NSA’S Cybersecurity Advisories &amp; Technical Guidance I hope you find these thought systems helpful! If you find your interest piqued as well, you can read more of what I&rsquo;ve written about cybersecurity here.
`,url:"https://victoria.dev/posts/leading-with-a-cybersecurity-mindset/"},"https://victoria.dev/posts/a-github-guide-for-non-technical-leaders/":{title:"A GitHub guide for non-technical leaders",tags:["leadership","leadership"],content:`As I write this, the front page of GitHub.com declares in big bold letters that this is &ldquo;Where the world builds software.&rdquo; This is true. In technology companies today, the creation of your product is largely happening where your developers spend time. It&rsquo;s where big and small product decisions are made every day &ndash; the kind of decisions that, wittingly or not, will decide the future of your company.
I&rsquo;m writing this guide for a very specific person &ndash; possibly you, or someone you know. I&rsquo;ll explain how a non-technical business leader can find information and take part in the decisions and questions that happen only on GitHub. You don&rsquo;t need to know how to use Git. You just need a few minutes to follow along, and a desire to be a resource and servant leader for your teams. Let&rsquo;s do it!
If you haven&rsquo;t signed up yet, click below to read the very first steps. Once you&rsquo;re logged in, read on to join in!
The very first steps GitHub comes in two different flavors: GitHub Enterprise, or GitHub.com. If your team uses GitHub.com, you can sign up here using your work email.
With GitHub Enterprise, signing up depends on your individual company&rsquo;s configuration. For example, you may be set up to log in with SAML single sign-on (such as through your GSuite credentials). Get in touch with the folks administering Enterprise in order to get signed up or logged in.
For the rest of this guide, it doesn&rsquo;t matter if you&rsquo;re using GitHub Enterprise or GitHub.com &ndash; they&rsquo;re largely the same. Just ensure you get connected to your company&rsquo;s Organization or Team, if there is one. Someone with administrative privileges needs to invite you using the email you signed up with.
Where the magic happens On GitHub, work is typically grouped by projects or products into what&rsquo;s called repositories. Your team or company may have just one of these that they regularly use (they might call it a &ldquo;monorepo&rdquo;) or several repositories that represent different technical components of a single product.
Once you log in, you&rsquo;ll be on the Recent activity page. You can search for the name of the repository you want to visit in the search bar at the top left. If your company&rsquo;s repositories are private, you may need to be invited by an administrator in order to view it.
When you view a repository, it looks like this. I&rsquo;ve pointed out some of the important bits.
In any repository, there are two main areas where decisions usually take place. These are in Issues and Pull Requests, and you&rsquo;ll mainly focus your attention here. Click on the Issues tab to see these.
You&rsquo;ll be presented with a list of Issues, which you can think of as individual topics. This format is essentially a discussion board. Clicking on any of the Issue titles will take you to its thread.
Here&rsquo;s where the magic happens! Folks on your team use Issues to discuss all kinds of topics. These may be a very technical and esoteric cost-benefit analysis, or a fundamental customer-facing design decision. A quick read of the first message in the thread is likely to reveal whether it&rsquo;s a decision that could use your help.
Issues are a starting point for work. Here, team members make decisions about the type and scope of a change they plan to make.
When someone has their changes ready, they&rsquo;ll open a Pull Request so that other team members can preview and give input on those changes before they become part of the repository. Click the Pull Request tab at the top to view these.
You&rsquo;re presented with a very similar view on this page &ndash; yes, it&rsquo;s another discussion board! You can click on any Pull Request to view its thread as well.
Pull Requests have some additional tabs that team members use for code reviews. All the conversation will show up in the Conversation tab.
Sorting out what&rsquo;s relevant A lot of discussion happens in Issues and Pull Request threads, and not all of it may be relevant for you to look at. Thankfully, GitHub has some excellent collaboration tools that can help your team direct your attention to where it&rsquo;s most needed. These are @mentions and labels.
Ask to be @mentioned These work the same way on GitHub as they do on Twitter. When someone @mentions you, you&rsquo;ll receive a notification via email, or you&rsquo;ll see it in your Notification Center when you&rsquo;re logged in. This depends on your notification settings, which you should adjust to your liking. If you only want to be notified when someone @mentions you or replies to you, you should uncheck everything on the Notification Settings page except for your preferred options under Participating.
Now when someone references you in a discussion on GitHub, you&rsquo;ll be notified and you&rsquo;ll have the chance to respond!
Using labels Another less direct way to see where you can effectively contribute is to ask your team to use labels. You may recall seeing these in the right sidebar of Issue and Pull Request threads:
You can create different labels to categorize a discussion, and you can apply as many labels to a discussion as you like. In order to have your team draw your attention to threads that might benefit from your input or guidance, ask folks to use a label to point these out. This could be the question label, or any new label of your choosing.
Clicking on a label in the sidebar will take you to a page that shows all the Issues or Pull Requests with that label. The URL will look something like:
https://github.com/&lt;organization name&gt;/&lt;repository name&gt;/labels/question You can also bookmark this page to easily check it on a regular schedule. This is a great, low-friction way for your team to indicate areas that could use your input.
Collaborating on files Similar to tracking changes in Google Docs or Word documents, you can edit documents in GitHub in a way that lets your team see the changes you&rsquo;ve made. This is a fantastic method for collaborating with your team where they work, and avoids the hassle of emailing attachments around.
Text files in repositories have extensions such as .md for Markdown, or .txt for plain text. The majority of documentation on GitHub is in Markdown format. Clicking on any file in the repository file list will open it on the GitHub website. At the top right of the document, look for these buttons:
Clicking on the pencil icon will let you edit the file right there in your web browser. You may not see anything special as you&rsquo;re typing, but once you commit (like saving) your file, all your changes are tracked with Git! For a step-by-step guide to editing, see Editing files in your repository.
Here are some helpful articles for formatting text with Markdown on GitHub.
Proactive participation GitHub is well-structured as a collaboration platform. That&rsquo;s why people of all professions use it not just for software development, but also for networking, getting jobs and sponsorships, and even for hosting simple no-code websites. My own company uses GitHub for everything from collaborating on company documentation to automated Change Control Board processes for FedRAMP.
At your leisure, I encourage you to chase your curiosity and explore. Don&rsquo;t be shy about asking questions, or asking technical folks on your team to explain something if you think it will enable you to be a bigger help to them. With so much of the world building software on GitHub, there&rsquo;s a lot you can contribute when you&rsquo;re where the work happens.
`,url:"https://victoria.dev/posts/a-github-guide-for-non-technical-leaders/"},"https://victoria.dev/posts/digital-resilience-redundancy-for-websites-and-communications/":{title:"Digital resilience: redundancy for websites and communications",tags:["privacy","cybersecurity","data","life","websites"],content:`When what seems like half the planet noped out of WhatsApp after its terms of service update, applications like Signal (which I highly recommend) saw an unprecedented increase in user traffic. Signal had so many new users sign up that it overwhelmed their existing infrastructure and lead to a 24-hour-ish outage.
Signal is experiencing technical difficulties. We are working hard to restore service as quickly as possible.
&mdash; Signal (@signalapp) January 15, 2021 The small team responded impressively quickly, especially given that a 4,200% spike in new users was utterly implausible before it occurred.
The downside of so many people moving onto this fantastic application is that it caused a brief outage. If you rely solely on a certain application for your communications, brief outages can be debilitating. Even when it seems implausible that your favorite chat, email, or website service could just &ndash; poof &ndash; vanish overnight, recent events have proved it isn&rsquo;t impossible.
Have a backup plan. Have several. Here&rsquo;s how you can improve your digital resiliency for things like websites, messaging, and email.
Messaging I recommend Signal because it is open source, end-to-end encrypted, cross-platform, and offers text, voice, video, and group chat. It&rsquo;s usually very reliable; however, strange things can happen.
It&rsquo;s important to set up a backup plan ahead of any service outages with the people you communicate with the most. Have an agreement for a secondary method of messaging &ndash; ideally another end-to-end encrypted service. Avoid falling back on insecure communications like SMS and social media messaging. Here&rsquo;s a short list for you to explore:
Signal Wire Session If you&rsquo;re particularly technically inclined, you can set up your own self-hosted chat service with Matrix.
Having a go-to plan B can help bring peace of mind and ensure you&rsquo;re still able to communicate when strange things happen.
Cloud contacts Do you know the phone numbers of your closest contacts? While memorizing them might not be practical, storing them solely online is an unnecessary risk. Most services allow you to export your contacts to vCard or CSV format.
I recommend keeping your contacts locally on your device whenever you can. This ensures you still know how to contact people if your cloud provider is unavailable, or if you don&rsquo;t have Internet access.
Full analog redundancy is also possible here. Remember that paper stuff? Write down the phone numbers of your most important contacts so you can access them if your devices run out of battery or otherwise can&rsquo;t turn on (drop your phone much?).
Local email synchronization If your email service exists solely online, there&rsquo;s a big email-shaped hole in your life. If you can&rsquo;t log in to your email for any reason &ndash; an outage on their end, a billing error, or your Internet is down &ndash; you&rsquo;ll have no way to access your messages for however long your exile lasts. If you think about all the things you do via email in a day, I think the appropriate reaction to not having local copies is 🤦.
Download an open source email client like Thunderbird. Follow instructions to install Thunderbird and set it up with your existing online email service. Your online service provider may have a help document that shows you how to set up Thunderbird.
You can maximize your privacy by turning off Thunderbird&rsquo;s telemetry.
To ensure that Thunderbird downloads your email messages and stores them locally on your machine:
Click the &ldquo;hamburger&rdquo; overflow menu and go to Account Settings Choose Synchronization &amp; Storage in the sidebar Ensure that under Message Synchronizing, the checkbox for Keep messages in all folders for this account on this computer is checked. You may need to visit each of your folders in order to trigger the initial download.
Some other settings you may want to update:
Choose Composition &amp; Addressing and uncheck the box next to Compose messages in HTML format to send plaintext emails instead. Under Return Receipts choose Global Preferences. Select the radio button for Never send a return receipt. You don&rsquo;t need to start using Thunderbird for all your email tasks. Just make sure you open it up regularly so that your messages sync and download to your machine.
Websites I strongly believe you should have your own independent website for reasons that go beyond redundancy. To truly make your site resilient, it&rsquo;s important to have your own domain.
If you know that my website is at the address victoria.dev, for example, it doesn&rsquo;t matter whether I&rsquo;m hosting it on GitHub Pages, AWS, Wordpress, or from a server in my basement. If my hosting provider becomes unavailable, my website won&rsquo;t go down with it. Getting back up and running would be as simple as updating my DNS configuration to point to a new host.
Price is hardly an excuse, either. You can buy a domain for less than a cup of coffee with my Namecheap affiliate link (thanks!). Namecheap also handles your DNS settings, so it&rsquo;s a one-stop shop.
With your own domain, you can build resiliency for your email address as well. Learn how to set up your custom domain with your email provider. If you need to switch providers in the future, your email address ports to the new service with you. Here are a few quick links for providers I&rsquo;d recommend:
ProtonMail: How to use a custom domain with Proton Mail Tutanota: Adding of custom email domains Fastmail: Custom Domains with Fastmail Build your digital resiliency I hope you&rsquo;ve found this article useful on your path to building digital resiliency. If you&rsquo;re interested in more privacy topics, you might like to learn about great apps for outsourcing security.
If your threat model includes anonymity or censorship, building digital resiliency is just a first step. The rest is outside the scope of my blog, but here are a few great resources I&rsquo;ve come across:
Tor Browser IntelTechniques Can&rsquo;t Cancel Me Tails portable OS `,url:"https://victoria.dev/posts/digital-resilience-redundancy-for-websites-and-communications/"},"https://victoria.dev/archive/create-a-self-hosted-chat-service-with-your-own-matrix-server/":{title:"Create a self-hosted chat service with your own Matrix server",tags:["privacy","protocols","linux","aws","go","terminal"],content:`Matrix is an open standard for decentralized real-time communication. The specification is production-ready and bridges to tons of silo products like Slack, Gitter, Telegram, Discord, and even Facebook Messenger. This lets you use Matrix to link together disjoint communities in one place, or create an alternative communication method that works with, but is independent of, communication silos.
You can create your own self-hosted Matrix chat for as little as $3.50 USD per month on an AWS Lightsail instance. Your homeserver can federate with other Matrix servers, giving you a reliable and fault-tolerant means of communication.
Matrix is most widely installed via its Synapse homeserver implementation written in Python 3. Dendrite, its second-generation homeserver implementation written in Go, is currently released in beta. Dendrite will provide more memory efficiency and reliability out-of-the-box, making it an excellent choice for running on a virtual instance.
Here&rsquo;s how to set up your own homeserver on AWS Lightsail with Dendrite. You can also contribute to Dendrite today.
Create a Lightsail instance Spin up a new Lightsail instance on AWS with Debian as your operating system. It&rsquo;s a good idea to create a new per-instance key for use with SSH. You can do this by with the SSH key pair manager on the instance creation page. Don&rsquo;t forget to download your private key and .gitignore your secrets.
Click Create Instance. Wait for the status of your instance to change from Pending to Running, then click its name to see further information. You&rsquo;ll need the Public IP address.
To enable people including yourself to connect to the instance, go to the Networking tab and add a firewall rule for HTTPS. This will open 443 so you can connect over IPv4. You can also do this for IPv6.
Connect DNS Give your instance a catchier address by buying a domain at Namecheap and setting up DNS records.
On your domain management page in the Nameservers section, choose Namecheap BasicDNS. On the Advanced DNS tab, click Add New Record. Add an A Record to your Lightsail Public IP. You can use a subdomain if you want one, for example,
Type: A Record Host: matrix Value: 13.59.251.229 This points matrix.example.org to your Lightsail instance.
Set up your Matrix homeserver Change permissions on the private key you downloaded:
chmod 600 &lt;path/to/key&gt; Then SSH to your Public IP:
ssh -i &lt;path/to/key&gt; admin@&lt;public ip&gt; Welcome to your instance! You can make it more interesting by downloading some packages you&rsquo;ll need for Dendrite. It&rsquo;s a good idea to use apt for this, but first you&rsquo;ll want to make sure you&rsquo;re getting the latest stuff.
Dec 2021 update: As the good people of Mastodon point out, you might like to ensure you&rsquo;re choosing the stable version for Debian. For instance, replace buster below with what&rsquo;s &ldquo;stable&rdquo; at the moment.
Change your sources list in order to get the newest version of Go:
sudo vim /etc/apt/sources.list Delete everything except these two lines:
deb http://cdn-aws.deb.debian.org/debian buster main deb-src http://cdn-aws.deb.debian.org/debian buster main Then replace the distributions:
:%s/buster main/testing main contrib non-free/g Run sudo apt dist-upgrade. If you&rsquo;re asked about modified configuration files, choose the option to &ldquo;keep the local version currently installed.&rdquo;
Once the upgrade is finished, restart your instance with sudo shutdown -r now.
Go make some coffee, then SSH back in. Get the packages you&rsquo;ll need with:
sudo apt update sudo apt upgrade sudo apt install -y git golang nginx python3-certbot-nginx You&rsquo;re ready to get Dendrite.
Get Dendrite Clone Dendrite and follow the README instructions to get started. You&rsquo;ll need to choose whether you want your Matrix instance to be federating. For simplicity, here&rsquo;s how to set up a non-federating deployment to start:
git clone https://github.com/matrix-org/dendrite cd dendrite ./build.sh # Generate a Matrix signing key for federation (required) ./bin/generate-keys --private-key matrix_key.pem # Generate a self-signed certificate (optional, but a valid TLS certificate is normally # needed for Matrix federation/clients to work properly!) ./bin/generate-keys --tls-cert server.crt --tls-key server.key # Copy and modify the config file - you&#39;ll need to set a server name and paths to the keys # at the very least, along with setting up the database connection strings. cp dendrite-config.yaml dendrite.yaml Configure Dendrite Modify the configuration file you just copied:
sudo vim dendrite.yaml At minimum, set:
server name to your shiny new domain name, e.g. matrix.example.org disable_federation to true or false registration_disabled to true or false You might like to read the Dendrite FAQ.
Configure nginx Get the required packages if you didn&rsquo;t already install them above:
sudo apt install nginx python3-certbot-nginx Create your site&rsquo;s configuration file under sites-available with:
cd /etc/nginx/sites-available ln -s /etc/nginx/sites-available/&lt;sitename&gt; /etc/nginx/sites-enabled/&lt;sitename&gt; sudo cp default &lt;sitename&gt; Edit your site configuration. Delete the root and index lines if you don&rsquo;t need them, and input your server name.
Your location block should look like:
location / { proxy_pass https://localhost:8448; } Remove the default with: sudo rm /etc/nginx/sites-enabled/default.
Create self-signed certificates You can use Certbot to generate self-signed certificates with Let&rsquo;s Encrypt.
sudo certbot --nginx -d &lt;your.site.address&gt; If you don&rsquo;t want to give an email, add the --register-unsafely-without-email flag.
Test your configuration and restart nginx with:
sudo nginx -t sudo systemctl restart nginx Then start up your Matrix server.
# Build and run the server: ./bin/dendrite-monolith-server --tls-cert server.crt --tls-key server.key --config dendrite.yaml Your Matrix server is up and running at your web address! If you disabled registration in your configuration, you may need to create a user. You can do this by running the included dendrite/bin/createuser.
You can log on to your new homeserver with any Matrix client, or Matrix-capable applications like Pidgin with the Matrix plugin.
Other troubleshooting Log files If you get an error such as:
... [github.com/matrix-org/dendrite/internal/log.go:155] setupFileHook Couldn&#39;t create directory /var/log/dendrite: &#34;mkdir /var/log/dendrite: permission denied&#34; You&rsquo;ll need to create a spot for your log files. Avoid the bad practice of running stuff with sudo whenever you can. Instead, create the necessary file with the right permissions:
sudo mkdir /var/log/dendrite sudo chown admin:admin /var/log/dendrite # Build and run the server: ./bin/dendrite-monolith-server --tls-cert server.crt --tls-key server.key --config dendrite.yaml Unable to decrypt If you see: Unable to decrypt: The sender's device has not sent us the keys for this message. you may need to verify a user (sometimes yourself).
In your client, open the user&rsquo;s profile. Click the lock icon if there is one, or otherwise look for a way to verify them. You may be asked to see if some emojis presented to both users match if you&rsquo;re using certain clients like Element. You can then re-request encryption keys for any sent messages. Set up your own Matrix server today I hope you found this introduction to setting up your own Matrix homeserver to be helpful!
`,url:"https://victoria.dev/archive/create-a-self-hosted-chat-service-with-your-own-matrix-server/"},"https://victoria.dev/tags/go/":{title:"Go",tags:[],content:"",url:"https://victoria.dev/tags/go/"},"https://victoria.dev/tags/protocols/":{title:"Protocols",tags:[],content:"",url:"https://victoria.dev/tags/protocols/"},"https://victoria.dev/tags/terminal/":{title:"Terminal",tags:[],content:"",url:"https://victoria.dev/tags/terminal/"},"https://victoria.dev/posts/do-i-raise-or-return-errors-in-python/":{title:"Do I raise or return errors in Python?",tags:["python","coding","leadership"],content:`I hear this question a lot: &ldquo;Do I raise or return this error in Python?&rdquo;
The right answer will depend on the goals of your application logic. You want to ensure your Python code doesn&rsquo;t fail silently, saving you and your teammates from having to hunt down deeply entrenched errors.
Here&rsquo;s the difference between raise and return when handling failures in Python.
When to raise The raise statement allows the programmer to force a specific exception to occur. (8.4 Raising Exceptions)
Use raise when you know you want a specific behavior, such as:
raise TypeError(&#34;Wanted strawberry, got grape.&#34;) Raising an exception terminates the flow of your program, allowing the exception to bubble up the call stack. In the above example, this would let you explicitly handle TypeError later. If TypeError goes unhandled, code execution stops and you&rsquo;ll get an unhandled exception message.
Raise is useful in cases where you want to define a certain behavior to occur. For example, you may choose to disallow certain words in a text field:
if &#34;raisins&#34; in text_field: raise ValueError(&#34;That word is not allowed here&#34;) Raise takes an instance of an exception, or a derivative of the Exception class. Here are all of Python&rsquo;s built-in exceptions.
Raise can help you avoid writing functions that fail silently. For example, this code will not raise an exception if JAM doesn&rsquo;t exist:
import os def sandwich_or_bust(bread: str) -&gt; str: jam = os.getenv(&#34;JAM&#34;) return bread + str(jam) + bread s = sandwich_or_bust(&#34;\\U0001F35E&#34;) print(s) # Prints &#34;🍞None🍞&#34; which is not very tasty. To cause the sandwich_or_bust() function to actually bust, add a raise:
import os def sandwich_or_bust(bread: str) -&gt; str: jam = os.getenv(&#34;JAM&#34;) if not jam: raise ValueError(&#34;There is no jam. Sad bread.&#34;) return bread + str(jam) + bread s = sandwich_or_bust(&#34;\\U0001F35E&#34;) print(s) # ValueError: There is no jam. Sad bread. Any time your code interacts with an external variable, module, or service, there is a possibility of failure. You can use raise in an if statement to help ensure those failures aren&rsquo;t silent.
Raise in try and except To handle a possible failure by taking an action if there is one, use a try &hellip; except statement.
try: s = sandwich_or_bust(&#34;\\U0001F35E&#34;) print(s) except ValueError: buy_more_jam() raise This lets you buy_more_jam() before re-raising the exception. If you want to propagate a caught exception, use raise without arguments to avoid possible loss of the stack trace.
If you don&rsquo;t know that the exception will be a ValueError, you can also use a bare except: or catch any derivative of the Exception class with except Exception:. Whenever possible, it&rsquo;s better to raise and handle exceptions explicitly.
Use else for code to execute if the try does not raise an exception. For example:
try: s = sandwich_or_bust(&#34;\\U0001F35E&#34;) print(s) except ValueError: buy_more_jam() raise else: print(&#34;Congratulations on your sandwich.&#34;) You could also place the print line within the try block, however, this is less explicit.
When to return When you use return in Python, you&rsquo;re giving back a value. A function returns to the location it was called from.
While it&rsquo;s more idiomatic to raise errors in Python, there may be occasions where you find return to be more applicable.
For example, if your Python code is interacting with other components that do not handle exception classes, you may want to return a message instead. Here&rsquo;s an example using a try &hellip; except statement:
from typing import Union def share_sandwich(sandwich: int) -&gt; Union[float, Exception]: try: bad_math = sandwich / 0 return bad_math except Exception as e: return e s = share_sandwich(1) print(s) # Prints &#34;division by zero&#34; Note that when you return an Exception class object, you&rsquo;ll get a representation of its associated value, usually the first item in its list of arguments. In the example above, this is the string explanation of the exception. In some cases, it may be a tuple with other information about the exception.
You may also use return to give a specific error object, such as with HttpResponseNotFound in Django. For example, you may want to return a 404 instead of a 403 for security reasons:
if object.owner != request.user: return HttpResponseNotFound Using return can help you write appropriately noisy code when your function is expected to give back a certain value, and when interacting with outside elements.
The most important part Silent failures create some of the most frustrating bugs to find and fix. You can help create a pleasant development experience for yourself and your team by using raise and return to ensure that errors are handled in your Python code.
I write about good development practices and how to improve productivity as a software developer. You can get these tips right in your inbox by signing up below!
`,url:"https://victoria.dev/posts/do-i-raise-or-return-errors-in-python/"},"https://victoria.dev/posts/what-tech-leaders-do-before-going-on-vacation/":{title:"What tech leaders do before going on vacation",tags:["leadership","leadership"],content:`As a technical person who leads a technical team, I know firsthand that it can be easy to get lost in finishing up your own work before a vacation. It takes a bit of dedicated attention to ensure you don&rsquo;t neglect the day-to-day tasks that don’t vanish while you’re away.
Here’s a pre-PTO checklist to make sure you’ve taken care of those responsibilities before you take off for a much-deserved vacation.
Does everyone know who&rsquo;s supposed to do what? A lot of information lives in your head alone, despite valiant efforts to document all the things. That’s the nature of the work. Instead of attempting to disseminate everything you’re thinking through, focus on the work your team will do while you’re away.
Hopefully, you already have a centralized list of priorities. Ensure it’s up to date, and that all the tasks that could conceivably be done during your time off have been assigned a caretaker.
Are there any decisions waiting on me? Review notes, discussion boards, and ask your team directly if anyone is waiting on an answer from you. If you’re able to make a decision before your vacation, do so. If not, delegate the decision to someone else with a clear explanation of your overall goals and any applicable parameters.
Don’t neglect internal auto-responders Set up auto-responders for communications from the public, as well as from your team! While a response from your public email may say that you’re out of the office and when you expect to be back, an internal auto-responder is an opportunity to provide even more value.
It could take the form of an email response for your internal inbox, but a post on your team’s message board or chat channel also works. Let people know where to look for information they might need, who to turn to if they need help while you’re away, and where to find your centralized priority list so they can decide what to work on next.
Tidy up loose ends Finally, close out work that depends on you and that no one else can do. If it’s work you can delegate with some written guidance attached, you might choose this route instead of attempting to finish it yourself in a hurry.
If you hand off any work, ensure that you communicate clear instructions as well as any deadlines.
Set expectations and follow them Your vacation is precious time to recoup, relax, and make room for those creative moments that only visit a quiet mind. Ensure you set expectations with your team for how often you might check in, if at all.
Responsible vacation planning for technology leaders I hope you benefit from the ideas in this post. Here&rsquo;s an easy way to put them into practice right now: grab this post as a pre-PTO Notion checklist template and instantly gain conscientious leadership powers!
Get checklist as a Notion template
Don&rsquo;t leave your team members hanging while you&rsquo;re away! If you have other ideas for good things to do before your vacation, I&rsquo;d love to hear about it.
`,url:"https://victoria.dev/posts/what-tech-leaders-do-before-going-on-vacation/"},"https://victoria.dev/posts/add-search-to-hugo-static-sites-with-lunr/":{title:"Add search to Hugo static sites with Lunr",tags:["websites","coding","data","javascript"],content:`Yes, you can have an interactive search feature on your static site! No need for servers or paid subscriptions here. Thanks to the open source Lunr and the power of Hugo static site generator, you can create a client-side search index with just a template and some JavaScript.
A number of my readers have been kind enough to tell me that you find my blog useful, but there&rsquo;s something that you don&rsquo;t know. Up until I recently implemented a search feature on victoria.dev, I had been my own unhappiest user.
My blog exists for all to read, but it&rsquo;s also my own personal Internet brain. I frequently pull up a post I&rsquo;ve written when trying to re-discover some bit of knowledge that I may have had the foresight to record. Without a search, finding it again took a few clicks and more than a few guesses. Now, all my previous discoveries are conveniently at my fingertips, ready to be rolled into even more future work.
If you&rsquo;d like to make your own personal Internet brain more useful, here&rsquo;s how you can implement your own search feature on your static Hugo site.
Get Lunr While you can install lunr.js via npm or include it from a CDN, I chose to vendorize it to minimize network impact. This means I host it from my own site files by placing the library in Hugo&rsquo;s static directory.
You can save your visitors some bandwidth by minifying lunr.js, which I did just by downloading lunr.js from source and using the JS &amp; CSS Minifier Visual Studio Code extension on the file. That brought the size down roughly 60% from 97.5 KB to 39.35 KB.
Save this as static/js/lunr.min.js.
Create a search form partial To easily place your search form wherever you like on your site, create the form as a partial template at layouts/partials/search-form.html
&lt;form id=&#34;search&#34; action=&#39;{{ with .GetPage &#34;/search&#34; }}{{.Permalink}}{{end}}&#39; method=&#34;get&#34;&gt; &lt;label hidden for=&#34;search-input&#34;&gt;Search site&lt;/label&gt; &lt;input type=&#34;text&#34; id=&#34;search-input&#34; name=&#34;query&#34; placeholder=&#34;Type here to search&#34;&gt; &lt;input type=&#34;submit&#34; value=&#34;search&#34;&gt; &lt;/form&gt; Include your search form in other templates with:
{{ partial &#34;search-form.html&#34; . }} Create a search page For your search to be useful, you&rsquo;ll need a way to trigger one. You can create a (static!) /search page that responds to a GET request, runs your search, and displays results.
Here&rsquo;s how to create a Hugo template file for a search page and get it to render.
Create layouts/search/list.html with the following minimum markup, assuming you&rsquo;re inheriting from a base template:
{{ define &#34;main&#34; }} {{ partial &#34;search-form.html&#34; . }} &lt;ul id=&#34;results&#34;&gt; &lt;li&gt; Enter a keyword above to search this site. &lt;/li&gt; &lt;/ul&gt; {{ end }} In order to get Hugo to render the template, a matching content file must be available. Create content/search/_index.md to satisfy this requirement. The file just needs minimal front matter to render:
--- title: Search me! --- You can run hugo serve and navigate to /search to see if everything builds as expected.
A few libraries exist to help you build a search index and implement Lunr. You can find them here on the Hugo site. If you want to fully understand the process, however, you&rsquo;ll find it&rsquo;s not complicated do this without additional dependencies, thanks to the power of Hugo&rsquo;s static site processing.
Build your search index Here&rsquo;s how to build an index for Lunr to search using Hugo&rsquo;s template rendering power. Use range to loop over the pages you want to make searchable, and capture your desired parameters in an array of documents. One way to do this is to create layouts/partials/search-index.html with:
&lt;script&gt; window.store = { // You can specify your blog section only: {{ range where .Site.Pages &#34;Section&#34; &#34;posts&#34; }} // For all pages in your site, use &#34;range .Site.Pages&#34; // You can use any unique identifier here &#34;{{ .Permalink }}&#34;: { // You can customize your searchable fields using any .Page parameters &#34;title&#34;: &#34;{{ .Title }}&#34;, &#34;tags&#34;: [{{ range .Params.Tags }}&#34;{{ . }}&#34;,{{ end }}], &#34;content&#34;: {{ .Content | plainify }}, // Strip out HTML tags &#34;url&#34;: &#34;{{ .Permalink }}&#34; }, {{ end }} } &lt;/script&gt; &lt;!-- Include Lunr and code for your search function, which you&#39;ll write in the next section --&gt; &lt;script src=&#34;/js/lunr.min.js&#34;&gt;&lt;/script&gt; &lt;script src=&#34;/js/search.js&#34;&gt;&lt;/script&gt; When Hugo renders your site, it will build your search index in much the same way as a List page is built, creating a document for each page with its parameters.
The last piece of the puzzle is the code to handle the search process: taking the search query, getting Lunr to perform the search, and displaying the results.
Perform the search and show results Create static/js/search.js to hold the JavaScript that ties it all together. This file has three main tasks: get the search query, perform the search with Lunr, and display the results.
Get query parameters with JavaScript This part&rsquo;s straightforward thanks to URLSearchParams:
const params = new URLSearchParams(window.location.search) const query = params.get(&#39;q&#39;) Search for the query with Lunr Define and configure an index for Lunr. This tells Lunr what you&rsquo;d like to search with, and you can optionally boost elements that are more important.
const idx = lunr(function () { // Search these fields this.ref(&#39;id&#39;) this.field(&#39;title&#39;, { boost: 15 }) this.field(&#39;tags&#39;) this.field(&#39;content&#39;, { boost: 10 }) // Add the documents from your search index to // provide the data to idx for (const key in window.store) { this.add({ id: key, title: window.store[key].title, tags: window.store[key].category, content: window.store[key].content }) } }) You can then execute the search and store results with:
const results = idx.search(query) Display results You&rsquo;ll need a function that builds a list of results and displays them on your search page. Recall the id you gave your ul element in layouts/search/list.html and store it as a variable:
const searchResults = document.getElementById(&#39;results&#39;) If a search results in some results (🥁), you can iterate over them and build a &lt;li&gt; element for each one.
if (results.length) { // Length greater than 0 is truthy let resultList = &#39;&#39; for (const n in results) { // Use the unique ref from the results list to get the full item // so you can build its &lt;li&gt; const item = store[results[n].ref] resultList += &#39;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;&#39; + item.url + &#39;&#34;&gt;&#39; + item.title + &#39;&lt;/a&gt;&lt;/p&gt;&#39; // Add a short clip of the content resultList += &#39;&lt;p&gt;&#39; + item.content.substring(0, 150) + &#39;...&lt;/p&gt;&lt;/li&gt;&#39; } searchResults.innerHTML = resultList } For each of your results, this produces a list item similar to:
&lt;li&gt; &lt;p&gt; &lt;a href=&#34;.../blog/add-search-to-hugo-with-lunr/&#34;&gt; Add search to Hugo static sites with Lunr &lt;/a&gt; &lt;/p&gt; &lt;p&gt;Yes, you can have an interactive search feature on your static site!...&lt;/p&gt; &lt;/li&gt; If there are no results, ham-handedly insert a message instead.
else { searchResults.innerHTML = &#39;No results found.&#39; } Full code for search.js Here&rsquo;s what static/js/search.js could look like in full.
search.js full code Make it go You now have Lunr, the search index, and the code that displays results. Since these are all included in layouts/partials/search-index.html, add this partial on all pages with a search form. In your page footer, place:
{{ partial &#34;search-index.html&#34; . }} You can see what this looks like when it&rsquo;s all put together by trying it out on my blog.
Make it go faster Since your site is static, it&rsquo;s possible to pre-build your search index as a JSON data file for Lunr to load. This is where those aforementioned libraries may be helpful, since a JSON-formatted search index would need to be built outside of running hugo to generate your site.
You can maximize your search speed by minifying assets, and minimizing computationally expensive or blocking JavaScript in your code.
Static sites get search, too! I hope this helps you make your Internet brain more useful for yourself and others, too! Don&rsquo;t worry if you haven&rsquo;t got the time to implement a search feature today &ndash; you can find this tutorial again when you visit victoria.dev and search for this post! 🥁
`,url:"https://victoria.dev/posts/add-search-to-hugo-static-sites-with-lunr/"},"https://victoria.dev/tags/javascript/":{title:"Javascript",tags:[],content:"",url:"https://victoria.dev/tags/javascript/"},"https://victoria.dev/posts/make-your-own-independent-website/":{title:"Make your own independent website",tags:["websites","life"],content:`The web that raised me was a digital playground in the truest sense. It was made up of HTML experiments frankensteined together by people still figuring it all out.
The beauty of not completely knowing what you&rsquo;re doing is a lack of premature judgement. Without a standard to rise to, you&rsquo;re free to go sideways. Explore. Try things that don&rsquo;t work, without any expectation they will work. An open world with a beginner&rsquo;s mindset.
The web that raised me was a little broken. Things didn&rsquo;t always display the way they were supposed to. That too is part of the beauty. It was just broken enough to make you think for yourself.
1991 was the year of the individual on the web, the first year any layperson could open a web browser and access the new hypermedia dimension. There were no go-to, search-suggested, centralized websites. There were newsgroups. You had what you made and what your meatspace contacts sent you. In 2021, I think we need a return to that level of individualism. We need to make 2021 the year of the independent web.
That&rsquo;s not to say I think the massive monopolistic platforms are going anywhere. Twitter, Facebook, mainstream &ldquo;news&rdquo; media sites &ndash; they&rsquo;re all a kind of utility now, like plumbing and electricity. They&rsquo;ll find their place in regulation and history. But they are not your website.
Your website is the one you create. Where the content, top-to-bottom, is yours alone to shape and present as you please. Your website is your place of self-expression, without follower counts or statistics to game. Your website is for creation, not reaction.
It&rsquo;s all yours, but it doesn&rsquo;t have to seem lonely. Your site can interact with the entire online world through syndication and protocols made possible by this thing we call the Internet. See:
IndieWeb for POSSE, an abbreviation for Publish (on your) Own Site, Syndicate Elsewhere Webmention and an easy way to implement them twtxt instances for a decentralized timeline experience Neofeed , my personal timeline project made for Neocities . (It&rsquo;s open source and you can help me extend it! ) Your website is your beginning point. The one source of truth for your identity online, from which you can generate and distribute disposable copies to any platform you please. This is what it means to truly own your content. And on the Internet, your content is you.
This is my website. When I first created it, I did so for myself. I had no expectation of visitors. I just knew I&rsquo;d rather have these thoughts and things I&rsquo;ve learned here, out here, made indelible in the folds of the public Internet, instead of on some dark corner of my machine, to be lost forever once I am.
Make your own website. You&rsquo;ll grow your own sense of well-deserved accomplishment and contribute to your independence on the web. You&rsquo;ll learn by doing, by scratching your own itch.
Learn about web technologies. Use them as you would if you were a child holding a pencil or paintbrush for the first time. Experiment, with no expectations other than discovering what you can do to make it delight you.
These sites and articles inspired this post and helped me implement webmentions!
Why the Indie Web movement is so important, Dan Gillmor Jamie Tanna Max Böck Paul Robert Lloyd Zachary Dunn Adding Webmentions to My Static Hugo Site, Ana Ulin Adding Webmention Support to a Static Site, Keith J Grant Webmention.rocks `,url:"https://victoria.dev/posts/make-your-own-independent-website/"},"https://victoria.dev/posts/how-to-get-hired-as-a-software-developer/":{title:"How to get hired as a software developer",tags:["life","coding","open-source"],content:`I&rsquo;m asked this question a lot, so let me be the first to give you the good news: there&rsquo;s no one right answer. As general tech-literacy increases, the culture of the coding industry is steadily, thankfully, moving away from a checklist approach. Instead of degrees and pre-requisites when it comes to deciding whether you&rsquo;re qualified to be hired as a software developer, companies (including my own) are far more concerned with just one question. What can you do?
There are some general best practices that will make you a far more attractive hire than the majority of applicants, and I&rsquo;ll discuss those in this post. For the most part, however, demonstrating what you&rsquo;re capable of is the best way to increase your chances of getting to the interview and beyond. Here&rsquo;s how to get hired as a software developer.
First, build projects Companies who are primarily focused on getting products built want to see that you&rsquo;ve built products. They don&rsquo;t need to be flashy or for-profit, but they do need to work. I&rsquo;m far more likely to consider a candidate with a colorful bouquet of working code in their GitHub or GitLab or CodePen portfolio. Here are some basic ideas to get you started:
Command line utilities that help with tedious tasks, like renaming image files Themes for static site generators like Hugo or Jekyll Automation tools, such as for GitHub Actions The best projects you could showcase are ones directly related to the specialty you want to apply for. Show that you have competency with the fundamentals. For instance, if you see yourself focusing on front end, demonstrate that you can build interactive web pages with no fancier tools than HTML, CSS, and vanilla JS. For back end focused developers, show that you know how to create a fundamental tool like an API for a service on a local server. Want to be well-rounded? Create an API with a web page UI.
Spend some time creating a good README. Use screenshots, highlight code snippets, include well-written instructions for local set up. Show that you care about and fully understand your own work.
Explore specific frameworks and libraries if they interest you, but keep in mind that those won&rsquo;t be interesting to a company unless the company already wants to use that framework.
You maximize your chances of getting hired by demonstrating that you already have the ability to learn on your own, build, and then present projects to the world. Out of everything else in this article, this is the one fundamental trait that a company won&rsquo;t want to have to teach you.
Next, stand out Familiarity with the following topics, along with demonstrating that understanding in your own code, will put you miles ahead of most applicants.
Reusable code Companies that build products are concerned with getting the most bang for their buck. This is the same idea as wanting to save yourself time when you&rsquo;re creating something individually. If you put a week of effort into building something, it would be nice if you could keep easily using it for a long time afterwards. How can you maximize the return on your efforts?
Be familiar with DRY code. Avoid creating highly customized pieces that only fit a particular use case, peppered with hard-coded variables and dependent on a particular input structure. Avoid writing code that is hard to update in the future. Recognize when you&rsquo;re writing a script or library that could apply in many different situations, and understand how to turn it into a reusable module.
Types and mutability Besides building projects, debugging them can be a company&rsquo;s most expensive task. It takes a lot of time to hunt down and fix bugs, but you can help reduce that cost by understanding the subtler ways that a lot of bugs occur. Understanding types and mutability &ndash; whether and how an object can be changed &ndash; can help open the door to even greater technical proficiency.
Get familiar with at least one type system. If there&rsquo;s a linter available for your language, use it. Understand how immutable and mutable objects work in the language you use. Be able to describe specific use cases for either one. Understand at a general level how mutability impacts system resources, the difference between referencing and copying, and what it means to be thread-safe.
Follow-on effects Keep in mind that organizations are made up of people. When you work together with colleagues, your work has an effect on someone else&rsquo;s. Being aware of these effects and demonstrating conscientiousness in this area will help show potential employers that you&rsquo;d benefit the team as a whole.
Consider the second and third-order effects of code you write. Avoid writing code that will unnecessarily slow down a larger system. Understand what blocking means and how to use concurrency or parallelism in your project. Include your thoughts on follow-on effects in your READMEs. Show that you always have the larger project, effort, costs, or organization in mind.
Other nice-to-haves If you&rsquo;ve fully taken advantage of the points above, you&rsquo;re most of the way to getting hired already. Seal the deal with these easy wins.
Be a friendly open source participant There&rsquo;s no better way to show a potential employer you can work well on a team than by providing plenty of examples. Get involved with open source projects, contribute features and fixes, and interact with contributors and maintainers. Create a real-life simulation for your future colleagues that leaves no doubt about what you&rsquo;d be like to work with. The further back this history goes, the better, so start right away.
Communicate with care If you&rsquo;re participating in open source or working remotely, most of your communication with your colleagues is going to take place online in text. Without facial expressions, tone or inflection, this form of communication leaves a lot to be desired. Some extra care on your part can help make sure your message always comes across as intended.
Get into a habit of drafting most everything you write, especially for long-form communication. Putting yourself in the mindset of creating a draft first lets you take all the time you need to craft your message. You can make sure you&rsquo;re choosing appropriate words and coming across with the emotions you intend to convey. Feeling hurried? Remember the golden rule of online communication: you never need to reply right away. Take a breath, then take your time.
Finally, use your imagination Software developers are creative people by necessity. Before you can write code, build a project, or design a page, you first have to be able to imagine it! So put that skill to good use.
In every application, every email, every chat message with your potential employer, imagine yourself in their position. What do they care about right now? What current goals does the company have? What information about yourself can you share that would make them feel comfortable hiring you?
Take your best guess, and then ask if you got it right. &ldquo;I think the company is looking for someone to [insert guess here], is that accurate?&rdquo; Show that you have both the capability to anticipate future needs and the desire to identify and solve them.
Get yourself hired Admittedly, this post is my own wishlist. Good candidates for software development positions are hard to come by, and people who can rightly say they do everything above are rare. I don&rsquo;t think the discrepancy is due to a lack of ability; perhaps just a lack of information.
I&rsquo;ve seen both sides of the virtual interview table, and this post is a result of me figuring things out the long and circuitous way. I hope this helps you to take a more direct route to getting yourself hired as a software developer.
`,url:"https://victoria.dev/posts/how-to-get-hired-as-a-software-developer/"},"https://victoria.dev/archive/how-to-become-a-software-developer/":{title:"How to become a software developer",tags:["coding","life"],content:`As a Director of Engineering, I’m a software developer who hires and leads other software developers. It’s not surprising then that I get asked this question a lot, in various forms:
How do I become a software developer? What language or framework should I learn first? Where do I start? While I’m certain there’s no one right answer for everyone, I’m also certain that the world needs more software developers and systems thinkers.
The best thing I can do to help you lead yourself, learn to code, and become a software developer is to share the most efficient parts of how I did it myself. This is the article I wish I had read when I started coding.
Depth matters Software is exceedingly complex. Like a good novel that you wish you’d never finish reading, there’s always more to discover and learn. If you don’t want to miss the best parts, don’t be satisfied with surface-level explanations. Always go deeper! Ask why, why, and why again until you get to the fundamentals. Soon enough, you’ll start to see patterns.
By digging deeper, you’ll begin to understand the fundamentals of how things connect, what makes things “fast,” and facets of software operation that you probably can’t even imagine exist. It’s like peeking behind the curtain and seeing a whole world of systems and processes that most people are never aware of.
Going in-depth can expand your mind and your capacity for learning. Keep asking why. Follow every link. Let your curiosity guide you.
Hard stuff matters Giving yourself the chance to be delighted through discovery doesn’t come for free. It takes a lot of hard work to read and compress complicated ideas into your meat brain.
It’s important not to gloss over the hard stuff. In fact, if something seems too hard to understand, you might benefit from doing it first. You might have to get creative to find ways to explain things to yourself, but when you succeed, it makes everything else easier later on.
Analogies are helpful for understanding hard concepts, but they’ll only help you start to understand concepts at a surface level. Remember to go in-depth. Don&rsquo;t stop at the analogy.
Writing matters Write right away. Create a habit of explaining everything you learn to yourself in long-form writing. Better than bullet points, writing with a conversational tone engages parts of your brain that help you to process and remember new information. It’s why humans like and remember stories, and it’s a superpower you get for free.
Start by writing for yourself. Write about what interests you. Try something new, even if it seems rudimentary, and write in-depth about what you learn. (One of my most popular posts is about iteration in Python. When I first wrote it, I considered myself a complete beginner.)
If you want to go a step further, share your writing with the world. Learn in public, like I do. I often get questions like, &ldquo;how do I choose a theme for my blog?&rdquo; or &ldquo;what platform should I use?&rdquo; or &ldquo;what popular language/framework/topic should I focus on?&rdquo; My answer is: don&rsquo;t worry about it.
Don&rsquo;t fret too much about your blog theme or platform. Pick the easiest option for you to get started with for now. All of that will change and improve as you learn, practice, and find your focus. Just start writing, ideally, yesterday.
Write for yourself by explaining what you’re doing, as if it were past-you teaching future-you — because it is. You will be your first reader, and the first judge of how useful your blog can be. Seek to impress yourself!
The language, framework, or version doesn’t matter Why pigeonhole your abilities before you even start? Pick any software language, framework, or technology that seems to make sense to you when you first read it. Start there.
Remember that it’s important to dig deep and understand the fundamentals. Basic concepts of software transcend languages. Whichever first language you choose, understand functions, variables, return values, iteration, and how immutability works. You’ll find that learning these concepts will make it easier to recognize them in your second language, and learn that too.
Your portfolio doesn’t matter If your first objective is to build a portfolio, you may be trying to run before you walk. Building a portfolio to showcase to potential employers is a great goal, but a terrible first step.
If you think of creating a polished portfolio as a first step, you’re liable to spend too much time making it pretty and presentable before focusing on the content. As someone who hires software developers, I can tell you wholeheartedly that I’d rather see clean and well-written code than a flashy front page.
Don’t confuse building a portfolio with building projects. Absolutely build projects, right from the beginning. There’s no better way to see the practical application of what you’re learning. Just treat them as first drafts, as training ground, and don’t worry about packaging them up for professional consumption.
By allowing yourself to build some draft projects first, you allow yourself the breathing room to learn from them. Focus on iteration, on making one small thing better each time, and you&rsquo;ll build a portfolio without even realizing it.
Focus on what matters Don’t follow this advice blindly; rather, incorporate it into your own systems. Experiment, make it work better than when you found it, then pay it forward by writing down what you&rsquo;ve learned for someone else to read!
Here are my favorite books for reading or listening to if you want to cultivate a learning mindset. See non-coding books for coders.
If this article benefits you in some way, I encourage you to write about it! The process of learning how to learn is never finished. You can be the next iteration.
`,url:"https://victoria.dev/archive/how-to-become-a-software-developer/"},"https://victoria.dev/archive/be-brave-and-build-in-public/":{title:"Be brave and build in public",tags:["open-source"],content:`I used to think that when I wanted to make updates to a project, I ought to hold back and do a big re-launch with all the changes at once. I figured that seeing big changes would feel like Christmas!
Unfortunately, Christmas only comes once a year. After years in the tech and cybersecurity world, my perspective has changed.
I&rsquo;ve found that people, including myself, value receiving small, constant, incremental improvements far more than big changes once or a few times a year. It makes sense if you think about it. The former constantly delights in small, unexpected ways that make the user experience better. The latter is invisible, except for a few times a year.
There are occasions when big changes make sense. Say you&rsquo;re re-launching functionality, or coinciding with an event that deserves all the fanfare of an unveiling.
Other than that, and for most of us, holding back doesn&rsquo;t serve us at all. It may even come from something far more insidious: fear of judgement.
Being brave Thinking such as &ldquo;I&rsquo;ll show it to the world when it&rsquo;s ready,&rdquo; always leaves out the most important detail. What does &ldquo;ready&rdquo; mean?
If you haven&rsquo;t written down your definition of &ldquo;ready,&rdquo; consider that you may be holding back for no good reason. What&rsquo;s the worst that could happen, anyway, if you make your work public when it&rsquo;s less than perfect?
I decided to find out when I started to build in public. Instead of holding back work, I released a first version as soon as it functioned as intended. I leaned on v0.0.* tags as a way to say, &ldquo;This is available, but still in progress.&rdquo; Or, I&rsquo;d say so outright, in the README.
In the world of open source, building in public can be scary stuff. It feels like making yourself vulnerable. It&rsquo;s opening up part of yourself, a creative part, for scrutiny and nitpicking &ndash; by strangers. Of course it&rsquo;s not comfortable.
Once I overcame the discomfort, once I decided to be brave and appreciate even possibly negative feedback, something amazing happened.
I suddenly had help.
Yes, there was scrutiny and nitpicking &ndash; but I don&rsquo;t think any of it was ill-intentioned. I found that there existed whole communities of people who wanted to help me build a project that they thought was interesting. In some cases, I was utterly amazed when people submitted pull requests for issues I&rsquo;d opened on my own projects describing enhancements I&rsquo;d like to have.
I&rsquo;ve been fortunate to have wonderful experiences with open source so far. Based on these experiences, I&rsquo;d like to share with you what I&rsquo;ve discovered to be the most effective ways to be brave and generous when it comes to open source.
&lsquo;Tis the season to share generously When unprompted strangers submit helpful comments, issues, and pull requests on your projects, it feels like Christmas. You can give the gift of helpfulness in your contributions as well.
Treat comments like a face-to-face conversation. Greet the person you&rsquo;re addressing. Use full sentences. Think about whether what you&rsquo;re writing will make someone&rsquo;s day better or worse, and be nice.
When writing issues, include as much technical detail as possible. Screenshots, console logs, screenshots of console logs, your operating system, browser, screen resolution &ndash; all these can help maintainers quickly diagnose a root cause.
Pull requests are the best presents ever. They make maintainers happy when well done, and it&rsquo;s a gift that gives back when your contribution gets merged! 🎉 Give your PR the best chance of getting accepted by looking for and following any project contribution guidelines.
Recognize the human Our brains are slightly lacking in an evolutionary sense when it comes to interacting with other humans through tiny screens. It can be easy to forget that the actions you put out there will eventually reach one or more other people.
You can help to maintain a great open source community by remembering the humans that make it exist. When commenting, take the time to do it well (see below) and recognize the time that someone else has put in. When closing a thread or merging a contribution, remember to say thank you to the people who pitched in to help. I try to use first names instead of screen names, whenever possible.
You can build personal relationships, too. If you&rsquo;re a project maintainer, you may choose to give people a way to contact you directly to ask questions or hash out complicated plans. Establishing one-to-one communications with regular contributors is also a great way to build a community around your project.
Recognizing the humans behind the open source community is a simple and meaningful way to give back.
Don&rsquo;t rush The vast majority of open source participants are volunteers, which means they don&rsquo;t get paid for the time they spend building up projects. That sometimes means that other work takes priority. It&rsquo;s okay if this describes you, too.
It&rsquo;s important to remember that in most cases, a well-done contribution later is preferred over a half-done contribution sooner. If you&rsquo;re too short on time now to write a thoughtful comment &ndash; don&rsquo;t! Either draft a quick note and set it aside for later, or comment something along the lines of:
Hi there! Just wanted to let you know that I&rsquo;ve seen this and I plan to help! I&rsquo;ll respond in full as soon as I have the time to write a thoughtful comment.
Showing that you think a comment is worth the time to do well is something that open source contributors and repository maintainers both appreciate.
Build generously When open source participants act with conscientiousness, every day feels like Christmas. Regardless of your type of contribution, you can help build this generous global community year-round.
The humans of open source, by self selection, mostly consist of good people who want to help. If you build openly, share feedback generously, and try to do good in general, I think you fit in here.
I hope you have a very happy holiday season and give many gifts that keep on giving!
`,url:"https://victoria.dev/archive/be-brave-and-build-in-public/"},"https://victoria.dev/archive/so-youre-the-family-tech-support/":{title:"So you're the family tech support",tags:["privacy","cybersecurity","life"],content:`🎄🌟 Happy holidays! 🌟🎄
For those of you seeing relatives this season, chances are that you’re the designated family tech support. If part of your time home for the holidays is spent on software updates and troubleshooting WiFi, here are a few other quick wins to help boost your family&rsquo;s online privacy and security.
1. Set up a VPN Using a VPN is Online Safety 101. Choose a reputable provider with a strict no-logging policy, or if you&rsquo;re up for it, roll your own.
2. Introduce a password manager If your family member uses the same password everywhere (&lt;petname&gt;+&lt;house number&gt;, same as last year) because passwords are hard to remember, introduce them to their new best friend, 1Password. Help your family get set up with secure passwords they don&rsquo;t have to write down on Post-It notes &ndash; just one master pass(phrase) is all you need.
When choosing a passphrase, avoid using information easily found on social media accounts, like pet names, favorite sports teams, favorite brands, or birthdays.
3. Switch to DuckDuckGo Help fight the Internet search monopoly by getting your family to use a search engine that respects their privacy. Go to your browser Settings and set your Default Search Engine (that uses the URL bar) to DuckDuckGo. Break the ice with an instant answer feature, like searching &ldquo;calendar&rdquo; so you can count down to Christmas.
(You might want to search for &ldquo;classic cocktails cheat sheet&rdquo; after all this.)
4. Install a better browser and blocker While I prefer a Pi-hole, setting one up can be complex. Instead, help set up a privacy-preserving browser like Firefox or a wide-spectrum blocking extension like uBlock Origin (GitHub source).
Your family will get faster page load times, less advertisements interrupting articles and videos, and fewer sneaky trackers leaking browsing habits to big tech, all with near-zero maintenance.
Be a home-for-the-holidays hero! Help improve your family&rsquo;s security posture this holiday season. A little beefed-up cybersecurity may be one of the best gifts you can give!
I&rsquo;m keeping it short-and-sweet this week. My annual Christmas post drops on December 24, full of warm fuzzy goodness and a tech tip or two. Thank you for being a subscriber &ndash; stay tuned!
`,url:"https://victoria.dev/archive/so-youre-the-family-tech-support/"},"https://victoria.dev/posts/how-to-write-good-documentation/":{title:"How to write good documentation",tags:["docs","leadership"],content:`If you&rsquo;ve ever half-written a software project before taking a few days off, this is the article you&rsquo;ll discover you needed when you reopen that IDE.
In the technology teams I lead, we make a constant effort to document all the things. Documentation lives alongside the code as an equal player. This helps ensure that no one needs to make assumptions about how something works, or is calling lengthy meetings to gain working knowledge of a feature. Good documentation saves us a lot of time and hassle.
That said, and contrary to popular belief, the most valuable software documentation is not primarily written for other people. As I said in this well-received tweet:
The secret to good documentation is to write it while you&#39;re writing the code. You are your first audience. Explain what you&#39;re doing to yourself. Future you will thank you!
&mdash; Victoria Drake November 24, 2020 Here are three concrete steps you can take to write good documentation before it&rsquo;s too late.
1. Start with accurate notes As you work out ideas in code, ensure you don’t soon forget important details by starting with accurate notes. While you will want to explain things to yourself in long-form later, short-form notes will suffice to capture details without interrupting your coding session flow.
Don&rsquo;t rely on inline comments that often fail to make sense once you&rsquo;ve forgotten the context. Keep a document open alongside your code and write down things like commands, decisions, and sources you use. This can include:
Terminal commands you typed in Why you chose a particular method over another Links you visited for help or coughcopy-pastecough inspiration The order in which you did things Don’t worry about full sentences at this point. Just ensure you accurately capture context, relevant code snippets, and helpful URLs. It can also be helpful to turn on any auto-save option available.
2. Explain decisions in long form The ideal time to tackle this step is when you take a break from coding, but before you completely go out to lunch on whatever it is you’re working on at the moment. You want to ensure that context, ideas, and decisions are all still fresh in your mind when you explain them to yourself.
Go over the short-form notes you took and start expanding them into conversational writing. Be your own rubber duck. Describe what you’re doing as if you were teaching it to someone else. You might cover topics such as:
Quirky-looking decisions: &ldquo;I would normally do it this way, but I chose to do something different because&hellip;&rdquo; Challenges you ran into and how you overcame them Architectural decisions that support your project goals Stick to the main points. Long-form writing doesn’t mean you’ll be paid by the word! Just use full sentences, and write as if explaining your project to a colleague. You’re explaining to future you, after all.
3. Don&rsquo;t neglect prerequisite knowledge This step is best done after a long lunch break, or even the next day (but probably not two). Re-read your document and fill in any blanks that become apparent after putting some distance between yourself and the project.
Take extra care to fill in or at least link to prerequisite knowledge, especially if you frequently use different languages or tools. Even an action as small as pasting in a link to the API documentation you used can save hours of future searching.
Write down or link to READMEs, installation steps, and relevant support issues. For frequently performed command-line actions, you can use a self-documenting Makefile to avoid having to man common tasks each time you come back to a project.
It’s easy to forget supporting details after even just a short break from your project. Capture anything you found helpful this time around.
Document all the things The next time you catch yourself thinking, “I’m sure I’ll remember this part, no need to write it down,” just recall this emoji: 🤦‍♀️
Software projects are made up of a lot more than just their code. To best set up your future self for success, document all the things! Whether it’s a process you’ve established, Infrastructure as Code, or a fleeting future roadmap idea — write it down! Future you will thank you for it.
If you enjoyed this post, there&rsquo;s a lot more where that came from! I write about computing, cybersecurity, and leading great technical teams. You can subscribe to see new articles first.
`,url:"https://victoria.dev/posts/how-to-write-good-documentation/"},"https://victoria.dev/posts/make-your-team-more-productive-by-literally-doing-one-thing/":{title:"Make your team more productive by literally doing one thing",tags:["leadership","leadership"],content:`In the tech teams I lead, &ldquo;priority&rdquo; has no plural form.
Whether you&rsquo;re leading a team of people or leading yourself, it&rsquo;s important to take account of all the important things that need doing in your organization. This does not mean that everything can be equally important.
Logically, everything can&rsquo;t be. Tasks are typically interdependent, and there&rsquo;s always one task on which another depends. Tasks can be time-sensitive. Certain tasks might block a logical path towards a goal.
It&rsquo;s the duty of a leader to make hard calls and decide which tasks are most important out of everything that needs doing. This necessitates comparing one to another, which is much easier to do with a centralized to-do list.
Here&rsquo;s how this one simple change to your perspective on to-do lists can help to build happier and more productive teams.
Keep a central prioritized to-do list Avoid working in silos. A single centralized list can make it easier for you and your team members to see what&rsquo;s being worked on. With all tasks out in the open, it&rsquo;s easier for people to spot opportunities for helping each other out and where they can contribute.
Encouraging a culture of openness can help people feel more comfortable asking questions, asking for help, and proposing ideas and improvements. Tracking work in the open also means that no one is left wondering what status a task is currently in.
For team leaders, a single list makes it easier to compare and prioritize tasks. This benefits team members by providing a completely unambiguous and transparent accounting of what needs doing next. Whichever task is most important, for the whole organization, is on top.
Priorities with autonomy A single priority doesn&rsquo;t necessarily pigeonhole someone into doing a task they don&rsquo;t feel cut out for. Each member of your team has different strengths, skill sets, and diverse ways of thinking. You can take full advantage of this by encouraging autonomy in task selection.
Have people choose whichever task is nearest to the top that they&rsquo;d like to tackle. They might pick the highest priority task that&rsquo;s in their wheelhouse, or experiment with a higher one that&rsquo;s in a domain they&rsquo;d like to improve their skills at.
Embrace opportunities for cross-training. If tasks high up on the list fall in a category that only one or a few people on your team are experts in, have your experts partner up with another team member who&rsquo;s taking on the task. By pooling your resources to cross-train across domains, you multiply the capabilities of each team member and your team as a result.
When a task is especially time-sensitive, have several team members swarm on it and distribute the work according to their interests or strengths.
Make yourself redundant Working off a single prioritized to-do list works best when your team members can take on tasks as independently as possible. This is especially important in remote teams where people work asynchronously.
If you&rsquo;re a leader and find that your team members frequently ask you what they should do next, you could be making your team dependent on you. Ask yourself if you&rsquo;re unnecessarily gatekeeping information that would let your team be more autonomous.
A team that overly depends on their leader is not an efficient one. Individual people, such as yourself, don&rsquo;t scale. Don&rsquo;t become a bottleneck to your team&rsquo;s productivity. A successful leader should be able to take several days off on short notice without productivity grinding to a halt.
To support your team&rsquo;s ability to work without you, make your team, product, and company goals painfully available. Put them where people hang out &ndash; your team&rsquo;s message board, chat channel, or document repository, for example. No one should be at a loss when asked what the team wants to achieve next, and why.
Make any applicable resources, style guides, product documents, or links to external documentation painfully available as well. If your team makes a decision about how something should be done, write it down. Don&rsquo;t rely on yours or anyone else&rsquo;s meat brain to remember an important decision, nor make yourself the only resource for recalling it.
Make yourself redundant when it comes to day-to-day work. Doing so empowers your team members to do work without you, think through solutions on their own, and propose paths of action that you probably wouldn&rsquo;t have thought of yourself.
Build happier and more productive teams From first-hand experience as both a team member and leader, I&rsquo;ve seen how encouraging a culture of openness, cross-training, and autonomy makes for happier team members and more productive teams. A single prioritized to-do list, coupled with available documentation and resources, opens the gates to let your technical team be maximally productive.
By removing bottlenecks, you allow people to make more decisions on their own and take ownership of their work. That&rsquo;s a technical team I&rsquo;d be proud to lead.
`,url:"https://victoria.dev/posts/make-your-team-more-productive-by-literally-doing-one-thing/"},"https://victoria.dev/archive/owasp-web-security-testing-guide-v4.2-released/":{title:"OWASP Web Security Testing Guide v4.2 released",tags:["cybersecurity","open-source"],content:`I&rsquo;m very happy and proud to share that the Open Web Application Security Project (OWASP) Web Security Testing Guide v4.2 is now available! This update is the result of a lot of hard work by the repository team and many dedicated contributors. With a team like this, I&rsquo;m honored to be a core maintainer and co-author.
Here&rsquo;s a reprint of the announcement I wrote for owasp.org. If you&rsquo;re interested in security testing for web applications and APIs, this is an update you&rsquo;ll definitely want to check out!
You can become a contributor yourself by joining us on GitHub!
Web Security Testing Guide v4.2 Released Thursday, December 3, 2020
The OWASP Web Security Testing Guide team is proud to announce version 4.2 of the Web Security Testing Guide (WSTG)! In keeping with a continuous delivery mindset, this new minor version adds content as well as improves the existing tests.
In recent years, the Web Security Testing Guide has sought to remain your foremost open source resource for web application testing. Our previous release marked a move from a cumbersome wiki platform to the highly collaborative world of GitHub. Since then, over 61 new contributors pushing over 600 commits have helped to make the WSTG better than ever.
Version 4.2 of the Web Security Testing Guide introduces new testing scenarios, updates existing chapters, and offers an improved reading experience with a clearer writing style and chapter layout. Readers will enjoy easier navigation and consistent testing instructions.
With new improvements to our development workflow, new contributors will find it easier than ever to help build future versions of the WSTG. A clear and concise contributor’s guide and style guide can help you write new tests or ensure existing scenarios stay current. Core maintainers Rick Mitchell, Elie Saad, Rejah Rehim, and Victoria Drake have implemented modern processes like continuous integration with GitHub Actions. New workflows help to build PDFs and make reviewing new additions and updates easier.
We couldn’t be happier to share this new version with you, and we don’t plan to slow down anytime soon. The dedicated volunteers who’ve made this release possible are already hard at work on the next major version of the WSTG. Come join us and become a contributor!
You can read the Web Security Testing Guide v4.2 online or download a PDF on our project page. We greatly appreciate all the authors, editors, reviewers, and readers who make this open source security endeavor worthwhile.
Thank you for being a part of the WSTG!
`,url:"https://victoria.dev/archive/owasp-web-security-testing-guide-v4.2-released/"},"https://victoria.dev/archive/what-is-tcp/ip-layers-and-protocols-explained/":{title:"What is TCP/IP? Layers and protocols explained",tags:["computing","protocols","data"],content:`A significant part of the process of creation is the ability to imagine things that do not yet exist. This skill was instrumental to the creation of the Internet. If no one had imagined the underlying technology that most now take for granted every day, there would be no cat memes.
To make the Internet possible, two things that needed imagining are layers and protocols. Layers are conceptual divides that group similar functions together. The word &ldquo;protocol,&rdquo; means &ldquo;the way we&rsquo;ve agreed to do things around here,&rdquo; more or less. In short, both layers and protocols can be explained to a five-year-old as &ldquo;ideas that people agreed sounded good, and then they wrote them down so that other people could do things with the same ideas.&rdquo;
The Internet Protocol Suite is described in terms of layers and protocols. Collectively, the suite refers to the communication protocols that enable our endless scrolling. It&rsquo;s often called by its foundational protocols: the Transmission Control Protocol (TCP) and the Internet Protocol (IP). Lumped together as TCP/IP, these protocols describe how data on the Internet is packaged, addressed, sent, and received.
Here&rsquo;s why the Internet Protocol Suite, or TCP/IP, is an imaginary rainbow layer cake.
Layers are imaginary If you consider the general nature of a rainbow layer sponge cake, it&rsquo;s mostly made up of soft, melt-in-your mouth vanilla-y goodness. This goodness is in itself comprised of something along the lines of eggs, butter, flour, and sweetener.
There isn&rsquo;t much to distinguish one layer of a rainbow sponge cake from another. Often, the only difference between layers is the food-coloring and a bit of frosting. When you think about it, it&rsquo;s all cake from top to bottom. The rainbow layers are only there because the baker thought they ought to be.
Similar to cake ingredients, layers in the context of computer networking are mostly composed of protocols, algorithms, and configurations, with some data sprinkled in. It can be easier to talk about computer networking if its many functions are split up into groups, so certain people came up with descriptions of layers, which we call network models. TCP/IP is just one network model among others. In this sense, layers are concepts, not things.
Some of the people in question are part of the Internet Engineering Task Force (IETF). They created the RFC-1122 publication, discussing the Internet&rsquo;s communications layers. Half of a whole, the standard:
&hellip;covers the communications protocol layers: link layer, IP layer, and transport layer; its companion RFC-1123 covers the application and support protocols.
The layers described by RFC-1122 and RFC-1123 each encapsulate protocols that satisfy the layer&rsquo;s functionality. Let&rsquo;s look at each of these communications layers and see how TCP and IP stack up in this model of the Internet layer cake.
Link layer protocols The link layer is the most basic, or lowest-level, classification of communication protocol. It deals with sending information between hosts on the same local network, and translating data from the higher layers to the physical layer. Protocols in the link layer describe how data interacts with the transmission medium, such as electronic signals sent over specific hardware. Unlike other layers, link layer protocols are dependent on the hardware being used.
Internet layer protocols Protocols in the Internet layer describe how data is sent and received over the Internet. The process involves packaging data into packets, addressing and transmitting packets, and receiving incoming packets of data.
The most widely known protocol in this layer gives TCP/IP its last two letters. IP is a connectionless protocol, meaning that it provides no guarantee that packets are sent or received in the right order, along the same path, or even in their entirety. Reliability is handled by other protocols in the suite, such as in the transport layer.
There are currently two versions of IP in use: IPv4, and IPv6. Both versions describe how devices on the Internet are assigned IP addresses, which are used when navigating to cat memes. IPv4 is more widely used, but has only 32 bits for addressing, allowing for about 4.3 billion (ca. 4.3×109) possible addresses. These are running out, and IPv4 and will eventually suffer from address exhaustion as more and more people use more devices on the Internet.
The successor version IPv6 aims to solve address exhaustion by using 128 bits for addresses. This provides, um, a lot more address possibilities (ca. 3.4×1038).
Transport layer protocols In May 1974, Vint Cerf and Bob Kahn (collectively often called &ldquo;the fathers of the Internet&rdquo;) published a paper entitled A Protocol for Packet Network Intercommunication. This paper contained the first description of a Transmission Control Program, a concept encompassing what would eventually be known as the Transmission Control Protocol (TCP) and User Datagram Protocol (UDP). (I had the pleasure of meeting Vint and can personally confirm that yes, he does look exactly like The Architect in the Matrix movies.)
The transport layer presently encapsulates TCP and UDP. Like IP, UDP is connectionless and can be used to prioritize time over reliability. TCP, on the other hand, is a connection-oriented transport layer protocol that prioritizes reliability over latency, or time. TCP describes transferring data in the same order as it was sent, retransmitting lost packets, and controls affecting the rate of data transmission.
Application layer protocols The application layer describes the protocols that software applications interact with most often. The specification includes descriptions of the remote login protocol Telnet, the File Transfer Protocol (FTP), and the Simple Mail Transfer Protocol (SMTP).
Also included in the application layer are the Hypertext Transfer Protocol (HTTP) and its successor, Hypertext Transfer Protocol Secure (HTTPS). HTTPS is secured by Transport Layer Security, or TLS, which can be said to be the top-most layer of the networking model described by the Internet protocol suite. If you&rsquo;d like to further understand TLS and how this protocol secures your cat meme viewing, I invite you read my article about TLS and cryptography.
The Internet cake is still baking Like a still-rising sponge cake, descriptions of layers, better protocols, and new models are being developed every day. The Internet, or whatever it will become in the future, is still in the process of being imagined.
If you enjoyed learning from this post, there&rsquo;s a lot more where this came from! I write about computing, cybersecurity, and building great technical teams. Subscribe to see new articles first.
`,url:"https://victoria.dev/archive/what-is-tcp/ip-layers-and-protocols-explained/"},"https://victoria.dev/archive/responsive-pages-and-color-themes-with-minimal-css/":{title:"Responsive pages and color themes with minimal CSS",tags:["websites","coding"],content:`Hello, do come in! If you&rsquo;re reading this on my website, you may notice I&rsquo;ve spruced up a bit. Victoria.dev can now better respond to your devices and preferences!
Most modern devices and web browsers allow users to choose either a light or dark theme for the user interface. With CSS media queries, you can have your own website&rsquo;s styles change to match this user setting!
Media queries are also a common way to have elements on web pages change to suit different screen sizes. This is an especially powerful tool when combined with custom properties set on the root element.
Here&rsquo;s how to use CSS media queries and custom properties to improve your visitor&rsquo;s browsing experience with just a few lines of CSS.
Catering to color preferences The prefers-color-scheme media feature can be queried to serve up your user&rsquo;s color scheme of choice. The light option is the go-to version if no active preference is set, and it has decent support across modern browsers.
Additionally, users reading on certain devices can also set light and dark color themes based on a schedule. For example, my phone uses light colors throughout its UI during the daytime, and dark colors at night. You can make your website follow suit!
Avoid repeating a lot of CSS by setting custom properties for your color themes on your :root pseudo-class. You can specify the themes available with the color-scheme property (currently part of a draft specification, but I like to write my articles to age well). Create a version for each theme you wish to support. Here&rsquo;s a quick example you can build on:
:root { color-scheme: light dark; } @media (prefers-color-scheme: light) { :root { --text-primary: #24292e; --background: white; --shadow: rgba(0, 0, 0, 0.15) 0px 2px 5px 0px; } } @media (prefers-color-scheme: dark) { :root { --text-primary: white; --background: #24292e; --shadow: rgba(0, 0, 0, 0.35) 0px 2px 5px 0px; } } As you can see, you can use custom properties to set all kinds of values. To use these as variables with other CSS elements, use the var() function:
header { color: var(--text-primary); background-color: var(--background); box-shadow: var(--shadow); } In this quick example, the header element will now display your user&rsquo;s preferred colors according to their browser settings!
Preferred color schemes are set by the user in different ways, depending on the browser. Here are a couple examples.
Firefox You can test out light and dark modes in Firefox by typing about:config into the address bar. Accept the warning if it pops up, then type ui.systemUsesDarkTheme into the search.
Choose a Number value for the setting, then input a 1 for dark or 0 for light.
Brave If you&rsquo;re using Brave, find color theme settings in Settings &gt; Appearance &gt; Brave colors.
Variable scaling You can also use a custom property to effortlessly adjust the size of text or other elements depending on your user&rsquo;s screen size. The width media feature tests the width of the viewport. While width: _px will match an exact size, you can also use min and max to create ranges.
Query with min-width: _px to match anything over _ pixels, and max-width: _px to match anything up to _ pixels.
Use these queries to set a custom property on the :root to create a ratio:
@media (min-width: 360px) { :root { --scale: 0.8; } } @media (min-width: 768px) { :root { --scale: 1; } } @media (min-width: 1024px) { :root { --scale: 1.2; } } Then make an element responsive by using the calc() function. Here are a few examples:
h1 { font-size: calc(42px * var(--scale)); } h2 { font-size: calc(26px * var(--scale)); } img { width: calc(200px * var(--scale)); } In this example, multiplying an initial value by your --scale custom property allows the size of headings and images to magically adjust to your user&rsquo;s device width.
The relative unit rem will have a similar effect. You can use it to define sizes for elements relative to the font size declared at the root element.
h1 { font-size: calc(5rem * var(--scale)); } h2 { font-size: calc(1.5rem * var(--scale)); } p { font-size: calc(1rem * var(--scale)); } Of course, you can also multiply two custom properties. For example, setting the --max-img as a custom property on the :root can help to save you time later on by not having to update a pixel value in multiple places:
img { max-width: calc(var(--max-img) * var(--scale)); } Raise your responsiveness game Try out these easy wins for a website that caters to your visitor&rsquo;s devices and preferences. I&rsquo;ve put them to good use now on victoria.dev. I invite you to let me know how you like it!
`,url:"https://victoria.dev/archive/responsive-pages-and-color-themes-with-minimal-css/"},"https://victoria.dev/archive/build-your-own-serverless-subscriber-list-with-go-and-aws/":{title:"Build your own serverless subscriber list with Go and AWS",tags:["api","aws","coding","go","data","cybersecurity","websites"],content:`You can now subscribe to my email list on victoria.dev! Here&rsquo;s how I lovingly built a subscription sign up flow with email confirmation that doesn&rsquo;t suck. You can too.
Introducing Simple Subscribe If you&rsquo;re interested in managing your own mailing list or newsletter, you can set up Simple Subscribe on your own AWS resources to collect email addresses. This open source API is written in Go, and runs on AWS Lambda. Visitors to your site can sign up to your list, which is stored in a DynamoDB table, ready to be queried or exported at your leisure.
When someone signs up, they&rsquo;ll receive an email asking them to confirm their subscription. This is sometimes called &ldquo;double opt-in,&rdquo; although I prefer the term &ldquo;verified.&rdquo; Simple Subscribe works on serverless infrastructure and uses an AWS Lambda to handle subscription, confirmation, and unsubscribe requests.
You can find the Simple Subscribe project, with its fully open-source code, on GitHub. I encourage you to pull up the code and follow along! In this post I&rsquo;ll share each build step, the thought process behind the API&rsquo;s single-responsibility functions, and security considerations for an AWS project like this one.
Building a verified subscription flow A non-verified email sign up process is straightforward. Someone puts their email into a box on your website, then that email goes into your database. However, if I&rsquo;ve taught you anything about not trusting user input, the very idea of a non-verified sign up process should raise your hackles. Spam may be great when fried in a sandwich, but no fun when it&rsquo;s running up your AWS bill.
While you can use a strategy like a CAPTCHA or puzzle for is-it-a-human verification, these can create enough friction to turn away your potential subscribers. Instead, a confirmation email can help to ensure both address correctness and user sentience.
To build a subscription flow with email confirmation, create single-responsibility functions that satisfy each logical step. Those are:
Accept an email address and record it. Generate a token associated with that email address and record it. Send a confirmation email to that email address with the token. Accept a verification request that has both the email address and token. To achieve each of these goals, Simple Subscribe uses the official AWS SDK for Go to interact with DynamoDB and SES.
At each stage, consider what the data looks like and how you store it. This can help to handle conundrums like, &ldquo;What happens if someone tries to subscribe twice?&rdquo; or even threat-modeling such as, &ldquo;What if someone subscribes with an email they don&rsquo;t own?&rdquo;
Ready? Let&rsquo;s break down each step and see how the magic happens.
Subscribing The subscription process begins with a humble web form, like the one on my site&rsquo;s main page. A form input with attributes type=&quot;email&quot; required helps with validation, thanks to the browser. When submitted, the form sends a GET request to the Simple Subscribe subscription endpoint.
Simple Subscribe receives a GET request to this endpoint with a query string containing the intended subscriber&rsquo;s email. It then generates an id value and adds both email and id to your DynamoDB table.
The table item now looks like:
email confirm id timestamp subscriber@example.com false uuid-xxxxx 2020-11-01 00:27:39 The confirm column, which holds a boolean, indicates that the item is a subscription request that has not yet been confirmed. To verify an email address in the database, you&rsquo;ll need to find the correct item and change confirm to true.
As you work with your data, consider the goal of each manipulation and how you might compare an incoming request to existing data.
For example, if someone made a subsequent subscription request for the same email address, how would you handle it? You might say, &ldquo;Create a new line item with a new id,&rdquo; however, this might not be best strategy when your serverless application database is paid for by request volume.
Since DynamoDB Pricing depends on how much data you read and write to your tables, it&rsquo;s advantageous to avoid piling on excess data.
With that in mind, it would be prudent to handle subscription requests for the same email by performing an update instead of adding a new line. Simple Subscribe actually uses the same function to either add or update a database item. This is typically referred to as, &ldquo;update or insert.&rdquo;
In a database like SQLite this is accomplished with the UPSERT syntax. In the case of DynamoDB, you use an update operation. For the Go SDK, its syntax is UpdateItem.
When a duplicate subscription request is received, the database item is matched on the email only. If an existing line item is found, its id and timestamp are overridden, which updates the existing database record and avoids flooding your table with duplicate requests.
Verifying email addresses After submitting the form, the intended subscriber then receives an email from SES containing a link. This link is built using the email and id from the table, and takes the format:
&lt;BASE_URL&gt;&lt;VERIFY_PATH&gt;/?email=subscriber@example.com&amp;id=uuid-xxxxx In this set up, the id is a UUID that acts as a secret token. It provides an identifier that you can match that is sufficiently complex and hard to guess. This approach deters people from subscribing with email addresses they don&rsquo;t control.
Visiting the link sends a request to your verification endpoint with the email and id in the query string. This time, it&rsquo;s important to compare both the incoming email and id values to the database record. This verifies that the recipient of the confirmation email is initiating the request.
The verification endpoint ensures that these values match an item in your database, then performs another update operation to set confirm to true, and update the timestamp. The item now looks like:
email confirm id timestamp subscriber@example.com true uuid-xxxxx 2020-11-01 00:37:39 Querying for emails You can now query your table to build your email list. Depending on your email sending solution, you might do this manually, with another Lambda, or even from the command line.
Since data for requested subscriptions (where confirm is false) is stored in the table alongside confirmed subscriptions, it&rsquo;s important to differentiate this data when querying for email addresses to send to. You&rsquo;ll want to ensure you only return emails where confirm is true.
Providing unsubscribe links Similar to verifying an email address, Simple Subscribe uses email and id as arguments to the function that deletes an item from your DynamoDB table in order to unsubscribe an email address. To allow people to remove themselves from your list, you&rsquo;ll need to provide a URL in each email you send that includes their email and id as a query string to the unsubscribe endpoint. It would look something like:
&lt;BASE_URL&gt;&lt;UNSUBSCRIBE_PATH&gt;/?email=subscriber@example.com&amp;id=uuid-xxxxx When the link is clicked, the query string is passed to the unsubscribe endpoint. If the provided email and id match a database item, that item will be deleted.
Proving a method for your subscribers to automatically remove themselves from your list, without any human intervention necessary, is part of an ethical and respectful philosophy towards handling the data that&rsquo;s been entrusted to you.
Caring for your data Once you decide to accept other people&rsquo;s data, it becomes your responsibility to care for it. This is applicable to everything you build. For Simple Subscribe, it means maintaining the security of your database, and periodically pruning your table.
In order to avoid retaining email addresses where confirm is false past a certain time frame, it would be a good idea to set up a cleaning function that runs on a regular schedule. This can be achieved manually, with an AWS Lambda function, or using the command line.
To clean up, find database items where confirm is false and timestamp is older than a particular point in time. Depending on your use case and request volumes, the frequency at which you choose to clean up will vary.
Also depending on your use case, you may wish to keep backups of your data. If you are particularly concerned about data integrity, you can explore On-Demand Backup or Point-in-Time Recovery for DynamoDB.
Build your independent subscriber base Building your own subscriber list can be an empowering endeavor! Whether you intend to start a newsletter, send out notifications for new content, or want to create a community around your work, there&rsquo;s nothing more personal or direct than an email from me to you.
I encourage you to start building your subscriber base with Simple Subscribe today! Like most of my work, it&rsquo;s open source and free for your personal use. Dive into the code at the GitHub repository or learn more at SimpleSubscribe.org.
`,url:"https://victoria.dev/archive/build-your-own-serverless-subscriber-list-with-go-and-aws/"},"https://victoria.dev/posts/wpa-key-wpa2-wpa3-and-wep-key-wi-fi-security-explained/":{title:"WPA Key, WPA2, WPA3, and WEP Key: Wi-Fi security explained",tags:["computing","algorithms","protocols","cybersecurity"],content:`Setting up new Wi-Fi? Picking the type of password you need can seem like an arbitrary choice. After all, WEP, WPA, WPA2, and WPA3 all have mostly the same letters in them. A password is a password, so what&rsquo;s the difference?
About 60 seconds to billions of years, as it turns out.
All Wi-Fi encryption is not created equal. Let&rsquo;s explore what makes these four acronyms so different, and how you can best protect your home and organization Wi-Fi.
Wired Equivalent Privacy (WEP) In the beginning, there was WEP.
Not to be confused with the name of a certain rap song.
Wired Equivalent Privacy is a deprecated security algorithm from 1997 that was intended to provide equivalent security to a wired connection. &ldquo;Deprecated&rdquo; means, &ldquo;Let&rsquo;s not do that anymore.&rdquo;
Even when it was first introduced, it was known not to be as strong as it could have been, for two reasons: one, its underlying encryption mechanism; and two, World War II.
During World War II, the impact of code breaking (or cryptanalysis) was huge. Governments reacted by attempting to keep their best secret-sauce recipes at home. Around the time of WEP, U.S. Government restrictions on the export of cryptographic technology caused access point manufacturers to limit their devices to 64-bit encryption. Though this was later lifted to 128-bit, even this form of encryption offered a very limited possible key size.
This proved problematic for WEP. The small key size resulted in being easier to brute-force, especially when that key doesn&rsquo;t often change.
WEP&rsquo;s underlying encryption mechanism is the RC4 stream cipher. This cipher gained popularity due to its speed and simplicity, but that came at a cost. It&rsquo;s not the most robust algorithm. WEP employs a single shared key among its users that must be manually entered on an access point device. (When&rsquo;s the last time you changed your Wi-Fi password? Right.) WEP didn&rsquo;t help matters either by simply concatenating the key with the initialization vector &ndash; which is to say, it sort of mashed its secret-sauce bits together and hoped for the best.
Initialization Vector (IV): fixed-size input to a low-level cryptographic algorithm, usually random.
Combined with the use of RC4, this left WEP particularly susceptible to related-key attack. In the case of 128-bit WEP, your Wi-Fi password can be cracked by publicly-available tools in a matter of around 60 seconds to three minutes.
While some devices came to offer 152-bit or 256-bit WEP variants, this failed to solve the fundamental problems of WEP&rsquo;s underlying encryption mechanism.
So, yeah. Let&rsquo;s not do that anymore.
Wi-Fi Protected Access (WPA) A new, interim standard sought to temporarily &ldquo;patch&rdquo; the problem of WEP&rsquo;s (lack of) security. The name Wi-Fi Protected Access (WPA) certainly sounds more secure, so that&rsquo;s a good start; however, WPA first started out with another, more descriptive name.
Ratified in a 2004 IEEE standard, Temporal Key Integrity Protocol (TKIP) uses a dynamically-generated, per-packet key. Each packet sent has a unique temporal 128-bit key, (See? Descriptive!) that solves the susceptibility to related-key attacks brought on by WEP&rsquo;s shared key mashing.
TKIP also implements other measures, such as a message authentication code (MAC). Sometimes known as a checksum, a MAC provides a cryptographic way to verify that messages haven&rsquo;t been changed. In TKIP, an invalid MAC can also trigger rekeying of the session key. If the access point receives an invalid MAC twice within a minute, the attempted intrusion can be countered by changing the key an attacker is trying to crack.
Unfortunately, in order to preserve compatibility with the existing hardware that WPA was meant to &ldquo;patch,&rdquo; TKIP retained the use of the same underlying encryption mechanism as WEP &ndash; the RC4 stream cipher. While it certainly improved on the weaknesses of WEP, TKIP eventually proved vulnerable to new attacks that extended previous attacks on WEP. These attacks take a little longer to execute by comparison: for example, twelve minutes in the case of one, and 52 hours in another. This is more than sufficient, however, to deem TKIP no longer secure.
WPA, or TKIP, has since been deprecated as well. So let&rsquo;s also not do that anymore.
Which brings us to&hellip;
Wi-Fi Protected Access II (WPA2) Rather than spend the effort to come up with an entirely new name, the improved Wi-Fi Protected Access II (WPA2) standard instead focuses on using a new underlying cipher. Instead of the RC4 stream cipher, WPA2 employs a block cipher called Advanced Encryption Standard (AES) to form the basis of its encryption protocol. The protocol itself, abbreviated CCMP, draws most of its security from the length of its rather long name (I&rsquo;m kidding): Counter Mode Cipher Block Chaining Message Authentication Code Protocol, which shortens to Counter Mode CBC-MAC Protocol, or CCM mode Protocol, or CCMP. 🤷
CCM mode is essentially a combination of a few good ideas. It provides data confidentiality through CTR mode, or counter mode. To vastly oversimplify, this adds complexity to plaintext data by encrypting the successive values of a count sequence that does not repeat. CCM also integrates CBC-MAC, a block cipher method for constructing a MAC.
AES itself is on good footing. The AES specification was established in 2001 by the U.S. National Institute of Standards and Technology (NIST) after a five-year competitive selection process during which fifteen proposals for algorithm designs were evaluated. As a result of this process, a family of ciphers called Rijndael (Dutch) was selected, and a subset of these became AES. For the better part of two decades, AES has been used to protect every-day Internet traffic as well as certain levels of classified information in the U.S. Government.
While possible attacks on AES have been described, none have yet been proven to be practical in real-world use. The fastest attack on AES in public knowledge is a key-recovery attack that improved on brute-forcing AES by a factor of about four. How long would it take? Some billions of years.
Wi-Fi Protected Access III (WPA3) The next installment of the WPA trilogy has been required for new devices since July 1, 2020. Expected to further enhance the security of WPA2, the WPA3 standard seeks to improve password security by being more resilient to word list or dictionary attacks.
Unlike its predecessors, WPA3 will also offer forward secrecy. This adds the considerable benefit of protecting previously exchanged information even if a long-term secret key is compromised. Forward secrecy is already provided by protocols like TLS by using asymmetric keys to establish shared keys. You can learn more about TLS in this post.
As WPA2 has not been deprecated, both WPA2 and WPA3 remain your top choices for Wi-Fi security.
If the other ones suck, why are they still around? You may be wondering why your access point even allows you to choose an option other than WPA2 or WPA3. The likely reason is that you&rsquo;re using legacy hardware, which is what tech people call your mom&rsquo;s router.
Since the deprecation of WEP and WPA occurred (in old-people terms) rather recently, it&rsquo;s possible in large organizations as well as your parent&rsquo;s house to find older hardware that still uses these protocols. Even newer hardware may have a business need to support these older protocols.
While I may be able to convince you to invest in a shiny new top-of-the-line Wi-Fi appliance, most organizations are a different story. Unfortunately, many just aren&rsquo;t yet cognizant of the important role cybersecurity plays in meeting customer needs and boosting that bottom line. Additionally, switching to newer protocols may require new internal hardware or firmware upgrades. Especially on complex systems in large organizations, upgrading devices can be financially or strategically difficult.
Boost your Wi-Fi security If it&rsquo;s an option, choose WPA2 or WPA3. Cybersecurity is a field that evolves by the day, and getting stuck in the past can have dire consequences.
If you can&rsquo;t use WPA2 or WPA3, do the best you can to take additional security measures. The best bang for your buck is to use a Virtual Private Network (VPN). Using a VPN is a good idea no matter which type of Wi-Fi encryption you have. On open Wi-Fi (coffee shops) and using WEP, it&rsquo;s plain irresponsible to go without a VPN. Kind of like shouting out your bank details as you order your second cappuccino.
When possible, ensure you only connect to known networks that you or your organization control. Many cybersecurity attacks are executed when victims connect to an imitation public Wi-Fi access point, also called an evil twin attack, or Wi-Fi phishing. These fake hotspots are easily created using publicly accessible programs and tools. A reputable VPN can help mitigate damage from these attacks as well, but it&rsquo;s always better not to take the risk. If you travel often, consider purchasing a portable hotspot that uses a cellular data plan, or using data SIM cards for all your devices.
Much more than just acronyms WEP, WPA, WPA2, and WPA3 mean a lot more than a bunch of similar letters &ndash; in some cases, it&rsquo;s a difference of billions of years minus about 60 seconds.
On more of a now-ish timescale, I hope I&rsquo;ve taught you something new about the security of your Wi-Fi and how you can improve it!
`,url:"https://victoria.dev/posts/wpa-key-wpa2-wpa3-and-wep-key-wi-fi-security-explained/"},"https://victoria.dev/archive/your-cybersecurity-starter-pack/":{title:"Your cybersecurity starter pack",tags:["cybersecurity"],content:`Readers of my blog typically know more about technology and cybersecurity than most people. This article is for most people. If someone you know could benefit from a simple and straightforward introduction to cybersecurity tools, please share this article with them &ndash; it benefits everyone!
If you&rsquo;ve ever said to yourself:
&ldquo;There&rsquo;s no one targeting lil ol&rsquo; me.&rdquo;
&ldquo;I have nothing to hide, anyway.&rdquo;
&ldquo;I&rsquo;m too busy to learn all this stuff. Why can&rsquo;t someone just give me a simple summary of best practices that I can skim in approximately seven minutes?&rdquo;
First of all, you might want to stop talking to yourself in public. Secondly, here is a simple summary of best practices that you can skim in approximately seven minutes.
Introducing your three-step starter pack While there are many different degrees of security, privacy, and anonymity, these three basics are accessible to all:
Use a VPN Use multifactor authentication Develop a healthy sense of skepticism I&rsquo;ll discuss each of these and help you get started with your security upgrade. But first&hellip;
Why is cybersecurity important? Would you let just anyone walk into your house, or even look through your open doorway from across the street? If not, you might appreciate that the cybersecurity practices we&rsquo;ll discuss today are not that different from locking your front door.
Cybersecurity isn&rsquo;t about finding some magic spell that completely secures your online activities &ndash; that would be nice, but it&rsquo;s unrealistic. Good security practices are about employing some thoughtful habits that make your online activities more secure than the next guy, in much the same way as you learned to lock your front door.
Security breaches and incidents happen every day. Most of them occur because an automated scanner cast a wide net and found a person or company with lax security that a hacker could then exploit. Don&rsquo;t be that guy.
1. Use a VPN Let&rsquo;s say you send a lot of mail, but never bother to put your letters in envelopes or even fold them in half. Anyone who bothers to look can read all your dirty secrets (not that you have any).
When you use a Virtual Private Network, or VPN, especially if you often connect to public WiFi, it&rsquo;s like putting your letters into cryptographically-sealed envelopes and sending them via a special invisible courier service. No one but the intended recipient can read your letters, and no one but you and the courier know to whom the letters are sent.
Encrypted mail still won&rsquo;t stop you from the accidental reply all, unfortunately.
VPNs prevent others from reading your communications. This may include opportunistic attackers who scan open WiFi, and even your own internet service provider (ISP) who may sell your usage data for advertising dollars.
Choosing a VPN A few important differentiating factors can help you choose a VPN provider.
Is it free? VPNs cost money to operate; if one is offered for free, consider what they might be doing in order to cover their costs. Generally, I recommend avoiding free VPN apps and services; they&rsquo;ll typically cost you much more than you&rsquo;ll know. Expect to pay between $5-$10 USD monthly for the service.
Where is it based? Understand where your VPN provider is based, and what that country&rsquo;s laws allow them to do with your data.
Do they keep logs? Part of the philosophy of using a VPN is that no one has any business getting into your business when it comes to online activities. When a VPN provider keeps logs of your usage, that defeats the purpose. Instead of your ISP knowing just what you&rsquo;re up to online, that knowledge is simply transferred to the logging VPN. Look for VPN providers with a strict no-logging policy, or if you&rsquo;re up for it, roll your own.
2. Use multifactor authentication Passwords are dead. Computationally, they are a solved problem. Cracking your password is just a matter of time.
Unfortunately, many people still help to speed up the process by using the same compromised passwords for multiple accounts, putting themselves at further risk.
The answer, at least for now, is multifactor authentication (MFA). MFA is made up of three kinds of authentication factors:
Something you know, like a pass phrase; Something you have, like a chip pin card or phone; and Something that you are, like your face or fingerprint. Also the name of my next beatboxing team.
Two or more of these factors are infinitely better than a password alone, especially if your password is on this list.
Multiple authentication factors are now widely supported by account providers and social media sites. If you have the choice, avoid using text messages, or SMS, as a way of receiving authentication codes. SMS authentication leaves you vulnerable to the SIM swap attack - please direct further questions to Jack Dorsey.
Instead, use a One Time Password (OTP) app such as Authy to generate codes on your device. This ensures that you alone, using that particular device, will have the correct authentication code.
You can also use hardware authentication keys such as the YubiKey, but these aren&rsquo;t yet as widely supported as OTP apps.
3. Develop a healthy sense of skepticism Social engineering, sometimes SE, is the use of psychological persuasion to get an unwitting target to give up access or information. This can take the form of phishing emails, letters, or phone calls (vishing) as well as far more sophisticated spear-phishing attacks of high-value targets, like company executives.
While some attacks are easier to spot, others use cognitive biases very effectively and are difficult even for security professionals to avoid. No human is immune.
Ultimately, the weakest link in your cybersecurity defense is you. All the VPNs and MFA on the Internet won&rsquo;t protect you if a scam can trick you into opening the front gates. Always look a Trojan gift horse in the mouth.
Yes, I know it&rsquo;s a very nice looking wooden horse. Also free. Did you order it? No? Then it can stay outside.
Develop the habit of second-guessing things delivered to your virtual doorstep. Email, phone, and messaging scams range in sophistication. Even security professionals can fall for a good scam.
One way to protect yourself is to practice a healthy sense of skepticism. Question communications that ask you to click on links or visit a website, even if they come from someone you know or a company you use.
If you&rsquo;re not certain that your bank or mother sent this email, pick up the phone and call them. Even if you think you are certain, pick up the phone and double check. You don&rsquo;t call your mother enough, anyway.
Oh, and if the person on the phone is from your local tax office or the IRS or the CRA and they&rsquo;re about to freeze your accounts because a case of mistaken identity has resulted in you being criminally charged for not repaying a loan on a 600-foot yacht in Malibu, just hang up. You know better than that. Tax agencies don&rsquo;t have phones.
A safer Internet Congratulations! You now have three tools to make your personal cybersecurity better than the next guy&rsquo;s. If enough people do that, the whole neighborhood (or in this case, the Internet) will benefit as a result.
If this article piqued your interest, you can go further and outsource your security with a password manager and temporary virtual credit cards.
Cheat sheets and other resources I&rsquo;ll leave you with a few resources that I&rsquo;ve enjoyed:
The Electronic Frontier Foundation website Surveillance Self Defense offers many great guides and how-to&rsquo;s, such as setting up the encrypted messaging app Signal on your mobile device, and protecting yourself on social media. The Cybersecurity and Infrastructure Security Agency (CISA) offers many shareable starter resources. Working from home? The National Security Agency Central Security Service has Telework and Mobile Security Guides that discuss best practices for an unprecedented era of remote work. `,url:"https://victoria.dev/archive/your-cybersecurity-starter-pack/"},"https://victoria.dev/tags/django/":{title:"Django",tags:[],content:"",url:"https://victoria.dev/tags/django/"},"https://victoria.dev/posts/increase-developer-confidence-with-a-great-django-test-suite/":{title:"Increase developer confidence with a great Django test suite",tags:["django","coding","leadership","python"],content:`Done correctly, tests are one of your application&rsquo;s most valuable assets.
The Django framework in particular offers your team the opportunity to create an efficient testing practice. Based on the Python standard library unittest, proper tests in Django are fast to write, faster to run, and can offer you a seamless continuous integration solution for taking the pulse of your developing application.
With comprehensive tests, developers have higher confidence when pushing changes. I&rsquo;ve seen firsthand in my own teams that good tests can boost development velocity as a direct result of a better developer experience.
In this article, I&rsquo;ll share my own experiences in building useful tests for Django applications, from the basics to the best possible execution. If you&rsquo;re using Django or building with it in your organization, you might like to read the rest of my Django series.
What to test Tests are extremely important. Far beyond simply letting you know if a function works, tests can form the basis of your team&rsquo;s understanding of how your application is intended to work.
Here&rsquo;s the main goal: if you hit your head and forgot everything about how your application works tomorrow, you should be able to regain most of your understanding by reading and running the tests you write today.
Here are some questions that may be helpful to ask as you decide what to test:
What is our customer supposed to be able to do? What is our customer not supposed to be able to do? What should this method, view, or logical flow achieve? When, how, or where is this feature supposed to execute? Tests that make sense for your application can help build developer confidence. With these sensible safeguards in place, developers make improvements more readily, and feel confident introducing innovative solutions to product needs. The result is an application that comes together faster, and features that are shipped often and with confidence.
Where to put tests If you only have a few tests, you may organize your test files similarly to Django&rsquo;s default app template by putting them all in a file called tests.py. This straightforward approach is best for smaller applications.
As your application grows, you may like to split your tests into different files, or test modules. One method is to use a directory to organize your files, such as projectroot/app/tests/. The name of each test file within that directory should begin with test, for example, test_models.py.
Besides being aptly named, Django will find these files using built-in test discovery based on the unittest module. All files in your application with names that begin with test will be collected into a test suite.
This convenient test discovery allows you to place test files anywhere that makes sense for your application. As long as they&rsquo;re correctly named, Django&rsquo;s test utility can find and run them.
How to document a test Use docstrings to explain what a test is intended to verify at a high level. For example:
def test_create_user(self): &#34;&#34;&#34;Creating a new user object should also create an associated profile object&#34;&#34;&#34; # ... These docstrings help you quickly understand what a test is supposed to be doing. Besides navigating the codebase, this helps to make it obvious when a test doesn&rsquo;t verify what the docstring says it should.
Docstrings are also shown when the tests are being run, which can be helpful for logging and debugging.
What a test needs to work Django tests can be quickly set up using data created in the setUpTestData() method. You can use various approaches to create your test data, such as utilizing external files, or even hard-coding silly phrases or the names of your staff. Personally, I much prefer to use a fake-data-generation library, such as faker.
The proper set up of arbitrary testing data can help you ensure that you&rsquo;re testing your application functionality instead of accidentally testing test data. Because generators like faker add some degree of unexpectedness to your inputs, it can be more representative of real-world use.
Here is an example set up for a test:
from django.test import TestCase from faker import Faker from app.models import MyModel, AnotherModel fake = Faker() class MyModelTest(TestCase): def setUpTestData(cls): &#34;&#34;&#34;Quickly set up data for the whole TestCase&#34;&#34;&#34; cls.user_first = fake.first_name() cls.user_last = fake.last_name() def test_create_models(self): &#34;&#34;&#34;Creating a MyModel object should also create AnotherModel object&#34;&#34;&#34; # In test methods, use the variables created above test_object = MyModel.objects.create( first_name=self.user_first, last_name=self.user_last, # ... ) another_model = AnotherModel.objects.get(my_model=test_object) self.assertEqual(another_model.first_name, self.user_first) # ... Tests pass or fail based on the outcome of the assertion methods. You can use Python&rsquo;s unittest methods, and Django&rsquo;s assertion methods.
For further guidance on writing tests, see Testing in Django.
Best possible execution for running your tests Django&rsquo;s test suite is manually run with:
./manage.py test I rarely run my Django tests this way.
The best, or most efficient, testing practice is one that occurs without you or your developers ever thinking, &ldquo;I need to run the tests first.&rdquo; The beauty of Django&rsquo;s near-effortless test suite set up is that it can be seamlessly run as a part of regular developer activities. This could be in a pre-commit hook, or in a continuous integration or deployment workflow.
I&rsquo;ve previously written about how to use pre-commit hooks to improve your developer ergonomics and save your team some brainpower. Django&rsquo;s speedy tests can be run this way, and they become especially efficient if you can run tests in parallel.
Tests that run as part of a CI/CD workflow, for example, on pull requests with GitHub Actions, require no regular effort from your developers to remember to run tests at all. I&rsquo;m not sure how plainly I can put it &ndash; this one&rsquo;s literally a no-brainer.
Testing your way to a great Django application Tests are extremely important, and underappreciated. They can catch logical errors in your application. They can help explain and validate how concepts and features of your product actually function. Best of all, tests can boost developer confidence and development velocity as a result.
The best tests are ones that are relevant, help to explain and define your application, and are run continuously without a second thought. I hope I&rsquo;ve now shown you how testing in Django can help you to achieve these goals for your team!
`,url:"https://victoria.dev/posts/increase-developer-confidence-with-a-great-django-test-suite/"},"https://victoria.dev/posts/django-project-best-practices-to-keep-your-developers-happy/":{title:"Django project best practices to keep your developers happy",tags:["django","python","leadership","coding","docs"],content:`Do you want your team to enjoy your development workflow? Do you think building software should be fun and existentially fulfilling? If so, this is the post for you!
I&rsquo;ve been developing with Django for years, and I&rsquo;ve never been happier with my Django project set up than I am right now. Here&rsquo;s how I&rsquo;m making a day of developing with Django the most relaxing and enjoyable development experience possible for myself and my engineering team.
A custom CLI tool for your Django project Instead of typing:
python3 -m venv env source env/bin/activate pip install -r requirements.txt python3 manage.py makemigrations python3 manage.py migrate python3 manage.py collectstatic python3 manage.py runserver Wouldn&rsquo;t it be much nicer to type:
make start &hellip;and have all that happen for you? I think so!
We can do that with a self-documenting Makefile! Here&rsquo;s one I frequently use when developing my Django applications, like ApplyByAPI.com:
VENV := env BIN := $(VENV)/bin PYTHON := $(BIN)/python SHELL := /bin/bash include .env .PHONY: help help: ## Show this help @egrep -h &#39;\\s##\\s&#39; $(MAKEFILE_LIST) | awk &#39;BEGIN {FS = &#34;:.*?## &#34;}; {printf &#34;\\033[36m%-20s\\033[0m %s\\n&#34;, $$1, $$2}&#39; .PHONY: venv venv: ## Make a new virtual environment python3 -m venv $(VENV) &amp;&amp; source $(BIN)/activate .PHONY: install install: venv ## Make venv and install requirements $(BIN)/pip install --upgrade -r requirements.txt freeze: ## Pin current dependencies $(BIN)/pip freeze &gt; requirements.txt migrate: ## Make and run migrations $(PYTHON) manage.py makemigrations $(PYTHON) manage.py migrate db-up: ## Pull and start the Docker Postgres container in the background docker pull postgres docker-compose up -d db-shell: ## Access the Postgres Docker database interactively with psql. Pass in DBNAME=&lt;name&gt;. docker exec -it container_name psql -d $(DBNAME) .PHONY: test test: ## Run tests $(PYTHON) manage.py test application --verbosity=0 --parallel --failfast .PHONY: run run: ## Run the Django server $(PYTHON) manage.py runserver start: install migrate run ## Install requirements, apply migrations, then start development server You&rsquo;ll notice the presence of the line include .env above. This ensures make has access to environment variables stored in a file called .env. This allows Make to utilize these variables in its commands, for example, the name of my virtual environment, or to pass in $(DBNAME) to psql.
What&rsquo;s with that weird &ldquo;##&rdquo; comment syntax? A Makefile like this gives you a handy suite of command-line aliases you can check in to your Django project. It&rsquo;s very useful so long as you&rsquo;re able to remember what all those aliases are.
The help command above, which runs by default, prints a helpful list of available commands when you run make or make help:
help Show this help venv Make a new virtual environment install Make venv and install requirements migrate Make and run migrations db-up Pull and start the Docker Postgres container in the background db-shell Access the Postgres Docker database interactively with psql test Run tests run Run the Django server start Install requirements, apply migrations, then start development server All the usual Django commands are covered, and we&rsquo;ve got a test command that runs our tests with the options we prefer. Brilliant.
You can read my full post about self-documenting Makefiles here, which also includes an example Makefile using pipenv.
Save your brainpower with pre-commit hooks I previously wrote about some technical ergonomics that can make it a lot easier for teams to develop great software.
One area that&rsquo;s a no-brainer is using pre-commit hooks to lint code prior to checking it in. This helps to ensure the quality of the code your developers check in, but most importantly, ensures that no one on your team is spending time trying to remember if it should be single or double quotes or where to put a line break.
The confusingly-named pre-commit framework is an otherwise fantastic way to keep hooks (which are not included in cloned repositories) consistent across local environments.
Here is my configuration file, .pre-commit-config.yaml, for my Django projects:
fail_fast: true repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v3.1.0 hooks: - id: detect-aws-credentials - repo: https://github.com/psf/black rev: 19.3b0 hooks: - id: black - repo: https://github.com/asottile/blacken-docs rev: v1.7.0 hooks: - id: blacken-docs additional_dependencies: [black==19.3b0] - repo: local hooks: - id: markdownlint name: markdownlint description: &#34;Lint Markdown files&#34; entry: markdownlint &#39;**/*.md&#39; --fix --ignore node_modules --config &#34;./.markdownlint.json&#34; language: node types: [markdown] These hooks check for accidental secret commits, format Python files using Black, format Python snippets in Markdown files using blacken-docs, and lint Markdown files as well. To install them, just type pre-commit install.
There are likely even more useful hooks available for your particular use case: see supported hooks to explore.
Useful gitignores An underappreciated way to improve your team&rsquo;s daily development experience is to make sure your project uses a well-rounded .gitignore file. It can help prevent files containing secrets from being committed, and can additionally save developers hours of tedium by ensuring you&rsquo;re never sifting through a git diff of generated files.
To efficiently create a gitignore for Python and Django projects, Toptal&rsquo;s gitignore.io can be a nice resource for generating a robust .gitignore file.
I still recommend examining the generated results yourself to ensure that ignored files suit your use case, and that nothing you want ignored is commented out.
Continuous testing with GitHub Actions If your team works on GitHub, setting up a testing process with Actions is low-hanging fruit.
Tests that run in a consistent environment on every pull request can help eliminate &ldquo;works on my machine&rdquo; conundrums, as well as ensure no one&rsquo;s sitting around waiting for a test to run locally.
A hosted CI environment like GitHub Actions can also help when running integration tests that require using managed services resources. You can use encrypted secrets in a repository to grant the Actions runner access to resources in a testing environment, without worrying about creating testing resources and access keys for each of your developers to use.
I&rsquo;ve written on many occasions about setting up Actions workflows, including using one to run your Makefile, and how to integrate GitHub event data. GitHub even interviewed me about Actions once.
For Django projects, here&rsquo;s a GitHub Actions workflow that runs tests with a consistent Python version whenever someone opens a pull request in the repository.
name: Run Django tests on: pull_request jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: &#39;3.8&#39; - name: Install dependencies run: make install - name: Run tests run: make test For the installation and test commands, I&rsquo;ve simply utilized the Makefile that&rsquo;s been checked in to the repository. A benefit of using your Makefile commands in your CI test workflows is that you only need to keep them updated in one place &ndash; your Makefile! No more &ldquo;why is this working locally but not in CI??!?&rdquo; headaches.
If you want to step up your security game, you can add Django Security Check as an Action too.
Set up your Django project for success Want to help keep your development team happy? Set them up for success with these best practices for Django development. Remember, an ounce of brainpower is worth a pound of software!
`,url:"https://victoria.dev/posts/django-project-best-practices-to-keep-your-developers-happy/"},"https://victoria.dev/posts/manipulating-data-with-django-migrations/":{title:"Manipulating data with Django migrations",tags:["django","coding","data","python"],content:`Growing, successful applications are a lovely problem to have. As a product develops, it tends to accumulate complication the way your weekend cake project accumulates layers of frosting. Thankfully, Django, my favorite batteries-included framework, handles complexity pretty well.
Django models help humans work with data in a way that makes sense to our brains, and the framework offers plenty of classes you can inherit to help you rapidly develop a robust application from scratch. As for developing on existing Django applications, there&rsquo;s a feature for that, too. In this article, we&rsquo;ll cover how to use Django migrations to update your existing models and database.
What&rsquo;s under the hood Django migrations are Python files that help you add and change things in your database tables to reflect changes in your Django models. To understand how Django migrations help you work with data, it may be helpful to understand the underlying structures we&rsquo;re working with.
What&rsquo;s a database table If you&rsquo;ve laid eyes on a spreadsheet before, you&rsquo;re already most of the way to understanding a database table. In a relational database, for example, a PostgreSQL database, you can expect to see data organized into columns and rows. A relational database table may have a set number of columns and any number of rows.
In Django, each model is its own table. For example, here&rsquo;s a Django model:
from django.db import models class Lunch(models.Model): left_side = models.CharField(max_length=100, null=True) center = models.CharField(max_length=100, null=True) right_side = models.CharField(max_length=100, null=True) Each field is a column, and each row is a Django object instance of that model. Here&rsquo;s a representation of a database table for the Django model &ldquo;Lunch&rdquo; above. In the database, its name would be lunch_table.
id left_side center right_side 1 Fork Plate Spoon The model Lunch has three fields: left_side, center, and right-side. One instance of a Lunch object would have &ldquo;Fork&rdquo; for the left_side, a &ldquo;Plate&rdquo; for the center, and &ldquo;Spoon&rdquo; for the right_side. Django automatically adds an id field if you don&rsquo;t specify a primary key.
If you wanted to change the name of your Lunch model, you would do so in your models.py code. For example, change &ldquo;Lunch&rdquo; to &ldquo;Dinner,&rdquo; then run python manage.py makemigrations. You&rsquo;ll see:
python manage.py makemigrations Did you rename the backend.Lunch model to Dinner? [y/N] y Migrations for &#39;backend&#39;: backend/migrations/0003_auto_20200922_2331.py - Rename model Lunch to Dinner Django automatically generates the appropriate migration files. The relevant line of the generated migrations file in this case would look like:
migrations.RenameModel(old_name=&#34;Lunch&#34;, new_name=&#34;Dinner&#34;), This operation would rename our &ldquo;Lunch&rdquo; model to &ldquo;Dinner&rdquo; while keeping everything else the same. But what if you also wanted to change the structure of the database table itself, its schema, as well as make sure that existing data ends up in the right place on your Dinner table?
Let&rsquo;s explore how to turn our Lunch model into a Dinner model that looks like this:
from django.db import models class Dinner(models.Model): top_left = models.CharField(max_length=100, null=True) top_center = models.CharField(max_length=100, null=True) top_right = models.CharField(max_length=100, null=True) bottom_left = models.CharField(max_length=100, null=True) bottom_center = models.CharField(max_length=100, null=True) bottom_right = models.CharField(max_length=100, null=True) &hellip;with a database table that would look like this:
id top_left top_center top_right bottom_left bottom_center bottom_right 1 Bread plate Spoon Glass Fork Plate Knife Manipulating data with Django migrations Before you begin to manipulate your data, it&rsquo;s always a good idea to create a backup of your database that you can restore in case something goes wrong. There are various ways to do this depending on the database you&rsquo;re using. You can typically find instructions by searching for &lt;your database name&gt; and keywords like backup, recovery, or snapshot.
In order to design your migration, it&rsquo;s helpful to become familiar with the available migration operations. Migrations are run step-by-step, and each operation is some flavor of adding, removing, or altering data. Like a strategic puzzle, it&rsquo;s important to make model changes one step at a time so that the generated migrations have the correct result.
We&rsquo;ve already renamed our model successfully. Now, we&rsquo;ll rename the fields that hold the data we want to retain:
class Dinner(models.Model): bottom_left = models.CharField(max_length=100, null=True) bottom_center = models.CharField(max_length=100, null=True) top_center = models.CharField(max_length=100, null=True) Django is sometimes smart enough to determine the old and new field names correctly. You&rsquo;ll be asked for confirmation:
python manage.py makemigrations Did you rename dinner.center to dinner.bottom_center (a CharField)? [y/N] y Did you rename dinner.left_side to dinner.bottom_left (a CharField)? [y/N] y Did you rename dinner.right_side to dinner.top_center (a CharField)? [y/N] y Migrations for &#39;backend&#39;: backend/migrations/0004_auto_20200914_2345.py - Rename field center on dinner to bottom_center - Rename field left_side on dinner to bottom_left - Rename field right_side on dinner to top_center In some cases, you&rsquo;ll want to try renaming the field and running makemigrations one at a time.
Now that the existing fields have been migrated to their new names, add the remaining fields to the model:
class Dinner(models.Model): top_left = models.CharField(max_length=100, null=True) top_center = models.CharField(max_length=100, null=True) top_right = models.CharField(max_length=100, null=True) bottom_left = models.CharField(max_length=100, null=True) bottom_center = models.CharField(max_length=100, null=True) bottom_right = models.CharField(max_length=100, null=True) Running makemigrations again now gives us:
python manage.py makemigrations Migrations for &#39;backend&#39;: backend/migrations/0005_auto_20200914_2351.py - Add field bottom_right to dinner - Add field top_left to dinner - Add field top_right to dinner You&rsquo;re done! By generating Django migrations, you&rsquo;ve successfully set up your dinner_table and moved existing data to its new spot.
Additional complexity You&rsquo;ll notice that our Lunch and Dinner models are not very complex. Out of Django&rsquo;s many model field options, we&rsquo;re just using CharField. We also set null=True to let Django store empty values as NULL in the database.
Django migrations can handle additional complexity, such as changing field types, and whether a blank or null value is permitted. I keep Django&rsquo;s model field reference handy as I work with varying types of data and different use cases.
De-mystified migrations I hope this article has helped you better understand Django migrations and how they work!
Now that you can change models and manipulate existing data in your Django application, be sure to use your powers wisely! Backup your database, research and plan your migrations, and always run tests before working with customer data. By doing so, you have the potential to enable your application to grow &ndash; with manageable levels of complexity.
`,url:"https://victoria.dev/posts/manipulating-data-with-django-migrations/"},"https://victoria.dev/archive/what-is-tls-transport-layer-security-encryption-explained-in-plain-english/":{title:"What is TLS? Transport Layer Security encryption explained in plain english",tags:["cybersecurity","algorithms","protocols","computing"],content:`If you want to have a confidential conversation with someone you know, you might meet up in person and find a private place to talk. If you want to send data confidentially over the Internet, you might have a few more considerations to cover.
TLS, or Transport Layer Security, refers to a protocol. &ldquo;Protocol&rdquo; is a word that means, &ldquo;the way we&rsquo;ve agreed to do things around here,&rdquo; more or less. The &ldquo;transport layer&rdquo; part of TLS simply refers to host-to-host communication, such as how a client and a server interact, in the Internet protocol suite model.
The TLS protocol attempts to solve these fundamental problems:
How do I know you are who you say you are? How do I know this message from you hasn&rsquo;t been tampered with? How can we communicate securely? Here&rsquo;s how TLS works, explained in plain English. As with many successful interactions, it begins with a handshake.
Getting to know you The basic process of a TLS handshake involves a client, such as your web browser, and a server, such as one hosting a website, establishing some ground rules for communication. It begins with the client saying hello. Literally. It&rsquo;s called a ClientHello message.
The ClientHello message tells the server which TLS protocol version and cipher suites it supports. While &ldquo;cipher suite&rdquo; sounds like a fancy hotel upgrade, it just refers to a set of algorithms that can be used to secure communications. The server, in a similarly named ServerHello message, chooses the protocol version and cipher suite to use from the choices offered. Other data may also be sent, for example, a session ID if the server supports resuming a previous handshake.
Depending on the cipher suite chosen, the client and server exchange further information in order to establish a shared secret. Often, this process moves the exchange from asymmetric cryptography to symmetric cryptography with varying levels of complexity. Let&rsquo;s explore these concepts at a general level and see why they matter to TLS.
Asymmetric beginnings This is asymmetry:
Small egg, big egg.
Asymmetric cryptography is one method by which you can perform authentication. When you authenticate yourself, you answer the fundamental question, &ldquo;How do I know you are who you say you are?&rdquo;
In an asymmetric cryptographic system, you use a pair of keys in order to achieve authentication. These keys are asymmetric. One key is your public key, which, as you would guess, is public. The other is your private key, which &ndash; well, you know.
Typically, during the TLS handshake, the server will provide its public key via its digital certificate, sometimes still called its SSL certificate, though TLS replaces the deprecated Secure Sockets Layer (SSL) protocol. Digital certificates are provided and verified by trusted third parties known as Certificate Authorities (CA), which are a whole other article in themselves.
While anyone may encrypt a message using your public key, only your private key can then decrypt that message. The security of asymmetric cryptography relies only on your private key staying private, hence the asymmetry. It&rsquo;s also asymmetric in the sense that it&rsquo;s a one-way trip. Alice can send messages encrypted with your public key to you, but neither of your keys will help you send an encrypted message to Alice.
Symmetric secrets Asymmetric cryptography also requires more computational resources than symmetric cryptography. Thus when a TLS handshake begins with an asymmetric exchange, the client and server will use this initial communication to establish a shared secret, sometimes called a session key. This key is symmetric, meaning that both parties use the same shared secret and must maintain that secrecy for the encryption to be secure.
Wise man say: share your public key, but keep your shared keys private.
By using the initial asymmetric communication to establish a session key, the client and server can rely on the session key being known only to them. For the rest of the session, they&rsquo;ll both use this same shared key to encrypt and decrypt messages, which speeds up communication.
Secure sessions A TLS handshake may use asymmetric cryptography or other cipher suites to establish the shared session key. Once the session key is established, the handshaking portion is complete and the session begins.
The session is the duration of encrypted communication between the client and server. During this time, messages are encrypted and decrypted using the session key that only the client and server have. This ensures that communication is secure.
The integrity of exchanged information is maintained by using a checksum. Messages exchanged using session keys have a message authentication code (MAC) attached. This is not the same thing as your device&rsquo;s MAC address. The MAC is generated and verified using the session key. Because of this, either party can detect if a message has been changed before being received. This solves the fundamental question, &ldquo;How do I know this message from you hasn&rsquo;t been tampered with?&rdquo;
Sessions can end deliberately, due to network disconnection, or from the client staying idle for too long. Once a session ends, it must be re-established via a new handshake or through previously established secrets called session IDs that allow resuming a session.
TLS and you Let&rsquo;s recap:
TLS is a cryptographic protocol for providing secure communication. The process of creating a secure connection begins with a handshake. The handshake establishes a shared session key that is then used to secure messages and provide message integrity. Sessions are temporary, and once ended, must be re-established or resumed. This is just a surface-level skim of the very complex cryptographic systems that help to keep your communications secure. For more depth on the topic, I recommend exploring cipher suites and the various supported algorithms.
The TLS protocol serves a very important purpose in your everyday life. It helps to secure your emails to family, your online banking activities, and the connection by which you&rsquo;re reading this article. The HTTPS communication protocol is encrypted using TLS. Every time you see that little lock icon in your URL bar, you&rsquo;re experiencing firsthand all the concepts you&rsquo;ve just read about in this article. Now you know the answer to the last question: &ldquo;How can we communicate securely?&rdquo;
`,url:"https://victoria.dev/archive/what-is-tls-transport-layer-security-encryption-explained-in-plain-english/"},"https://victoria.dev/posts/deceptively-simple-search-and-replace-across-multiple-files/":{title:"Deceptively simple search-and-replace across multiple files",tags:["terminal","linux"],content:`While a multitude of methods exist to search for and replace words in a single file, what do you do when you&rsquo;ve got a string to update across multiple unrelated files, all with different names? You harness the power of command line tools, of course!
First, you&rsquo;ll need to find all the files you want to change. Stringing together what are effectively search queries for find is really only limited by your imagination. Here&rsquo;s a simple example that finds Python files:
find . -name &#39;*.py&#39; The -name test searches for a pattern, such as all files ending in .py, but find can do a lot more with other test conditions, including -regex tests. Run find --help to see the multitude of options.
Further tune your search by using grep to get only the files that contain the string you want to change, such as by adding:
grep -le &#39;\\&lt;a whale\\&gt;&#39; The -l option gives you just the file names for all files containing a pattern (denoted with -e) that match &ldquo;a whale&rdquo;.
Using Vim&rsquo;s impressive :bufdo lets you run the same command across multiple buffers, interactively working with all of these files without the tedium of opening, saving, and closing each file, one at a time.
Let&rsquo;s plug your powerful find+grep results into Vim with:
vim \`find . -name &#39;*.py&#39; \\ -exec grep -le &#39;\\&lt;a whale\\&gt;&#39; {} \\;\` Using backtick-expansion to pass our search to Vim opens up multiple buffers ready to go. (Do :h backtick-expansion in Vim for more.) Now you can apply the Vim command :bufdo to all of these files and perform actions such as interactive search-and-replace:
:bufdo %s/a whale/a bowl of petunias/gce The g for &ldquo;global&rdquo; will change occurrences of the pattern on all lines. The e will omit errors if the pattern is not found. The c option makes this interactive; if you&rsquo;re feeling confident, you can omit it to make the changes without reviewing each one.
If one of the patterns contains a / character, you can substitute the separator in the above command to make it more readable. Vim will assume the character following the %s is the separator, so for example:
:bufdo %s_a whale_a bowl of peonies/petunias_gce When you&rsquo;ve finished going through all the buffers, save all the work you&rsquo;ve completed with:
:bufdo wq! Then bask in the glory of your saved time and effort.
`,url:"https://victoria.dev/posts/deceptively-simple-search-and-replace-across-multiple-files/"},"https://victoria.dev/archive/how-github-codespaces-increases-productivity-and-lowers-barriers/":{title:"How GitHub Codespaces increases productivity and lowers barriers",tags:["open-source","coding","leadership"],content:`The most recent integration between Visual Studio Code and GitHub can help make development accessible and welcoming: Codespaces in GitHub!
Now in beta, GitHub Codespaces provide an online, in-the-browser IDE powered by Visual Studio Code. This lets you use this full-featured IDE, complete with extensions, terminal, Git commands, and all the settings you&rsquo;re accustomed to, on any machine. You can now bring your development workflow anywhere using a tablet or other browser-based device.
Codespaces is great news for open source contributors, too. Adding a codespace configuration to your project is a great way to invite new folks to easily start contributing.
A new open source contributor or new hire at your organization can quickly fire up a codespace and get hacking on a good first issue with no local environment set up or installations necessary!
We&rsquo;ve added codespace configuration settings over at the OWASP Web Security Testing Guide (WSTG). Want to take it for a spin? See our open issues.
Configuring Codespaces You can use Visual Studio Code&rsquo;s .devcontainer folder to configure a development container for your repository as well.
Many pre-built containers are available &ndash; just copy the .devcontainer you need to your repository root. If your repository doesn&rsquo;t have one, a default base Linux image will be used.
Here&rsquo;s a reason to remove .vscode from your .gitignore file. Any new codespaces created in your repository will now respect settings found at .vscode/settings.json. This means that your online IDE can have the same Workspace configuration as you have on your local machine. Isn&rsquo;t that useful!
Making Codespaces personal For next-level dotfiles personalization, consider committing relevant files from your local dotfiles folder as a public GitHub repository at yourusername/dotfiles.
When you create a new codespace, this brings in your configurations, such as shell aliases and preferences, by creating symlinks to dotfiles in your codespace $HOME. This personalizes all the codespaces you create in your account.
Need some inspiration? Browse my dotfiles repository on GitHub.
Developing in a codespace is a familiar experience for Visual Studio Code users, right down to running an application locally.
Thanks to port forwarding, when I run an application in a codespace terminal, clicking on the resulting localhost URL takes me to the appropriate port as output from my codespace.
When I&rsquo;m working on this website in my codespace, for example, I run hugo serve then click the provided localhost:1313 link to see a preview of my changes in another browser tab.
Want to stay in sync between devices? There&rsquo;s an extension for that. You can connect to your codespace from Visual Studio Code on your local machine so you can always pick up right where you left off.
Develop anywhere Codespaces is a super exciting addition to my GitHub workflow. It allows me to access my full development process pretty much anywhere, using devices like my iPad.
It&rsquo;ll also make it easier for new open source contributors or new hires at your organization to hit the ground running with a set-up IDE. If you have access to the limited beta, I invite you to spin up a codespace and try contributing to the WSTG, or to an issue on one of my open source projects.
I&rsquo;m looking forward to general availability and seeing what the open source community will dream up for GitHub Codespaces next!
And yes &ndash; codespaces support your favorite Visual Studio Code theme. 😈
Screenshot of a codespace with the Kabukichō theme for Visual Studio Code
`,url:"https://victoria.dev/archive/how-github-codespaces-increases-productivity-and-lowers-barriers/"},"https://victoria.dev/posts/how-to-create-a-self-documenting-makefile/":{title:"How to create a self-documenting Makefile",tags:["coding","ci/cd","docs","leadership"],content:`My new favorite way to completely underuse a Makefile? Creating personalized, per-project repository workflow command aliases that you can check in.
Can a Makefile improve your DevOps and keep developers happy? How awesome would it be if a new developer working on your project didn&rsquo;t start out by copying and pasting commands from your README? What if instead of:
pip3 install pipenv pipenv shell --python 3.8 pipenv install --dev npm install pre-commit install --install-hooks # look up how to install Framework X... # copy and paste from README... npm run serve &hellip; you could just type:
make start &hellip;and then start working?
Making a difference I use make every day to take the tedium out of common development activities like updating programs, installing dependencies, and testing. To do all this with a Makefile (GNU make), we use Makefile rules and recipes. Similar parallels exist for POSIX flavor make, like Target Rules; here&rsquo;s a great article on POSIX-compatible Makefiles.
Here&rsquo;s some examples of things we can make easier (sorry):
update: ## Do apt upgrade and autoremove sudo apt update &amp;&amp; sudo apt upgrade -y sudo apt autoremove -y env: pip3 install pipenv pipenv shell --python 3.8 install: ## Install or update dependencies pipenv install --dev npm install pre-commit install --install-hooks serve: ## Run the local development server hugo serve --enableGitInfo --disableFastRender --environment development initial: update env install serve ## Install tools and start development server Now we have some command-line aliases that you can check in! Great idea! If you&rsquo;re wondering what&rsquo;s up with that weird ## comment syntax, it gets better.
A self-documenting Makefile Aliases are great, if you remember what they all are and what they do without constantly typing cat Makefile. Naturally, you need a help command:
.PHONY: help help: ## Show this help @egrep -h &#39;\\s##\\s&#39; $(MAKEFILE_LIST) | sort | awk &#39;BEGIN {FS = &#34;:.*?## &#34;}; {printf &#34;\\033[36m%-20s\\033[0m %s\\n&#34;, $$1, $$2}&#39; With a little command-line magic, this egrep command takes the output of MAKEFILE_LIST, sorts it, and uses awk to find strings that follow the ## pattern. It then prints a helpful formatted version of the comments.
We&rsquo;ll put it at the top of the file so it&rsquo;s the default target. Now to see all our handy shortcuts and what they do, we just run make, or make help:
help Show this help initial Install tools and start development server install Install or update dependencies serve Run the local development server update Do apt upgrade and autoremove Now we have our very own personalized and project-specific CLI tool!
The possibilities for improving your DevOps flow with a self-documenting Makefile are almost endless. You can use one to simplify any workflow and produce some very happy developers.
Please enjoy the (live!) Makefile I use to manage and develop this Hugo site. I hope it inspires you!
My Hugo site Makefile SHELL := /bin/bash .POSIX: .PHONY: help env install upgrade-hugo serve build start initial help: ## Show this help @egrep -h &#39;\\s##\\s&#39; $(MAKEFILE_LIST) | sort | awk &#39;BEGIN {FS = &#34;:.*?## &#34;}; {printf &#34;\\033[36m%-20s\\033[0m %s\\n&#34;, $$1, $$2}&#39; env: pip3 install pipenv shell: ## Enter the virtual environment pipenv shell install: ## Install or update dependencies pipenv install --dev pre-commit install --install-hooks npm install HUGO_VERSION:=$(shell curl -s https://api.github.com/repos/gohugoio/hugo/releases/latest | grep &#39;tag_name&#39; | cut -d &#39;&#34;&#39; -f 4 | cut -c 2-) upgrade-hugo: ## Get the latest Hugo mkdir tmp/ &amp;&amp; \\ cd tmp/ &amp;&amp; \\ curl -sSL https://github.com/gohugoio/hugo/releases/download/v$(HUGO_VERSION)/hugo_extended_$(HUGO_VERSION)_Linux-64bit.tar.gz | tar -xvzf- &amp;&amp; \\ sudo mv hugo /usr/local/bin/ &amp;&amp; \\ cd .. &amp;&amp; \\ rm -rf tmp/ hugo version dev: ## Run the local development server git submodule update --init --recursive hugo serve --enableGitInfo --disableFastRender --environment development future: ## Run the local development server in the future hugo serve --enableGitInfo --buildFuture --disableFastRender --environment development build: ## Build site hugo --minify --cleanDestinationDir initial: env install upgrade-hugo serve ## Install tools and start development server `,url:"https://victoria.dev/posts/how-to-create-a-self-documenting-makefile/"},"https://victoria.dev/posts/climbing-mt.-fuji/":{title:"Climbing Mt. Fuji",tags:[],content:`In 2017, I climbed Mt. Fuji, in Japan.
Mt. Fuji is, some folks would say, the cakewalk of mountain climbing. Physically, the hardest portions amount to scrambling over some big boulders; most of it is no more taxing than a hike or climbing a set of stairs. For spiritual reasons, some Japanese folks make the climb at ages upwards of 80 years. There are huts to stop at along the way where you can rent a sleeping bag inside, and buy food and water. Naturally, having done this research and deciding it sounded like a fun outing, I arrived at basecamp in sneakers.
Most of the way up was amazing and thoroughly enjoyable. I saw sights I&rsquo;d never seen before, like the glow of a city under the sun through a break in the clouds, from above. Walking a path through a cloud was like taking a road into nothingness, with blank grey on all sides that weren&rsquo;t a mountain. Every time we hit a station marker, I felt pride and accomplishment.
Until it was time to summit.
Most of the people who climb Mt. Fuji wish to reach the summit at sunrise. Some for spiritual reasons, others for Instagram, and for those like myself, it just seemed like the thing to do. Regardless, it was because of these other 5,000 average daily climbers that I found myself in an actual queue that snaked the entire path from the last station hut to the summit &ndash; in the pitch black pre-dawn cold. It took hours, for most of which, we stood stock-still, going nowhere. I took to doing calisthenics to stave off frostbite from the cold that threatened my sneaker-shod toes.
We did, eventually, reach the summit, and before sunrise. It remains one of the most beautiful sunrises I&rsquo;ve seen &ndash; a pink-gold light that lit up the peak like breathing life into a painting, and that brought, mercifully, a degree of warmth. I was extremely happy, and felt pride and accomplishment.
Until it was time to descend.
There is a Japanese proverb: “A wise man will climb Mt Fuji once; a fool will climb Mt Fuji twice.” It is my own suspicion that this saying is based entirely on the difficulty of climbing down. The descent is essentially a loosely-packed, dirt and gravel road &ndash; on a decline. It is not, I imagine, significantly taxing with proper hiking boots, maybe snow tread, and a couple good spiked hiking poles thrown in. Wearing a pair of flat-soled street shoes, however, I fell. I fell often, and hard, about every three steps, for hours. I tried to take larger steps; it didn&rsquo;t help. I tried to take smaller steps; that didn&rsquo;t help, either. I tried cunningly to find a way to surf-slide my way down the mountainside and nearly ended up with a mouthful of dirt. As if literally rubbing salt into my wounds, without the gaiters I hadn&rsquo;t brought, sand found its way into my shoes. It was without a doubt the most stupefyingly discouraging experience of my life.
On several occasions, more seasoned (smarter? well-prepared?) hikers passed me, a good many of them at least twice my age. I&rsquo;m hard-pressed to remember another time in my life where I have been so thoroughly shown up by someone who might have been my grandmother, plunking hiking poles into the earth and sauntering past at a steady pace while I picked myself up, elbows scratched and covered in dirt, for the umpteenth time.
Eventually, we reached the bottom. At a tiny basecamp gift shop, I ate a delicious bowl of ramen and the tastiest sponge cake in the shape of a mountain that I&rsquo;ll likely ever have.
The experience drove home two lessons that have gone on to serve me well: one, that all the good research in the world will not guarantee your experience; and two, that even when faced with a discouraging situation that you can&rsquo;t seem to think yourself out of and thus the only way is &ldquo;through,&rdquo; there may still be something to learn from it, and there may be really good cake at the bottom.
`,url:"https://victoria.dev/posts/climbing-mt.-fuji/"},"https://victoria.dev/posts/go-automate-your-github-profile-readme/":{title:"Go automate your GitHub profile README",tags:["ci/cd","go"],content:`GitHub&rsquo;s new profile page README feature is having the wonderful effect of bringing some personality to the Myspace pages of the developer Internet. Though Markdown lends itself best to standard static text content, that&rsquo;s not stopping creative folks from working to create a next-level README. You can include GIFs and images to add some motion and pizazz (they&rsquo;re covered in GitHub Flavor Markdown), but I&rsquo;m thinking of something a little more dynamic.
At front-and-center on your GitHub profile, your README is a great opportunity to let folks know what you&rsquo;re about, what you find important, and to showcase some highlights of your work. You might like to show off your latest repositories, tweet, or blog post. Keeping it up to date doesn&rsquo;t have to be a pain either, thanks to continuous delivery tools like GitHub Actions.
My current README refreshes itself daily with a link to my latest blog post. Here&rsquo;s how I&rsquo;m creating a self-updating README.md with Go and GitHub actions.
Reading and writing files with Go I&rsquo;ve been writing a lot of Python lately, but for some things I really like using Go. You could say it&rsquo;s my go-to language for just-for-func projects. Sorry. Couldn&rsquo;t stop myself.
To create my README.md, I&rsquo;m going to get some static content from an existing file, mash it together with some new dynamic content that we&rsquo;ll generate with Go, then bake the whole thing at 400 degrees until something awesome comes out.
Here&rsquo;s how we read in a file called static.md and put it in string form:
// Unwrap Markdown content content, err := ioutil.ReadFile(&#34;static.md&#34;) if err != nil { log.Fatalf(&#34;cannot read file: %v&#34;, err) return err } // Make it a string stringyContent := string(content) The possibilities for your dynamic content are only limited by your imagination! Here, I&rsquo;ll use the github.com/mmcdole/gofeed package to read the RSS feed from my blog and get the newest post.
fp := gofeed.NewParser() feed, err := fp.ParseURL(&#34;https://victoria.dev/index.xml&#34;) if err != nil { log.Fatalf(&#34;error getting feed: %v&#34;, err) } // Get the freshest item rssItem := feed.Items[0] To join these bits together and produce stringy goodness, we use fmt.Sprintf() to create a formatted string.
// Whisk together static and dynamic content until stiff peaks form blog := &#34;Read my latest blog post: **[&#34; + rssItem.Title + &#34;](&#34; + rssItem.Link + &#34;)**&#34; data := fmt.Sprintf(&#34;%s\\n%s\\n&#34;, stringyContent, blog) Then to create a new file from this mix, we use os.Create(). There are more things to know about deferring file.Close(), but we don&rsquo;t need to get into those details here. We&rsquo;ll add file.Sync() to ensure our README gets written.
// Prepare file with a light coating of os file, err := os.Create(&#34;README.md&#34;) if err != nil { return err } defer file.Close() // Bake at n bytes per second until golden brown _, err = io.WriteString(file, data) if err != nil { return err } return file.Sync() View the full code here in my README repository.
Mmmm, doesn&rsquo;t that smell good? 🍪 Let&rsquo;s make this happen on the daily with a GitHub Action.
Running your Go program on a schedule with Actions You can create a GitHub Action workflow that triggers both on a push to your master branch as well as on a daily schedule. Here&rsquo;s a slice of the .github/workflows/update.yaml that defines this:
on: push: branches: - master schedule: - cron: &#39;0 11 * * *&#39; To run the Go program that rebuilds our README, we first need a copy of our files. We use actions/checkout for that:
steps: - name: 🍽️ Get working copy uses: actions/checkout@master with: fetch-depth: 1 This step runs our Go program:
- name: 🍳 Shake &amp; bake README run: | cd \${GITHUB_WORKSPACE}/update/ go run main.go Finally, we push the updated files back to our repository. Learn more about the variables shown at Using variables and secrets in a workflow.
- name: 🚀 Deploy run: | git config user.name &#34;\${GITHUB_ACTOR}&#34; git config user.email &#34;\${GITHUB_ACTOR}@users.noreply.github.com&#34; git add . git commit -am &#34;Update dynamic content&#34; git push --all -f https://\${{ secrets.GITHUB_TOKEN }}@github.com/\${GITHUB_REPOSITORY}.git View the full code for this Action workflow here in my README repository.
Go forth and auto-update your README Congratulations and welcome to the cool kids&rsquo; club! You now know how to build an auto-updating GitHub profile README. You may now go forth and add all sorts of neat dynamic elements to your page &ndash; just go easy on the GIFs, okay?
`,url:"https://victoria.dev/posts/go-automate-your-github-profile-readme/"},"https://victoria.dev/posts/writing-efficient-django/":{title:"Writing efficient Django",tags:["django","coding","python"],content:`I like Django. It&rsquo;s a well-considered and intuitive framework with a name I can pronounce out loud. You can use it to quickly spin up a weekend-sized project, and you can still use it to run full-blown production applications at scale. I&rsquo;ve done both these things, and over the years I&rsquo;ve discovered how to use some of Django&rsquo;s features for maximum efficiency. These are:
Class-based versus function-based views Django models Retrieving objects with queries Understanding these main features are the building blocks for maximizing development efficiency with Django. They&rsquo;ll build the foundation for you to test efficiently and create an awesome development experience for your engineers. Let&rsquo;s look at how these tools let you create a performant Django application that&rsquo;s pleasant to build and maintain.
Class-based versus function-based views Remember that Django is all Python under the hood. When it comes to views, you&rsquo;ve got two choices: view functions (sometimes called &ldquo;function-based views&rdquo;), or class-based views.
Years ago when I first built ApplyByAPI, it was initially composed entirely of function-based views. These offer granular control, and are good for implementing complex logic; just as in a Python function, you have complete control (for better or worse) over what the view does. With great control comes great responsibility, and function-based views can be a little tedious to use. You&rsquo;re responsible for writing all the necessary methods for the view to work - this is what allows you to completely tailor your application.
In the case of ApplyByAPI, there were only a sparse few places where that level of tailored functionality was really necessary. Everywhere else, function-based views began making my life harder. Writing what is essentially a custom view for run-of-the-mill operations like displaying data on a list page became tedious, repetitive, and error-prone.
With function-based views, you&rsquo;ll need figure out which Django methods to implement in order to handle requests and pass data to views. Unit testing can take some work to write. In short, the granular control that function-based views offer also requires some granular tedium to properly implement.
I ended up holding back ApplyByAPI while I refactored the majority of the views into class-based views. This was not a small amount of work and refactoring, but when it was done, I had a bunch of tiny views that made a huge difference. I mean, just look at this one:
class ApplicationsList(ListView): model = Application template_name = &#34;applications.html&#34; It&rsquo;s three lines. My developer ergonomics, and my life, got a lot easier.
You may think of class-based views as templates that cover most of the functionality any app needs. There are views for displaying lists of things, for viewing a thing in detail, and editing views for performing CRUD (Create, Read, Update, Delete) operations. Because implementing one of these generic views takes only a few lines of code, my application logic became dramatically succinct. This gave me less repeated code, fewer places for something to go wrong, and a more manageable application in general.
Class-based views are fast to implement and use. The built-in class-based generic views may require less work to test, since you don&rsquo;t need to write tests for the base view Django provides. (Django does its own tests for that; no need for your app to double-check.) To tweak a generic view to your needs, you can subclass a generic view and override attributes or methods. In my case, since I only needed to write tests for any customizations I added, my test files became dramatically shorter, as did the time and resources it took to run them.
When you&rsquo;re weighing the choice between function-based or class-based views, consider the amount of customization the view needs, and the future work that will be necessary to test and maintain it. If the logic is common, you may be able to hit the ground running with a generic class-based view. If you need sufficient granularity that re-writing a base view&rsquo;s methods would make it overly complicated, consider a function-based view instead.
Django models Models organize your Django application&rsquo;s central concepts to help make them flexible, robust, and easy to work with. If used wisely, models are a powerful way to collate your data into a definitive source of truth.
Like views, Django provides some built-in model types for the convenience of implementing basic authentication, including the User and Permission models. For everything else, you can create a model that reflects your concept by inheriting from a parent Model class.
class StaffMember(models.Model): user = models.OneToOneField(User, on_delete=models.CASCADE) company = models.OneToOneField(Company, on_delete=models.CASCADE) def __str__(self): return self.company.name + &#34; - &#34; + self.user.email When you create a custom model in Django, you subclass Django&rsquo;s Model class and take advantage of all its power. Each model you create generally maps to a database table. Each attribute is a database field. This gives you the ability to create objects to work with that humans can better understand.
You can make a model useful to you by defining its fields. Many built-in field types are conveniently provided. These help Django figure out the data type, the HTML widget to use when rendering a form, and even form validation requirements. If you need to, you can write custom model fields.
Database relationships can be defined using a ForeignKey field (many-to-one), or a ManyToManyField (give you three guesses). If those don&rsquo;t suffice, there&rsquo;s also a OneToOneField. Together, these allow you to define relations between your models with levels of complexity limited only by your imagination. (Depending on the imagination you have, this may or may not be an advantage.)
Retrieving objects with queries Use your model&rsquo;s Manager (objects by default) to construct a QuerySet. This is a representation of objects in your database that you can refine, using methods, to retrieve specific subsets. All available methods are in the QuerySet API and can be chained together for even more fun.
Post.objects.filter( type=&#34;new&#34; ).exclude( title__startswith=&#34;Blockchain&#34; ) Some methods return new QuerySets, such as filter(), or exclude(). Chaining these can give you powerful queries without affecting performance, as QuerySets aren&rsquo;t fetched from the database until they are evaluated. Methods that evaluate a QuerySet include get(), count(), len(), list(), or bool().
Iterating over a QuerySet also evaluates it, so avoid doing so where possible to improve query performance. For instance, if you just want to know if an object is present, you can use exists() to avoid iterating over database objects.
Use get() in cases where you want to retrieve a specific object. This method raises MultipleObjectsReturned if something unexpected happens, as well as the DoesNotExist exception, if, take a guess.
If you&rsquo;d like to get an object that may not exist in the context of a user&rsquo;s request, use the convenient get_object_or_404() or get_list_or_404() which raises Http404 instead of DoesNotExist. These helpful shortcuts are suited to just this purpose. To create an object that doesn&rsquo;t exist, there&rsquo;s also the convenient get_or_create().
Efficient essentials You&rsquo;ve now got a handle on these three essential tools for building your efficient Django application &ndash; congratulations! You can make Django work even better for you by learning about manipulating data with migrations, testing effectively, and setting up your team&rsquo;s Django development for maximum happiness.
If you&rsquo;re going to build on GitHub, you may like to set up my django-security-check GitHub Action. In the meantime, you&rsquo;re well on your way to building a beautiful software project.
`,url:"https://victoria.dev/posts/writing-efficient-django/"},"https://victoria.dev/archive/look-mom-im-a-github-action-hero/":{title:"Look mom, I'm a GitHub Action Hero",tags:["ci/cd","open-source","cybersecurity","life","coding","terminal","api"],content:`GitHub recently interviewed me for their blog editorial entitled GitHub Action Hero: Victoria Drake. Here&rsquo;s a behind-the-scenes peek at the original interview questions and my answers.
What is the name of your Action? Please include a link too. Among the several Actions I&rsquo;ve built, I have two current favorites. One is hugo-remote, which lets you continuously deploy a Hugo static site from a private source repository to a public GitHub Pages repository. This keeps the contents of the source repository private, such as your unreleased drafts, while still allowing you to have a public open source site using GitHub Pages.
The second is django-security-check. It&rsquo;s an effortless way to continuously check that your production Django application is free from a variety of security misconfigurations. You can think of it as your little CI/CD helper for busy projects &ndash; a security linter!
Tell us a little bit more about yourself—how did you get started in software tools? When I was a kid, I spent several summer vacations coding a huge medieval fantasy world MUD (Multi-User Dungeon, like a multiplayer role-playing game) written in LPC, with friends. It was entirely text-based, and built and played via Telnet. I fell in love with the terminal and learned a lot about object-oriented programming and prototype-based programming early on.
I became a freelance developer and had the privilege of working on a wide variety of client projects. Realizing the difficulty that companies have with hiring experienced developers, I built ApplyByAPI.com to help. As you might imagine, it allows candidates to apply for jobs via API, instead of emailing a resume. It&rsquo;s based on the Django framework, so in the process, I learned even more about building reusable units of software.
When I became a co-author and a core maintainer for the Open Web Application Security Project (OWASP) Web Security Testing Guide (WSTG), I gained an even broader appreciation for how a prototype-based, repeatable approach can help build secure web applications. Organizations worldwide consider the WSTG the foremost open source resource for testing the security of web applications. We&rsquo;ve applied this thinking via the use of GitHub Actions in our repository &ndash; I&rsquo;ll tell you more about that later.
Whether I&rsquo;m creating an open source tool or leading a development team, my childhood experience still informs how I think about programming today. I strive to create repeatable units of software like GitHub Actions &ndash; only now, I make them for large enterprises in the real world!
What is the story behind your built GitHub Action? (Why did you build this?) Developers take on a lot of responsibility when it comes to building secure applications these days. I&rsquo;m a full-time senior software developer at a cybersecurity company. I&rsquo;ve found that I&rsquo;m maximally productive when I create systems and processes that help myself and my team make desired outcomes inevitable. So I spend my free time building tools that make it easy for other developers to build secure software as well. My Actions help to automate contained, repeatable units of work that can make a big difference in a developer&rsquo;s day.
Do you have future plans for this or other Actions? Yes! I&rsquo;m always finding ways for tools like GitHub Actions to boost the velocity of technical teams, whether at work or in my open source projects. Remember the Open Web Application Security Project? In the work I&rsquo;ve lead with OWASP, I&rsquo;ve championed the effort to increase automation using GitHub Actions to maintain quality, securely deploy new versions to the web, and even build PDFs of the WSTG. We&rsquo;re constantly looking into new ways that GitHub Actions can make our lives easier and our readers&rsquo; projects more secure.
What has been your favorite feature of GitHub Actions? I like that I can build an Action using familiar and portable technologies, like Docker. Actions are easy for collaborators to work on too, since in the case of a Dockerized Action, you can use any language your team is comfortable with. This is especially useful in large organizations with polyglot teams and environments. There aren&rsquo;t any complicated dependencies for running these portable tasks, and you don&rsquo;t need to learn any special frameworks to get started.
One of my first blog posts about GitHub Actions even describes how I used an Action to run a Makefile! This is especially useful for large legacy applications that want to modernize their pipeline by using GitHub Actions.
What are the biggest challenges you’ve faced while building your GitHub Action? The largest challenge of GitHub Actions isn&rsquo;t really in GitHub Actions, but in the transition of legacy software and company culture.
Migrating legacy software is always challenging, particularly with large legacy applications. Moving to modern CI/CD processes requires changes at the software level, team level, and even a shift in thinking when it comes to individual developers. It can help to have a tool like GitHub Actions, which is at once seamlessly modern and familiar, when transitioning legacy code to a modern pipeline.
Anything else you would like to share about your experience? Any stories or lessons learned through building your Action? I&rsquo;m happiest when I&rsquo;m solving a challenge that makes developing secure software less challenging in the future, both for myself and for the technology organization I&rsquo;m leading. With tools like GitHub Actions, a lot of mental overhead can be offloaded to automatic processes &ndash; like getting a whole other brain, for free! This can massively help organizations that are ready to scale up their development output.
In the realm of cybersecurity, not only does creating portable and reusable software make developers&rsquo; lives easier, it helps to make whole workflows repeatable, which in turn makes software development processes more secure. With smart processes in place, technical teams are happier. As an inevitable result, they&rsquo;ll build better software for customers, too.
`,url:"https://victoria.dev/archive/look-mom-im-a-github-action-hero/"},"https://victoria.dev/posts/technical-ergonomics-for-the-efficient-developer/":{title:"Technical ergonomics for the efficient developer",tags:["coding","life"],content:`This article isn&rsquo;t going to tell you about saving your neck with a Roost stand, or your wrists with a split keyboard - I&rsquo;ve already done that. This article is about saving your brain.
When I first began to program full time, I found myself constantly tired from the mental exertion. Programming is hard! Thankfully, you can take some solace in knowing it gets easier with practice, and with a great supporting cast. Some very nice folks who preceded us both came up with tools to make the difficult bits of communicating with computers much easier on our poor human meatbrains.
I invite you to explore these super helpful technical tools. They&rsquo;ll improve your development set up and alleviate much of the mental stress of programming. You soon won&rsquo;t believe you could have done without them.
Not your average syntax highlighting If you&rsquo;re still working with syntax highlighting that just picks out variable and class names for you, that&rsquo;s cute. Time to turn it up a notch.
In all seriousness, syntax highlighting can make it much easier to find what you&rsquo;re looking for on your screen: the current line, where your current code block starts and ends, or the absolute game-changing which-bracket-set-am-I-in highlight. I primarily use Visual Studio Code, but similar extensions can be found for the major text editors.
The theme pictured in Visual Studio Code above is Kabukichō. I made it.
Use Git hooks I previously brought you an interactive pre-commit checklist in the style of infomercials that&rsquo;s both fun and useful for reinforcing the quality of your commits. But that&rsquo;s not all!
Git hooks are scripts that run automatically at pre-determined points in your workflow. Use them well, and you can save a ton of brainpower. A pre-commit hook remembers to do things like lint and format code, and even runs local tests for you before you indelibly push something embarrassing. Hooks can be a little annoying to share (the .git/hooks directory isn&rsquo;t tracked and thus omitted when you clone or fork a repository) but there&rsquo;s a framework for that: the confusingly-named pre-commit framework, which allows you to create a sharable configuration file of Git hook plugins, not just for pre-commit.
I spend a majority of my time these days coding in Python, so here is my current .pre-commit-config.yaml:
fail_fast: true repos: - repo: https://github.com/DavidAnson/markdownlint-cli2 rev: v0.1.3 hooks: - id: markdownlint-cli2 name: markdownlint-cli2 description: "Checks the style of Markdown/CommonMark files." entry: markdownlint-cli2 language: node types: [markdown] minimum_pre_commit_version: 0.15.0 There are tons of supported hooks to explore.
Use a type system If you write in languages like Python and JavaScript, get yourself an early birthday present and start using a static type system. Not only will this help improve the way you think about code, it can help make type errors clear before running a single line.
For Python, I like using mypy for static type checking. You can set it up as a pre-commit hook (see above) and it&rsquo;s supported in Visual Studio Code too.
TypeScript is my preferred way to write JavaScript. You can run the compiler on the command line using Node.js (see instructions in the repo), it works pretty well with Visual Studio Code out of the box, and of course there are multiple options for extension integrations.
Quit unnecessarily beating up your meatbrain I mean, you wouldn&rsquo;t stand on your head all day to do your work. It would be rather inconvenient to read things upside down all the time (at least until your brain adjusted), and in any case you&rsquo;d likely get uncomfortably congested in short order. Working without taking advantage of the technical ergonomic tools I&rsquo;ve given you today is a little like unnecessary inversion - why would you, if you don&rsquo;t have to?
`,url:"https://victoria.dev/posts/technical-ergonomics-for-the-efficient-developer/"},"https://victoria.dev/posts/how-to-choose-and-care-for-a-secure-open-source-project/":{title:"How to choose and care for a secure open source project",tags:["open-source","cybersecurity","leadership"],content:`There is a rather progressive sect of the software development world that believes that most people would be a lot happier and get a lot more work done if they just stopped building things that someone else has already built and is offering up for free use. They&rsquo;re called the open source community. They want you to take their stuff.
Besides existing without you having to lift a finger, open source tools and software have some distinct advantages. Especially in the case of well-established projects, it&rsquo;s highly likely that someone else has already worked out all the most annoying bugs for you. Thanks to the ease with which users can view and modify source code, it&rsquo;s also more likely that a program has been tinkered with, improved, and secured over time. When many developers contribute, they bring their own unique expertise and experiences. This can result in a product far more robust and capable than one a single developer can produce.
Of course, being as varied as the people who build them, not all open source projects are created equal, nor maintained to be equally secure. There are many factors that affect a project&rsquo;s suitability for your use case. Here are a few general considerations that make a good starting point when choosing an open source project.
How to choose an open source project As its most basic requirements, a good software project is reliable, easy to understand, and has up-to-date components and security. There are several indicators that can help you make an educated guess about whether an open source project satisfies these criteria.
Who&rsquo;s using it Taken in context, the number of people already using an open source project may be indicative of how good it is. If a project has a hundred users, for instance, it stands to reason that someone has tried to use it at least a hundred times before you found it. Thus by the ancient customs of &ldquo;I don&rsquo;t know what&rsquo;s in that cave, you go first,&rdquo; it&rsquo;s more likely to be fine.
You can draw conclusions about a project&rsquo;s user base by looking at available statistics. Depending on your platform, these may include the number of downloads, reviews, issues or tickets, comments, contributions, forks, or &ldquo;stars,&rdquo; whatever those are.
Evaluate social statistics on platforms like GitHub with a grain of salt. They can help you determine how popular a project may be, but only in the same way that restaurant review apps can help you figure out if you should eat at Foo&rsquo;s Grill &amp; Bar. Depending on where Foo&rsquo;s Grill &amp; Bar is, when it opened, and how likely people are to be near it when the invariable steak craving should call, having twenty-six reviews may be a good sign or a terrible one. While you would not expect a project that addresses a very obscure use case or technology to have hundreds of users, having a few active users is, in such a case, just as confidence-inspiring.
External validation can also be useful. For example, packages that are included in a Linux operating system distribution (distro) must conform to stringent standards and undergo vetting. Choosing software that is included in a distro&rsquo;s default repositories can mean it&rsquo;s more likely to be secure.
Perhaps one of the best indications to look for is whether a project&rsquo;s development team is using their own project. Look for issues, discussions, or blog posts that show that the project&rsquo;s creators and maintainers are using what they&rsquo;ve built themselves. Commonly referred to as &ldquo;eating your own dog food,&rdquo; or &ldquo;dogfooding,&rdquo; it&rsquo;s an indicator that the project is most likely to be well-maintained by its developers.
Who&rsquo;s building it The main enemy of good open source software is usually a lack of interest. The parties involved in an open source project can make the difference between a flash-in-the-pan library and a respected long-term utility. Multiple committed maintainers, even making contributions in their spare time, have a much higher success rate of sustaining a project and generating interest.
Projects with healthy interest are usually supported by, and in turn cultivate, a community of contributors and users. New contributors may be actively welcomed, clear guides are available explaining how to help, and project maintainers are available and approachable when people have inevitable questions. Some communities even have chat rooms or forums where people can interact outside of contributions. Active communities help sustain project interest, relevance, and its ensuing quality.
In a less organic fashion, a project can also be sustained through organizations that sponsor it. Governments and companies with financial interest are open source patrons too, and a project that enjoys public sector use or financial backing has added incentive to remain relevant and useful.
How alive is it The recency and frequency of an open source project&rsquo;s activity is perhaps the best indicator of how much attention is likely paid to its security. Look at releases, commit history, changelogs, or documentation revisions to determine if a project is active. As projects vary in size and scope, here are some general things to look for.
Maintaining security is an ongoing endeavor that requires regular monitoring and updates, especially for projects with third-party components. These may be libraries or any part of the project that relies on something outside itself, such as a payment gateway integration. An inactive project is more likely to have outdated code or use outdated versions of components. For a more concrete determination, you can research a project&rsquo;s third-party components and compare their most recent patches or updates with the project&rsquo;s last updates.
Projects without third-party components may have no outside updates to apply. In these cases, you can use recent activity and release notes to determine how committed a project&rsquo;s maintainers may be. Generally, active projects should show updates within the last months, with a notable release within the last year. This can be a good indication of whether the project is using an up-to-date version of its language or framework.
You can also judge how active a project may be by looking at the project maintainers themselves. Active maintainers quickly respond to feedback or new issues, even if it&rsquo;s just to say, &ldquo;We&rsquo;re on it.&rdquo; If the project has a community, its maintainers are a part of it. They may have a dedicated website or write regular blogs. They may offer ways to contact them directly and privately, especially to raise security concerns.
Can you understand it Having documentation is a baseline requirement for a project that&rsquo;s intended for anyone but its creator to use. Good open source projects have documentation that is easy to follow, honest, and thorough.
Having well-written documentation is one way a project can stand out and demonstrate the thoughtfulness and dedication of its maintainers. A &ldquo;Getting Started&rdquo; section may detail all the requirements and initial set up for running the project. An accurate list of topics in the documentation enables users to quickly find the information they need. A clear license statement leaves no doubt as to how the project can be used, and for what purposes. These are characteristic aspects of documentation that serves its users.
A project that is following sound coding practices likely has code that is as readable as its documentation. Code that is easy to read lends itself to being understood. Generally, it has clearly defined and appropriately-named functions and variables, a logical flow, and apparent purpose. Readable code is easier to fix, secure, and build upon.
How compatible is it A few factors will determine how compatible a project is with your goals. These are objective qualities, and can be determined by looking at a project&rsquo;s repository files. They include:
Code language Specific technologies or frameworks License compatibility Compatibility doesn&rsquo;t necessarily mean a direct match. Different code languages can interact with each other, as can various technologies and frameworks. You should carefully read a project&rsquo;s license to understand if it permits usage for your goal, or if it is compatible with a license you would like to use.
Ultimately, a project that satisfies all these criteria may still not quite suit your use case. Part of the beauty of open source software, however, is that you may still benefit from it by making alterations that better suit your usage. If those alterations make the project better for everyone, you can pay it back and pay it forward by contributing your work to the project.
Proper care and feeding of an open source project Once you adopt an open source project, a little attention is required to make sure it continues to be a boon to your goals. While its maintainers will look after the upstream project files, you alone are responsible for your own copy. Like all software, your open source project must be well-maintained in order to remain as secure and useful as possible.
Have a system that provides you with notifications when updates for your software are made available. Update software promptly, treating each patch as if it were vital to security; it may well be. Keep in mind that open source project creators and maintainers are, in most cases, acting only out of the goodness of their own hearts. If you&rsquo;ve got a particularly awesome one, its developers may make updates and security patches available on a regular basis. It&rsquo;s up to you to keep tabs on updates and promptly apply them.
As with most things in software, keeping your open source additions modular can come in handy. You might use git submodules, branches, or environments to isolate your additions. This can make it easier to apply updates or pinpoint the source of any bugs that arise.
So although an open source project may cost no money, caveat emptor, which means, &ldquo;Jimmy, if we get you a puppy, it&rsquo;s your responsibility to take care of it.&rdquo;
`,url:"https://victoria.dev/posts/how-to-choose-and-care-for-a-secure-open-source-project/"},"https://victoria.dev/archive/if-you-want-to-build-a-treehouse-start-at-the-bottom/":{title:"If you want to build a treehouse, start at the bottom",tags:["cybersecurity","coding"],content:`If you&rsquo;ve ever watched a kid draw a treehouse, you have some idea of how applications are built when security isn&rsquo;t made a priority. It&rsquo;s far more fun to draw the tire swing, front porch, and swimming pool than to worry about how a ten-thousand-gallon bucket of water stays suspended in midair. With too much attention spent on fun and flashy features, foundations suffer.
Of course, spending undue hours building a back end like Fort Knox may not be necessary for your application, either. Being an advocate for security doesn&rsquo;t mean always wearing your tinfoil hat (although you do look dashing in it) but does mean building in an appropriate amount of security.
How much security is appropriate? The answer, frustratingly, is, &ldquo;it depends.&rdquo; The right amount of security for your application depends on who&rsquo;s using it, what it does, and most importantly, what undesirable things it could be made to do. It takes some analysis to make decisions about the kinds of risks your application faces and how you&rsquo;ll prepare to handle them. Okay, now&rsquo;s a good time to don your tinfoil hat. Let&rsquo;s imagine the worst.
Threat modeling: what&rsquo;s the worst that could happen A threat model is a stuffy term for the result of trying to imagine the worst things that could happen to an application. Using your imagination to assess risks (fittingly called risk assessment) is a conveniently non-destructive method for finding ways an application can be attacked. You won&rsquo;t need any tools; just an understanding of how the application might work, and a little imagination. You&rsquo;ll want to record your results with pen and paper. For the younger folks, that means the notes app on your phone.
A few different methodologies for application risk assessment can be found in the software world, including the in-depth NIST Special Publication 800-30. Each method&rsquo;s framework has specific steps and output, and will go into various levels of detail when it comes to defining threats. If following a framework, first choose the one you&rsquo;re most likely to complete. You can always add more depth and detail from there.
Even informal risk assessments are beneficial. Typically taking the form of a set of questions, they may be oriented around possible threats, the impact to assets, or ways a vulnerability could be exploited. Here are some examples of questions addressing each orientation:
What kind of adversary would want to break my app? What would they be after? If the control of x fell into the wrong hands, what could an attacker do with it? Where could a x vulnerability occur in my app? A basic threat model explains the technical, business, and human considerations for each risk. It will typically detail:
The vulnerabilities or components that can cause the risk The impact that a successful execution of the risk would have on the application The consequences for the application&rsquo;s users or organization The result of a risk assessment exercise is your threat model; in other words, a list of things you would very much like not to occur. It is usually sorted in a hierarchy of risks, from the worst to the mildest. The worst risks have the most negative impact, and are most important to protect against. The mildest risks are the most acceptable - while still an undesirable outcome, they have the least negative impact on the application and users.
You can use this resulting hierarchy as a guide to determine how much of your cybersecurity efforts to apply to each risk area. An appropriate amount of security for your application will eliminate (where possible) or mitigate the worst risks.
Pushing left Although it sounds like a dance move meme, pushing left refers instead to building in as much of your planned security as possible in the early stages of software development.
Building software is a lot like building a treehouse, just without the pleasant fresh air. You start with the basic supporting components, such as attaching a platform to a tree. Then comes the framing, walls, and roof, and finally, your rustic-modern Instagram-worthy wall hangings and deer bust.
The further along in the build process you are, the harder and more costly it becomes to make changes to a component that you&rsquo;ve already installed. If you discover a problem with the walls only after the roof is put in place, you may need to change or remove the roof in order to fix it. Similar parallels can be drawn for software components, only without similar ease in detangling the attached parts.
In the case of a treehouse, it&rsquo;s rather impossible to start with decorations or even a roof, since you can&rsquo;t really suspend them in midair. In the case of software development, it is, unfortunately, possible to build many top-layer components and abstractions without a sufficient supporting architecture. A push-left approach views each additional layer as adding cost and complication. Pushing left means attempting to mitigate security risks as much as possible at each development stage before proceeding to the next.
Building bottom-to-top By considering your threat model in the early stages of developing your application, you reduce the chances of necessitating a costly remodel later on. You can make choices about architecture, components, and code that support the main security goals of your particular application.
While it&rsquo;s not possible to foresee all the functionality your application may one day need to support, it is possible to prepare a solid foundation that allows additional functionality to be added more securely. Building in appropriate security from the bottom to the top will help make mitigating security risks much easier in the future.
`,url:"https://victoria.dev/archive/if-you-want-to-build-a-treehouse-start-at-the-bottom/"},"https://victoria.dev/archive/hugo-vs-jekyll-an-epic-battle-of-static-site-generator-themes/":{title:"Hugo vs Jekyll: an epic battle of static site generator themes",tags:["websites","coding","go","open-source"],content:`I recently took on the task of creating a documentation site theme for two projects. Both projects needed the same basic features, but one uses Jekyll while the other uses Hugo.
In typical developer rationality, there was clearly only one option. I decided to create the same theme in both frameworks, and to give you, dear reader, a side-by-side comparison.
This post isn&rsquo;t a comprehensive theme-building guide, but intended to familiarize you with the process of building a theme in either generator. Here&rsquo;s what we&rsquo;ll cover:
How theme files are organized Where to put content How templating works Creating a top-level menu with the pages object Creating a menu with nested links from a data list Putting the template together Create a stylesheet Sass and CSS in Jekyll Sass and Hugo Pipes in Hugo Configure and deploy to GitHub Pages Configure Jekyll Configure Hugo Deploy to GitHub Pages Showtime Wait who won Here&rsquo;s a crappy wireframe of the theme I&rsquo;m going to create.
If you&rsquo;re planning to build-along, it may be helpful to serve the theme locally as you build it; both generators offer this functionality. For Jekyll, run jekyll serve, and for Hugo, hugo serve.
There are two main elements: the main content area, and the all-important sidebar menu. To create them, you&rsquo;ll need template files that tell the site generator how to generate the HTML page. To organize theme template files in a sensible way, you first need to know what directory structure the site generator expects.
How theme files are organized Jekyll supports gem-based themes, which users can install like any other Ruby gems. This method hides theme files in the gem, so for the purposes of this comparison, we aren&rsquo;t using gem-based themes.
When you run jekyll new-theme &lt;name&gt;, Jekyll will scaffold a new theme for you. Here&rsquo;s what those files look like:
. ├── assets ├── Gemfile ├── _includes ├── _layouts │ ├── default.html │ ├── page.html │ └── post.html ├── LICENSE.txt ├── README.md ├── _sass └── &lt;name&gt;.gemspec The directory names are appropriately descriptive. The _includes directory is for small bits of code that you reuse in different places, in much the same way you&rsquo;d put butter on everything. (Just me?) The _layouts directory contains templates for different types of pages on your site. The _sass folder is for Sass files used to build your site&rsquo;s stylesheet.
You can scaffold a new Hugo theme by running hugo new theme &lt;name&gt;. It has these files:
. ├── archetypes │ └── default.md ├── layouts │ ├── 404.html │ ├── _default │ │ ├── baseof.html │ │ ├── list.html │ │ └── single.html │ ├── index.html │ └── partials │ ├── footer.html │ ├── header.html │ └── head.html ├── LICENSE ├── static │ ├── css │ └── js └── theme.toml You can see some similarities. Hugo&rsquo;s page template files are tucked into layouts/. Note that the _default page type has files for a list.html and a single.html. Unlike Jekyll, Hugo uses these specific file names to distinguish between list pages (like a page with links to all your blog posts on it) and single pages (like one of your blog posts). The layouts/partials/ directory contains the buttery reusable bits, and stylesheet files have a spot picked out in static/css/.
These directory structures aren&rsquo;t set in stone, as both site generators allow some measure of customization. For example, Jekyll lets you define collections, and Hugo makes use of page bundles. These features let you organize your content multiple ways, but for now, lets look at where to put some simple pages.
Where to put content To create a site menu that looks like this:
Introduction Getting Started Configuration Deploying Advanced Usage All Configuration Settings Customizing Help and Support You&rsquo;ll need two sections (&ldquo;Introduction&rdquo; and &ldquo;Advanced Usage&rdquo;) containing their respective subsections.
Jekyll isn&rsquo;t strict with its content location. It expects pages in the root of your site, and will build whatever&rsquo;s there. Here&rsquo;s how you might organize these pages in your Jekyll site root:
. ├── 404.html ├── assets ├── Gemfile ├── _includes ├── index.markdown ├── intro │ ├── config.md │ ├── deploy.md │ ├── index.md │ └── quickstart.md ├── _layouts │ ├── default.html │ ├── page.html │ └── post.html ├── LICENSE.txt ├── README.md ├── _sass ├── &lt;name&gt;.gemspec └── usage ├── customizing.md ├── index.md ├── settings.md └── support.md You can change the location of the site source in your Jekyll configuration.
In Hugo, all rendered content is expected in the content/ folder. This prevents Hugo from trying to render pages you don&rsquo;t want, such as 404.html, as site content. Here&rsquo;s how you might organize your content/ directory in Hugo:
. ├── _index.md ├── intro │ ├── config.md │ ├── deploy.md │ ├── _index.md │ └── quickstart.md └── usage ├── customizing.md ├── _index.md ├── settings.md └── support.md To Hugo, _index.md and index.md mean different things. It can be helpful to know what kind of Page Bundle you want for each section: Leaf, which has no children, or Branch.
Now that you have some idea of where to put things, let&rsquo;s look at how to build a page template.
How templating works Jekyll page templates are built with the Liquid templating language. It uses braces to output variable content to a page, such as the page&rsquo;s title: {{ page.title }}.
Hugo&rsquo;s templates also use braces, but they&rsquo;re built with Go Templates. The syntax is similar, but different: {{ .Title }}.
Both Liquid and Go Templates can handle logic. Liquid uses tags syntax to denote logic operations:
{% if user %} Hello {{ user.name }}! {% endif %} And Go Templates places its functions and arguments in its braces syntax:
{{ if .User }} Hello {{ .User }}! {{ end }} Templating languages allow you to build one skeleton HTML page, then tell the site generator to put variable content in areas you define. Let&rsquo;s compare two possible default page templates for Jekyll and Hugo.
Jekyll&rsquo;s scaffold default theme is bare, so we&rsquo;ll look at their starter theme Minima. Here&rsquo;s _layouts/default.html in Jekyll (Liquid):
&lt;!DOCTYPE html&gt; &lt;html lang=&#34;{{ page.lang | default: site.lang | default: &#34;en&#34; }}&#34;&gt; {%- include head.html -%} &lt;body&gt; {%- include header.html -%} &lt;main class=&#34;page-content&#34; aria-label=&#34;Content&#34;&gt; &lt;div class=&#34;wrapper&#34;&gt; {{ content }} &lt;/div&gt; &lt;/main&gt; {%- include footer.html -%} &lt;/body&gt; &lt;/html&gt; Here&rsquo;s Hugo&rsquo;s scaffold theme layouts/_default/baseof.html (Go Templates):
&lt;!DOCTYPE html&gt; &lt;html&gt; {{- partial &#34;head.html&#34; . -}} &lt;body&gt; {{- partial &#34;header.html&#34; . -}} &lt;div id=&#34;content&#34;&gt; {{- block &#34;main&#34; . }}{{- end }} &lt;/div&gt; {{- partial &#34;footer.html&#34; . -}} &lt;/body&gt; &lt;/html&gt; Different syntax, same idea. Both templates pull in reusable bits for head.html, header.html, and footer.html. These show up on a lot of pages, so it makes sense not to have to repeat yourself. Both templates also have a spot for the main content, though the Jekyll template uses a variable ({{ content }}) while Hugo uses a block ({{- block &quot;main&quot; . }}{{- end }}). Blocks are just another way Hugo lets you define reusable bits.
Now that you know how templating works, you can build the sidebar menu for the theme.
Creating a top-level menu with the pages object You can programmatically create a top-level menu from your pages. It will look like this:
Introduction Advanced Usage Let&rsquo;s start with Jekyll. You can display links to site pages in your Liquid template by iterating through the site.pages object that Jekyll provides and building a list:
&lt;ul&gt; {% for page in site.pages %} &lt;li&gt;&lt;a href=&#34;{{ page.url | absolute_url }}&#34;&gt;{{ page.title }}&lt;/a&gt;&lt;/li&gt; {% endfor %} &lt;/ul&gt; This returns all of the site&rsquo;s pages, including all the ones that you might not want, like 404.html. You can filter for the pages you actually want with a couple more tags, such as conditionally including pages if they have a section: true parameter set:
&lt;ul&gt; {% for page in site.pages %} {%- if page.section -%} &lt;li&gt;&lt;a href=&#34;{{ page.url | absolute_url }}&#34;&gt;{{ page.title }}&lt;/a&gt;&lt;/li&gt; {%- endif -%} {% endfor %} &lt;/ul&gt; You can achieve the same effect with slightly less code in Hugo. Loop through Hugo&rsquo;s .Pages object using Go Template&rsquo;s range action:
&lt;ul&gt; {{ range .Pages }} &lt;li&gt; &lt;a href=&#34;{{.Permalink}}&#34;&gt;{{.Title}}&lt;/a&gt; &lt;/li&gt; {{ end }} &lt;/ul&gt; This template uses the .Pages object to return all the top-level pages in content/ of your Hugo site. Since Hugo uses a specific folder for the site content you want rendered, there&rsquo;s no additional filtering necessary to build a simple menu of site pages.
Creating a menu with nested links from a data list Both site generators can use a separately defined data list of links to render a menu in your template. This is more suitable for creating nested links, like this:
Introduction Getting Started Configuration Deploying Advanced Usage All Configuration Settings Customizing Help and Support Jekyll supports data files in a few formats, including YAML. Here&rsquo;s the definition for the menu above in _data/menu.yml:
section: - page: Introduction url: /intro subsection: - page: Getting Started url: /intro/quickstart - page: Configuration url: /intro/config - page: Deploying url: /intro/deploy - page: Advanced Usage url: /usage subsection: - page: Customizing url: /usage/customizing - page: All Configuration Settings url: /usage/settings - page: Help and Support url: /usage/support Here&rsquo;s how to render the data in the sidebar template:
{% for a in site.data.menu.section %} &lt;a href=&#34;{{ a.url }}&#34;&gt;{{ a.page }}&lt;/a&gt; &lt;ul&gt; {% for b in a.subsection %} &lt;li&gt;&lt;a href=&#34;{{ b.url }}&#34;&gt;{{ b.page }}&lt;/a&gt;&lt;/li&gt; {% endfor %} &lt;/ul&gt; {% endfor %} This method allows you to build a custom menu, two nesting levels deep. The nesting levels are limited by the for loops in the template. For a recursive version that handles further levels of nesting, see Nested tree navigation with recursion.
Hugo does something similar with its menu templates. You can define menu links in your Hugo site config, and even add useful properties that Hugo understands, like weighting. Here&rsquo;s a definition of the menu above in config.yaml:
sectionPagesMenu: main menu: main: - identifier: intro name: Introduction url: /intro/ weight: 1 - name: Getting Started parent: intro url: /intro/quickstart/ weight: 1 - name: Configuration parent: intro url: /intro/config/ weight: 2 - name: Deploying parent: intro url: /intro/deploy/ weight: 3 - identifier: usage name: Advanced Usage url: /usage/ - name: Customizing parent: usage url: /usage/customizing/ weight: 2 - name: All Configuration Settings parent: usage url: /usage/settings/ weight: 1 - name: Help and Support parent: usage url: /usage/support/ weight: 3 Hugo uses the identifier, which must match the section name, along with the parent variable to handle nesting. Here&rsquo;s how to render the menu in the sidebar template:
&lt;ul&gt; {{ range .Site.Menus.main }} {{ if .HasChildren }} &lt;li&gt; &lt;a href=&#34;{{ .URL }}&#34;&gt;{{ .Name }}&lt;/a&gt; &lt;/li&gt; &lt;ul class=&#34;sub-menu&#34;&gt; {{ range .Children }} &lt;li&gt; &lt;a href=&#34;{{ .URL }}&#34;&gt;{{ .Name }}&lt;/a&gt; &lt;/li&gt; {{ end }} &lt;/ul&gt; {{ else }} &lt;li&gt; &lt;a href=&#34;{{ .URL }}&#34;&gt;{{ .Name }}&lt;/a&gt; &lt;/li&gt; {{ end }} {{ end }} &lt;/ul&gt; The range function iterates over the menu data, and Hugo&rsquo;s .Children variable handles nested pages for you.
Putting the template together With your menu in your reusable sidebar bit (_includes/sidebar.html for Jekyll and partials/sidebar.html for Hugo), you can add it to the default.html template.
In Jekyll:
&lt;!DOCTYPE html&gt; &lt;html lang=&#34;{{ page.lang | default: site.lang | default: &#34;en&#34; }}&#34;&gt; {%- include head.html -%} &lt;body&gt; {%- include sidebar.html -%} {%- include header.html -%} &lt;div id=&#34;content&#34; class=&#34;page-content&#34; aria-label=&#34;Content&#34;&gt; {{ content }} &lt;/div&gt; {%- include footer.html -%} &lt;/body&gt; &lt;/html&gt; In Hugo:
&lt;!DOCTYPE html&gt; &lt;html&gt; {{- partial &#34;head.html&#34; . -}} &lt;body&gt; {{- partial &#34;sidebar.html&#34; . -}} {{- partial &#34;header.html&#34; . -}} &lt;div id=&#34;content&#34; class=&#34;page-content&#34; aria-label=&#34;Content&#34;&gt; {{- block &#34;main&#34; . }}{{- end }} &lt;/div&gt; {{- partial &#34;footer.html&#34; . -}} &lt;/body&gt; &lt;/html&gt; When the site is generated, each page will contain all the code from your sidebar.html.
Create a stylesheet Both site generators accept Sass for creating CSS stylesheets. Jekyll has Sass processing built in, and Hugo uses Hugo Pipes. Both options have some quirks.
Sass and CSS in Jekyll To process a Sass file in Jekyll, create your style definitions in the _sass directory. For example, in a file at _sass/style-definitions.scss:
$background-color: #eef !default; $text-color: #111 !default; body { background-color: $background-color; color: $text-color; } Jekyll won&rsquo;t generate this file directly, as it only processes files with front matter. To create the end-result filepath for your site&rsquo;s stylesheet, use a placeholder with empty front matter where you want the .css file to appear. For example, assets/css/style.scss. In this file, simply import your styles:
--- --- @import &#34;style-definitions&#34;; This rather hackish configuration has an upside: you can use Liquid template tags and variables in your placeholder file. This is a nice way to allow users to set variables from the site _config.yml, for example.
The resulting CSS stylesheet in your generated site has the path /assets/css/style.css. You can link to it in your site&rsquo;s head.html using:
&lt;link rel=&#34;stylesheet&#34; href=&#34;{{ &#34;/assets/css/style.css&#34; | relative_url }}&#34; media=&#34;screen&#34;&gt; Sass and Hugo Pipes in Hugo Hugo uses Hugo Pipes to process Sass to CSS. You can achieve this by using Hugo&rsquo;s asset processing function, resources.ToCSS, which expects a source in the assets/ directory. It takes the SCSS file as an argument. With your style definitions in a Sass file at assets/sass/style.scss, here&rsquo;s how to get, process, and link your Sass in your theme&rsquo;s head.html:
{{ $style := resources.Get &#34;/sass/style.scss&#34; | resources.ToCSS }} &lt;link rel=&#34;stylesheet&#34; href=&#34;{{ $style.RelPermalink }}&#34; media=&#34;screen&#34;&gt; Hugo asset processing requires extended Hugo, which you may not have by default. You can get extended Hugo from the releases page.
Configure and deploy to GitHub Pages Before your site generator can build your site, it needs a configuration file to set some necessary parameters. Configuration files live in the site root directory. Among other settings, you can declare the name of the theme to use when building the site.
Configure Jekyll Here&rsquo;s a minimal _config.yml for Jekyll:
title: Your awesome title description: &gt;- # this means to ignore newlines until &#34;baseurl:&#34; Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description. baseurl: &#34;&#34; # the subpath of your site, e.g. /blog url: &#34;&#34; # the base hostname &amp; protocol for your site, e.g. http://example.com theme: # for gem-based themes remote_theme: # for themes hosted on GitHub, when used with GitHub Pages With remote_theme, any Jekyll theme hosted on GitHub can be used with sites hosted on GitHub Pages.
Jekyll has a default configuration, so any parameters added to your configuration file will override the defaults. Here are additional configuration settings.
Configure Hugo Here&rsquo;s a minimal example of Hugo&rsquo;s config.yml:
baseURL: https://example.com/ # The full domain your site will live at languageCode: en-us title: Hugo Docs Site theme: # theme name Hugo makes no assumptions, so if a necessary parameter is missing, you&rsquo;ll see a warning when building or serving your site. Here are all configuration settings for Hugo.
Deploy to GitHub Pages Both generators build your site with a command.
For Jekyll, use jekyll build. See further build options here.
For Hugo, use hugo. You can run hugo help or see further build options here.
You&rsquo;ll have to choose the source for your GitHub Pages site; once done, your site will update each time you push a new build. Of course, you can also automate your GitHub Pages build using GitHub Actions. Here&rsquo;s one for building and deploying with Hugo, and one for building and deploying Jekyll.
Showtime All the substantial differences between these two generators are under the hood; all the same, let&rsquo;s take a look at the finished themes, in two color variations.
Here&rsquo;s Hugo:
Here&rsquo;s Jekyll:
Spiffy!
Wait who won 🤷
Both Hugo and Jekyll have their quirks and conveniences.
From this developer&rsquo;s perspective, Jekyll is a workable choice for simple sites without complicated organizational needs. If you&rsquo;re looking to render some one-page posts in an available theme and host with GitHub Pages, Jekyll will get you up and running fairly quickly.
Personally, I use Hugo. I like the organizational capabilities of its Page Bundles, and it&rsquo;s backed by a dedicated and conscientious team that really seems to strive to facilitate convenience for their users. This is evident in Hugo&rsquo;s many functions, and handy tricks like Image Processing and Shortcodes. They seem to release new fixes and versions about as often as I make a new cup of coffee.
If you still can&rsquo;t decide, don&rsquo;t worry. Many themes are available for both Hugo and Jekyll! Start with one, switch later if you want. That&rsquo;s the benefit of having options.
`,url:"https://victoria.dev/archive/hugo-vs-jekyll-an-epic-battle-of-static-site-generator-themes/"},"https://victoria.dev/archive/outsourcing-security-with-1password-authy-and-privacy.com/":{title:"Outsourcing security with 1Password, Authy, and Privacy.com",tags:["cybersecurity","privacy","data","life"],content:`We&rsquo;ve already got enough to deal with without worrying about our cybersecurity. When humans are busy and under stress, we tend to get lax in less-obviously-pressing areas, like the integrity of our online accounts. These areas only become an obvious problem when it&rsquo;s too late for prevention.
Cybersecurity can be fiddly and time-consuming. You might need to reset forgotten passwords, transfer multifactor authentication (MFA) codes to different devices, or deal with the fallout of compromised payment details in the event one of your accounts is still breached.
Thankfully, most of the work necessary to keep up our cybersecurity measures can be outsourced.
Here are three changes you can make to significantly reduce the chances of needing to fiddle with any of these things again.
1Password I&rsquo;ve historically avoided password managers because of an irrational knee-jerk reaction to putting all my eggs in one basket. You know what&rsquo;s great for irrational reactions? Education.
To figure out if putting all my passwords into a password manager is more secure than not using one, I set out to see what some smart people wrote about it.
First, we need to know a thing or two about passwords. Troy Hunt figured out almost a decade ago that trying to remember strong passwords doesn&rsquo;t work. In more recent times, Alex Weinert expanded on this in Your Pa$$word doesn&rsquo;t matter. TL;DR: our brains aren&rsquo;t better at passwords than computers, and please use MFA.
So passwords don&rsquo;t matter, but complicated passwords are still better than memorable and guessable ones. Since I&rsquo;ve next to no hope of remembering a dozen variations of p/q2-q4! (I&rsquo;m not a chess player), this is a task I can outsource to 1Password. I&rsquo;ll still need to remember one, long, complicated master password - 1Password uses this to encrypt my data, so I really can&rsquo;t lose it - but I can handle just one.
Using 1Password specifically has another, decidedly obvious, advantage. I chose 1Password because of their Watchtower feature. Thanks to Troy Hunt&rsquo;s Have I Been Pwned, Watchtower will alert you if any of your passwords show up in a breach so you can change them. Passwords still don&rsquo;t completely work, but this is probably the best band-aid there is.
One last bonus is that using a password manager is a heck of a lot more convenient. I don&rsquo;t need to take a few tries to type in a complicated password. I don&rsquo;t end up spending time resetting passwords I&rsquo;ve forgotten on sites I only rarely use.
When tasked with remembering all their own passwords, people typically create simpler passwords that are easier to remember &ndash; and easier to hack. This occurs most frequently on sites that are considered unimportant. Using 1Password and generated passwords, those sites are now also first-class citizens in the land of strong passwords, instead of being half-abandoned and half-open attack vectors.
So, yes, all my eggs are in one basket. A well-protected, complex, and monitored basket.
Authy Okay - so it&rsquo;s more like one-and-a-half baskets. 🤷🏻
Authy, from the folks over at Twilio, provides a 2FA solution that&rsquo;s more secure than SMS. Unlike Google Authenticator, you can choose to back up your 2FA codes in case you lose or change your phone. (1Password offers 2FA functionality as well - but, you know, redundancies.)
With Authy, your back up is encrypted with your password, similarly to how 1Password works. This makes it the second password you can&rsquo;t forget, if you don&rsquo;t want to lose access to your codes. If you reset your account, they all go away. I can deal with remembering two passwords; I&rsquo;ll take that trade.
I&rsquo;ve tried other methods of MFA, including hardware keys, which can make accessing accounts on your phone more complicated than I care to put up with. I find the combination of 1Password and Authy to be the most practical combination of convenience and security that yet exists to my knowledge.
Privacy.com Finally, there&rsquo;s one last line of defense you can put in place in the unfortunate event that one of your accounts is still compromised. All the strong passwords and MFA in the world won&rsquo;t help if you open the doors yourself, and scams and phishing are a thing.
Since it&rsquo;s rather impractical to use a different real credit card every place you shop, virtual cards are just a great idea. There&rsquo;s no good reason to spend an afternoon (or more) resetting your payment information on every account just to thwart a misbehaving merchant or patch up a data breach from that online shop for cute salt shakers you made a purchase at last year (just me?).
As a bonus, a partnership between 1Password and Privacy.com lets you easily create virtual credit cards using the 1Password extension.
By setting up a separate virtual card for each merchant, in the event that one of those merchants is compromised, you can simply pause or delete that card. None of your other accounts or actual bank details are caught up in the process. Cards can have time-based limits or be one-off burner numbers, making them ideal for setting up subscriptions.
This is the sort of basic functionality that I hope, one day, becomes more prevalent from banks and credit cards. In the meantime, I&rsquo;ll keep using Privacy.com. That&rsquo;s my referral link; if you&rsquo;d like to thank me by using it, we&rsquo;ll both get five bucks as a bonus.
Outsource better security All together, implementing these changes will probably take up an afternoon, depending on how many accounts you have. It&rsquo;s worth it for the time you&rsquo;d otherwise spend resetting passwords, setting up new devices, or (knock on wood) recovering from compromised banking details. Best of all, you&rsquo;ll have continual protection just running in the background.
1Password Authy Privacy.com We have the technology. Free up some brain cycles to focus on other things - or simply remove some unnecessary stress from your life by outsourcing the fiddly bits.
Want to give the gift of cybersecurity to someone you know? Get them started with a cybersecurity starter pack.
`,url:"https://victoria.dev/archive/outsourcing-security-with-1password-authy-and-privacy.com/"},"https://victoria.dev/posts/sqlite-in-production-with-wal/":{title:"SQLite in production with WAL",tags:["data","computing"],content:`Update: read the HackerNews discussion.
SQLite (&ldquo;see-quell-lite&rdquo;) is a lightweight Sequel, or Structured Query Language (SQL), database engine. Instead of using the client-server database management system model, SQLite is self-contained in a single file. It is library, database, and data, all in one package.
For certain applications, SQLite is a solid choice for a production database. It&rsquo;s lightweight, ultra-portable, and has no external dependencies. Remember when MacBook Air first came out? It&rsquo;s nothing like that.
SQLite is best suited for production use in applications that:
Desire fast and simple set up. Require high reliability in a small package. Have, and want to retain, a small footprint. Are read-heavy but not write-heavy. Don&rsquo;t need multiple user accounts or features like multiversion concurrency snapshots. If your application can benefit from SQLite&rsquo;s serverless convenience, you may like to know about the different modes available for managing database changes.
With and without WAL POSIX system call fsync() commits buffered data (data saved in the operating system cache) referred to by a specified file descriptor to permanent storage or disk. This is relevant to understanding the difference between SQLite&rsquo;s two modes, as fsync() will block until the device reports the transfer is complete.
For efficiency, SQLite uses atomic commits to batch database changes into a single transaction. This enables the apparent writing of many transactions to a database file simultaneously. Atomic commits are performed using one of two modes: a rollback journal, or a write-ahead log (WAL).
Rollback journal A rollback journal is essentially a back-up file created by SQLite before write changes occur on a database file. It has the advantage of providing high reliability by helping SQLite restore the database to its original state in case a write operation is compromised during the disk-writing process.
Assuming a cold cache, SQLite first needs to read the relevant pages from a database file before it can write to it. Information is read out into the operating system cache, then transferred into user space. SQLite obtains a reserved lock on the database file, preventing other processes from writing to the database. At this point, other processes may still read from the database.
SQLite creates a separate file, the rollback journal, with the original content of the pages that will be changed. Initially existing in the cache, the rollback journal is written to persistent disk storage with fsync() to enable SQLite to restore the database should its next operations be compromised.
SQLite then obtains an exclusive lock preventing other processes from reading or writing, and writes the page changes to the database file in cache. Since writing to disk is slower than interaction with the cache, writing to disk doesn&rsquo;t occur immediately. The rollback journal continues to exist until changes are safely written to disk, with a second fsync(). From a user-space process point of view, the change to the disk (the COMMIT, or end of the transaction) happens instantaneously once the rollback journal is deleted - hence, atomic commits. However, the two fsync() operations required to complete the COMMIT make this option, from a transactional standpoint, slower than SQLite&rsquo;s lesser known WAL mode.
Write-ahead logging (WAL) While the rollback journal method uses a separate file to preserve the original database state, the WAL method uses a separate WAL file to instead record the changes. Instead of a COMMIT depending on writing changes to disk, a COMMIT in WAL mode occurs when a record of one or more commits is appended to the WAL. This has the advantage of not requiring blocking read or write operations to the database file in order to make a COMMIT, so more transactions can happen concurrently.
WAL mode introduces the concept of the checkpoint, which is when the WAL file is synced to persistent storage before all its transactions are transferred to the database file. You can optionally specify when this occurs, but SQLite provides reasonable defaults. The checkpoint is the WAL version of the atomic commit.
In WAL mode, write transactions are performed faster than in the traditional rollback journal mode. Each transaction involves writing the changes only once to the WAL file instead of twice - to the rollback journal, and then to disk - before the COMMIT signals that the transaction is over.
The simplicity of SQLite For medium-sized read-heavy applications, SQLite may be a great choice. Using SQLite in WAL mode may make it an even better one. Benchmarks on the smallest EC2 instance, with no provisioned IOPS, put this little trooper at 400 write transactions per second, and thousands of reads. That&rsquo;s some perfectly adequate capability, in a perfectly compact package.
`,url:"https://victoria.dev/posts/sqlite-in-production-with-wal/"},"https://victoria.dev/posts/multithreaded-python-slithering-through-an-i/o-bottleneck/":{title:"Multithreaded Python: slithering through an I/O bottleneck",tags:["python","computing","ci/cd","data","open-source"],content:`I recently developed a project that I called Hydra: a multithreaded link checker written in Python. Unlike many Python site crawlers I found while researching, Hydra uses only standard libraries, with no external dependencies like BeautifulSoup. It&rsquo;s intended to be run as part of a CI/CD process, so part of its success depended on being fast.
Multiple threads in Python is a bit of a bitey subject (not sorry) in that the Python interpreter doesn&rsquo;t actually let multiple threads execute at the same time. Python&rsquo;s Global Interpreter Lock, or GIL, prevents multiple threads from executing Python bytecodes at once. Each thread that wants to execute must first wait for the GIL to be released by the currently executing thread. The GIL is pretty much the microphone in a low-budget conference panel, except where no one gets to shout.
This has the advantage of preventing race conditions. It does, however, lack the performance advantages afforded by running multiple tasks in parallel. (If you&rsquo;d like a refresher on concurrency, parallelism, and multithreading, see Concurrency, parallelism, and the many threads of Santa Claus.) While I prefer Go for its convenient first-class primitives that support concurrency (see Goroutines), this project&rsquo;s recipients were more comfortable with Python. I took it as an opportunity to test and explore!
Simultaneously performing multiple tasks in Python isn&rsquo;t impossible; it just takes a little extra work. For Hydra, the main advantage is in overcoming the input/output (I/O) bottleneck.
In order to get web pages to check, Hydra needs to go out to the Internet and fetch them. When compared to tasks that are performed by the CPU alone, going out over the network is comparatively slower. How slow?
Here are approximate timings for tasks performed on a typical PC:
Task Time CPU execute typical instruction 1/1,000,000,000 sec = 1 nanosec CPU fetch from L1 cache memory 0.5 nanosec CPU branch misprediction 5 nanosec CPU fetch from L2 cache memory 7 nanosec RAM Mutex lock/unlock 25 nanosec RAM fetch from main memory 100 nanosec Network send 2K bytes over 1Gbps network 20,000 nanosec RAM read 1MB sequentially from memory 250,000 nanosec Disk fetch from new disk location (seek) 8,000,000 nanosec (8ms) Disk read 1MB sequentially from disk 20,000,000 nanosec (20ms) Network send packet US to Europe and back 150,000,000 nanosec (150ms) Peter Norvig first published these numbers some years ago in Teach Yourself Programming in Ten Years. Since computers and their components change year over year, the exact numbers shown above aren&rsquo;t the point. What these numbers help to illustrate is the difference, in orders of magnitude, between operations.
Compare the difference between fetching from main memory and sending a simple packet over the Internet. While both these operations occur in less than the blink of an eye (literally) from a human perspective, you can see that sending a simple packet over the Internet is over a million times slower than fetching from RAM. It&rsquo;s a difference that, in a single-thread program, can quickly accumulate to form troublesome bottlenecks.
In Hydra, the task of parsing response data and assembling results into a report is relatively fast, since it all happens on the CPU. The slowest portion of the program&rsquo;s execution, by over six orders of magnitude, is network latency. Not only does Hydra need to fetch packets, but whole web pages! One way of improving Hydra&rsquo;s performance is to find a way for the page fetching tasks to execute without blocking the main thread.
Python has a couple options for doing tasks in parallel: multiple processes, or multiple threads. These methods allow you to circumvent the GIL and speed up execution in a couple different ways.
Multiple processes To execute parallel tasks using multiple processes, you can use Python&rsquo;s ProcessPoolExecutor. A concrete subclass of Executor from the concurrent.futures module, ProcessPoolExecutor uses a pool of processes spawned with the multiprocessing module to avoid the GIL.
This option uses worker subprocesses that maximally default to the number of processors on the machine. The multiprocessing module allows you to maximally parallelize function execution across processes, which can really speed up compute-bound (or CPU-bound) tasks.
Since the main bottleneck for Hydra is I/O and not the processing to be done by the CPU, I&rsquo;m better served by using multiple threads.
Multiple threads Fittingly named, Python&rsquo;s ThreadPoolExecutor uses a pool of threads to execute asynchronous tasks. Also a subclass of Executor, it uses a defined number of maximum worker threads (at least five by default, according to the formula min(32, os.cpu_count() + 4)) and reuses idle threads before starting new ones, making it pretty efficient.
Here is a snippet of Hydra with comments showing how Hydra uses ThreadPoolExecutor to achieve parallel multithreaded bliss:
# Create the Checker class class Checker: # Queue of links to be checked TO_PROCESS = Queue() # Maximum workers to run THREADS = 100 # Maximum seconds to wait for HTTP response TIMEOUT = 60 def __init__(self, url): ... # Create the thread pool self.pool = futures.ThreadPoolExecutor(max_workers=self.THREADS) def run(self): # Run until the TO_PROCESS queue is empty while True: try: target_url = self.TO_PROCESS.get(block=True, timeout=2) # If we haven&#39;t already checked this link if target_url[&#34;url&#34;] not in self.visited: # Mark it as visited self.visited.add(target_url[&#34;url&#34;]) # Submit the link to the pool job = self.pool.submit(self.load_url, target_url, self.TIMEOUT) job.add_done_callback(self.handle_future) except Empty: return except Exception as e: print(e) You can view the full code in Hydra&rsquo;s GitHub repository.
Single thread to multithread If you&rsquo;d like to see the full effect, I compared the run times for checking my website between a prototype single-thread program, and the multiheadedmultithreaded Hydra.
time python3 slow-link-check.py https://victoria.dev real 17m34.084s user 11m40.761s sys 0m5.436s time python3 hydra.py https://victoria.dev real 0m15.729s user 0m11.071s sys 0m2.526s The single-thread program, which blocks on I/O, ran in about seventeen minutes. When I first ran the multithreaded version, it finished in 1m13.358s - after some profiling and tuning, it took a little under sixteen seconds. Again, the exact times don&rsquo;t mean all that much; they&rsquo;ll vary depending on factors such as the size of the site being crawled, your network speed, and your program&rsquo;s balance between the overhead of thread management and the benefits of parallelism.
The more important thing, and the result I&rsquo;ll take any day, is a program that runs some orders of magnitude faster.
`,url:"https://victoria.dev/posts/multithreaded-python-slithering-through-an-i/o-bottleneck/"},"https://victoria.dev/posts/breaking-bottlenecks/":{title:"Breaking bottlenecks 🍾",tags:["computing","ci/cd","coding","cybersecurity","go","python"],content:`I recently gave a lecture on the benefits of building non-blocking processes. This is a write-up of the full talk, minus any &ldquo;ums&rdquo; that may have occurred.
I&rsquo;ve been helping out a group called the Open Web Application Security Project (OWASP). They&rsquo;re a non-profit foundation that produces some of the foremost application testing guides and cybersecurity resources. OWASP&rsquo;s publications, checklists, and reference materials are a help to security professionals, penetration testers, and developers all over the world. Most of the individual teams that create these materials are run almost entirely by volunteers.
OWASP is a great group doing important work. I&rsquo;ve seen this firsthand as part of the core team that produces the Web Security Testing Guide. However, while OWASP inspires in its large volunteer base, it lacks in the area of central organization.
This lack of organization was most recently apparent in the group&rsquo;s website, OWASP.org. A big organization with an even bigger website to match, OWASP.org enjoys hundreds of thousands of visitors. Unfortunately, many of its pages - individually managed by disparate projects - are infrequently updated. Some are abandoned. The website as a whole lacks a centralized quality assurance process, and as a result, OWASP.org is peppered with broken links.
The trouble with broken links Customers don&rsquo;t like broken links; attackers really do. That&rsquo;s because broken links are a security vulnerability. Broken links can signal opportunities for attacks like broken link hijacking and subdomain takeovers. At their least effective, these attacks can be embarrassing; at their worst, severely damaging to businesses and organizations. One OWASP group, the Application Security Verification Standard (ASVS) project, writes about integrity controls that can help to mitigate the likelihood of these attacks. This knowledge, unfortunately, has not yet propagated throughout the rest of OWASP yet.
This is the story of how I created a fast and efficient tool to help OWASP solve this problem.
The job I took on the task of creating a program that could run as part of a CI/CD process to detect and report broken links. The program needed to:
Find and enumerate all the broken links on OWASP.org in a report. Keep track of the parent pages the broken links were on so they could be fixed. Run efficiently as part of a CI/CD pipeline. Essentially; I need to build a web crawler.
My original journey through this process was also in Python, as that was a comfortable language choice for everyone in the OWASP group. Personally, I prefer to use Go for higher performance as it offers more convenient concurrency primitives. Between the task and this talk, I wrote three programs: a prototype single-thread Python program, a multithreaded Python program, and a Go program using goroutines. We&rsquo;ll see a comparison of how each worked out near the end of the talk - first, let&rsquo;s explore how to build a web crawler.
Prototyping a web crawler Here&rsquo;s what our web crawler will need to do:
Get the HTML data of the first page of the website (for example, https://victoria.dev) Check all of the links on the page Keep track of the links we&rsquo;ve already visited so we don&rsquo;t end up checking them twice Record any broken links we find Fetch more HTML data from any valid links on the page, as long as they&rsquo;re in the same domain (https://victoria.dev and not https://github.com, for instance) Repeat step #2 until all of the links on the site have been checked Here&rsquo;s what the execution flow will look like:
As you can see, the nodes &ldquo;GET page&rdquo; -&gt; &ldquo;HTML&rdquo; -&gt; &ldquo;Parse links&rdquo; -&gt; &ldquo;Valid link&rdquo; -&gt; &ldquo;Check visited&rdquo; all form a loop. These are what enable our web crawler to continue crawling until all the links on the site have been accounted for in the &ldquo;Check visited&rdquo; node. When the crawler encounters links it&rsquo;s already checked, it will &ldquo;Stop.&rdquo; This loop will become more important in a moment.
For now, the question on everyone&rsquo;s mind (I hope): how do we make it fast?
How fast can you do the thing Here are some approximate timings for tasks performed on a typical PC:
Type Task Time CPU execute typical instruction 1/1,000,000,000 sec = 1 nanosec CPU fetch from L1 cache memory 0.5 nanosec CPU branch misprediction 5 nanosec CPU fetch from L2 cache memory 7 nanosec RAM Mutex lock/unlock 25 nanosec RAM fetch from main memory 100 nanosec RAM read 1MB sequentially from memory 250,000 nanosec Disk fetch from new disk location (seek) 8,000,000 nanosec (8ms) Disk read 1MB sequentially from disk 20,000,000 nanosec (20ms) Network send packet US to Europe and back 150,000,000 nanosec (150ms) Peter Norvig first published these numbers some years ago in Teach Yourself Programming in Ten Years. They typically crop up now and then in articles titled along the lines of, &ldquo;Latency numbers every developer should know.&rdquo;
Since computers and their components change year over year, the exact numbers shown above aren&rsquo;t the point. What these numbers help to illustrate is the difference, in orders of magnitude, between operations.
Compare the difference between fetching from main memory and sending a simple packet over the Internet. While both these operations occur in less than the blink of an eye (literally) from a human perspective, you can see that sending a simple packet over the Internet is over a million times slower than fetching from RAM. It&rsquo;s a difference that, in a single-thread program, can quickly accumulate to form troublesome bottlenecks.
Bottleneck: network latency The numbers above mean that the difference in time it takes to send something over the Internet compared to fetching data from main memory is over six orders of magnitude. Remember the loop in our execution chart? The &ldquo;GET page&rdquo; node, in which our crawler fetches page data over the network, is going to be a million times slower than the next slowest thing in the loop!
We don&rsquo;t need to run our prototype to see what that means in practical terms; we can estimate it. Let&rsquo;s take OWASP.org, which has upwards of 12,000 links, as an example:
150 milliseconds x 12,000 links --------- 1,800,000 milliseconds (30 minutes) A whole half hour, just for the network tasks. It may even be much slower than that, since web pages are frequently much larger than a packet. This means that in our single-thread prototype web crawler, our biggest bottleneck is network latency. Why is this problematic?
Feedback loops I previously wrote about feedback loops. In essence, in order to improve at doing anything, you first need to be able to get feedback from your last attempt. That way, you have the necessary information to make adjustments and get closer to your goal on your next iteration.
As a software developer, bottlenecks can contribute to long and inefficient feedback loops. If I&rsquo;m waiting on a process that&rsquo;s part of a CI/CD pipeline, in our bottlenecked web crawler example, I&rsquo;d be sitting around for a minimum of a half hour before learning whether or not changes in my last push were successful, or whether they broke master (hopefully staging).
Multiply a slow and inefficient feedback loop by many runs per day, over many days, and you&rsquo;ve got a slow and inefficient developer. Multiply that by many developers in an organization bottlenecked on the same process, and you&rsquo;ve got a slow and inefficient company.
The cost of bottlenecks To add insult to injury, not only are you waiting on a bottlenecked process to run; you&rsquo;re also paying to wait. Take the serverless example - AWS Lambda, for instance. Here&rsquo;s a chart showing the cost of functions by compute time and CPU usage.
Source: Understanding and Controlling AWS Lambda Costs
Again, the numbers change over the years, but the main concepts remain the same: the bigger the function and the longer its compute time, the bigger the cost. For applications taking advantage of serverless, these costs can add up dramatically.
Bottlenecks are a recipe for failure, for both productivity and the bottom line.
The good news is that bottlenecks are mostly unnecessary. If we know how to identify them, we can strategize our way out of them. To understand how, let&rsquo;s get some tacos.
Tacos and threading Everyone, meet Bob. He&rsquo;s a gopher who works at the taco stand down the street as the cashier. Say &ldquo;Hi,&rdquo; Bob.
🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🌮 ╔══════════════╗ 🌮 Hi I&#39;m Bob 🌳 🌮 ╚══════════════╝ \\ 🌮 🐹 🌮 🌮 🌮 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 Bob works very hard at being a cashier, but he&rsquo;s still just one gopher. The customers who frequent Bob&rsquo;s taco stand can eat tacos really quickly; but in order to get the tacos to eat them, they&rsquo;ve got to order them through Bob. Here&rsquo;s what our bottlenecked, single-thread taco stand currently looks like:
🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🌮 🌮 🌳 🌮 🐹 🧑💵🧑💵🧑💵🧑💵🧑💵🧑💵🧑💵🧑💵🧑💵 🌮 🌮 🌮 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 As you can see, all the customers are queued up, right out the door. Poor Bob handles one customer&rsquo;s transaction at a time, starting and finishing with that customer completely before moving on to the next. Bob can only do so much, so our taco stand is rather inefficient at the moment. How can we make Bob faster?
We can try splitting the queue:
🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🌮 🧑💵🧑💵🧑💵🧑💵 🌮 🌳 🌮 🐹 🌮 🌮 🧑💵🧑💵🧑💵🧑💵🧑💵 🌮 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 Now Bob can do some multitasking. For example, he can start a transaction with a customer in one queue; then, while that customer counts their bills, Bob can pop over to the second queue and get started there. This arrangement, known as a concurrency model, helps Bob go a little bit faster by jumping back and forth between lines. However, it&rsquo;s still just one Bob, which limits our improvement possibilities. If we were to make four queues, they&rsquo;d all be shorter; but Bob would be very thinly stretched between them. Can we do better?
We could get two Bobs:
🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🌮 🌳 🌮 🐹 🧑💵🧑💵🧑💵🧑💵 🌮 🌳 🌮 🐹 🧑💵🧑💵🧑💵🧑💵🧑💵 🌮 🌳 🌮 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 With twice the Bobs, each can handle a queue of his own. This is our most efficient solution for our taco stand so far, since two Bobs can handle much more than one Bob can, even if each customer is still attended to one at a time.
We can do even better than that:
🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🐹 🧑💵🧑💵 🌮 🌳 🌮 🐹 🧑💵🧑💵 🌮 🌳 🌮 🐹 🧑💵🧑💵 🌮 🌳 🌮 🐹 🧑💵🧑💵🧑💵 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 With quadruple the Bobs, we have some very short queues, and a much more efficient taco stand. In computing, the concept of having multiple workers do tasks in parallel is called multithreading.
In Go, we can apply this concept using goroutines. Here are some illustrative snippets from my Go solution.
Setting up a Go web crawler In order to share data between our goroutines, we&rsquo;ll need to create some data structures. Our Checker structure will be shared, so it will have a Mutex (mutual exclusion) to allow our goroutines to lock and unlock it. The Checker structure will also hold a list of brokenLinks results, and visitedLinks. The latter will be a map of strings to booleans, which we&rsquo;ll use to directly and efficiently check for visited links. By using a map instead of iterating over a list, our visitedLinks lookup will have a constant complexity of O(1) as opposed to a linear complexity of O(n), thus avoiding the creation of another bottleneck. For more on time complexity, see my coffee-break introduction to time complexity of algorithms article.
type Checker struct { startDomain string brokenLinks []Result visitedLinks map[string]bool workerCount, maxWorkers int sync.Mutex } ... // Page allows us to retain parent and sublinks type Page struct { parent, loc, data string } // Result adds error information for the report type Result struct { Page reason string code int } To extract links from HTML data, here&rsquo;s a parser I wrote on top of package html:
// Extract links from HTML func parse(parent, data string) ([]string, []string) { doc, err := html.Parse(strings.NewReader(data)) if err != nil { fmt.Println(&#34;Could not parse: &#34;, err) } goodLinks := make([]string, 0) badLinks := make([]string, 0) var f func(*html.Node) f = func(n *html.Node) { if n.Type == html.ElementNode &amp;&amp; checkKey(string(n.Data)) { for _, a := range n.Attr { if checkAttr(string(a.Key)) { j, err := formatURL(parent, a.Val) if err != nil { badLinks = append(badLinks, j) } else { goodLinks = append(goodLinks, j) } break } } } for c := n.FirstChild; c != nil; c = c.NextSibling { f(c) } } f(doc) return goodLinks, badLinks } If you&rsquo;re wondering why I didn&rsquo;t use a more full-featured package for this project, I highly recommend the story of left-pad. The short of it: more dependencies, more problems.
Here are snippets of the main function, where we pass in our starting URL and create a queue (or channels, in Go) to be filled with links for our goroutines to process.
func main() { ... startURL := flag.String(&#34;url&#34;, &#34;http://example.com&#34;, &#34;full URL of site&#34;) ... firstPage := Page{ parent: *startURL, loc: *startURL, } toProcess := make(chan Page, 1) toProcess &lt;- firstPage var wg sync.WaitGroup The last significant piece of the puzzle is to create our workers, which we&rsquo;ll do here:
for i := range toProcess { wg.Add(1) checker.addWorker() 🐹 go worker(i, &amp;checker, &amp;wg, toProcess) if checker.workerCount &gt; checker.maxWorkers { time.Sleep(1 * time.Second) // throttle down } } wg.Wait() A WaitGroup does just what it says on the tin: it waits for our group of goroutines to finish. When they have, we&rsquo;ll know our Go web crawler has finished checking all the links on the site.
Did we do the thing fast Here&rsquo;s a comparison of the three programs I wrote on this journey. First, the prototype single-thread Python version:
time python3 slow-link-check.py https://victoria.dev real 17m34.084s user 11m40.761s sys 0m5.436s This finished crawling my website in about seventeen-and-a-half minutes, which is rather long for a site at least an order of magnitude smaller than OWASP.org.
The multithreaded Python version did a bit better:
time python3 hydra.py https://victoria.dev real 1m13.358s user 0m13.161s sys 0m2.826s My multithreaded Python program (which I dubbed Hydra) finished in one minute and thirteen seconds.
How did Go do?
time ./go-link-check --url=https://victoria.dev real 0m7.926s user 0m9.044s sys 0m0.932s At just under eight seconds, I found the Go version to be extremely palatable.
Breaking bottlenecks As fun as it is to simply enjoy the speedups, we can directly relate these results to everything we&rsquo;ve learned so far. Consider taking a process that used to soak up seventeen minutes and turning it into an eight-second-affair instead. Not only will that give developers a much shorter and more efficient feedback loop, it will give companies the ability to develop faster, and thus grow more quickly - while costing less. To drive the point home: a process that runs in seventeen-and-a-half minutes when it could take eight seconds will also cost over a hundred and thirty times as much to run!
A better work day for developers, and a better bottom line for companies. There&rsquo;s a lot of benefit to be had in making functions, code, and processes as efficient as possible - by breaking bottlenecks.
`,url:"https://victoria.dev/posts/breaking-bottlenecks/"},"https://victoria.dev/posts/command-line-tricks-for-managing-your-messy-open-source-repository/":{title:"Command line tricks for managing your messy open source repository",tags:["terminal","linux","open-source"],content:`Effective collaboration, especially in open source software development, starts with effective organization. To make sure that nothing gets missed, the general rule, &ldquo;one issue, one pull request&rdquo; is a nice rule of thumb.
Instead of opening an issue with a large scope like, &ldquo;Fix all the broken links in the documentation,&rdquo; open source projects will have more luck attracting contributors with several smaller and more manageable issues. In the preceding example, you might scope broken links by section or by page. This allows more contributors to jump in and dedicate small windows of their time, rather than waiting for one person to take on a larger and more tedious contribution effort.
Smaller scoped issues also help project maintainers see where work has been completed and where it hasn&rsquo;t. This reduces the chances that some part of the issue is missed, assumed to be completed, and later leads to bugs or security vulnerabilities.
That&rsquo;s all well and good; but what if you&rsquo;ve already opened several massively-scoped issues, some PRs have already been submitted or merged, and you currently have no idea where the work started or stopped?
It&rsquo;s going to take a little sorting out to get the state of your project back under control. Thankfully, there are a number of command line tools to help you scan, sort, and make sense of a messy repository. Here&rsquo;s a small selection of ones I use.
Jump to:
Interactive search-and-replace with vim Find dead links in Markdown files with a node module List subdirectories with or without a git repository with find Pull multiple git repositories from a list with xargs List issues by number with jot CLI-powered open source organization Interactive search-and-replace with vim You can open a file in Vim, then interactively search and replace with:
:%s/\\&lt;word\\&gt;/newword/gc The % indicates to look in all lines of the current file; s is for substitute; \\&lt;word\\&gt; matches the whole word; and the g for &ldquo;global&rdquo; is for every occurrence. The c at the end will let you view and confirm each change before it&rsquo;s made. You can run it automatically, and much faster, without c; however, you put yourself at risk of complicating things if you&rsquo;ve made a pattern-matching error.
Find dead links in Markdown files with a node module The markdown-link-check node module has a great CLI buddy.
I use this so often I turned it into a Bash alias function. To do the same, add this to your .bashrc:
# Markdown link check in a folder, recursive function mlc () { find $1 -name \\*.md -exec markdown-link-check -p {} \\; } Then run with mlc &lt;filename&gt;.
List subdirectories with or without a git repository with find Print all subdirectories that are git repositories, or in other words, have a .git in them:
find . -maxdepth 1 -type d -exec test -e &#39;{}/.git&#39; &#39;;&#39; -printf &#34;is git repo: %p\\n&#34; To print all subdirectories that are not git repositories, negate the test with !:
find . -maxdepth 1 -type d -exec test &#39;!&#39; -e &#39;{}/.git&#39; &#39;;&#39; -printf &#34;not git repo: %p\\n&#34; Pull multiple git repositories from a list with xargs I initially used this as part of automatically re-creating my laptop with Bash scripts, but it&rsquo;s pretty handy when you&rsquo;re working with cloud instances or Dockerfiles.
Given a file, repos.txt with a repository’s SSH link on each line (and your SSH keys set up), run:
xargs -n1 git clone &lt; repos.txt If you want to pull and push many repositories, I previously wrote about how to use a Bash one-liner to manage your repositories.
List issues by number with jot I&rsquo;m a co-author and maintainer for the OWASP Web Security Testing Guide repository where I recently took one large issue (yup, it was &ldquo;Fix all the broken links in the documentation&rdquo; - how&rsquo;d you guess?) and broke it up into several smaller, more manageable issues. A whole thirty-seven smaller, more manageable issues.
I wanted to enumerate all the issues that the original one became, but the idea of typing out thirty-seven issue numbers (#275 through #312) seemed awfully tedious and time-consuming. So, in natural programmer fashion, I spent the same amount of time I would have used to type out all those numbers and crafted a way to automate it instead.
The jot utility (apt install athena-jot) is a tiny tool that&rsquo;s a big help when you want to print out some numbers. Just tell it how many you want, and where to start and stop.
# jot [ reps [ begin [ end ] ] ] jot 37 275 312 This prints each number, inclusively, from 275 to 312 on a new line. To make these into issue number notations that GitHub and many other platforms automatically recognize and turn into links, you can pipe the output to awk.
jot 37 275 312 | awk &#39;{printf &#34;#&#34;$0&#34;, &#34;}&#39; #275, #276, #277, #278, #279, #280, #281, #282, #283, #284, #285, #286, #287, #288, #289, #290, #291, #292, #293, #295, #296, #297, #298, #299, #300, #301, #302, #303, #304, #305, #306, #307, #308, #309, #310, #311, #312 You can also use jot to generate random or redundant data, mainly for development or testing purposes.
CLI-powered open source organization A well-organized open source repository is a well-maintained open source project. Save this post for handy reference, and use your newfound CLI superpowers for good! 🚀
`,url:"https://victoria.dev/posts/command-line-tricks-for-managing-your-messy-open-source-repository/"},"https://victoria.dev/archive/why-pixelfed-wont-save-us-from-instagram/":{title:"Why PixelFed won't save us from Instagram",tags:["life"],content:`PixelFed is a decentralized photo sharing network based on the ActivityPub protocol, the same one that Mastodon uses. For a lot of people divorced (or wanting to be) from Instagram over mental health concerns and issues like forced consent to post-GDPR terms, a decentralized social network like PixelFed sounds like an exciting and promising alternative.
Personally, I stopped using Instagram once I accepted the fact that its core premise and integral structure of social interaction was encouraging me to form habits that were harmful to my life goals. I&rsquo;m not alone - studies have shown that people are happier after deleting apps like Facebook. The reasons for this don&rsquo;t differ greatly from why any social network can be bad for you - they&rsquo;re just found in much greater intensity on photo sharing sites, and specifically Instagram.
It is still early days for PixelFed. As I write this I have no way to know what kind of network it will become, or even if it will survive at all. I do know, however, that there are many glaring and fundamental problems that a decentralized photo sharing network like PixelFed won&rsquo;t solve. To elaborate, I&rsquo;m going to discuss what makes Instagram so poisonous to health, why centralized social networks aren&rsquo;t likely to ever be healthy, and why decentralized social networks have a very slim chance of being better.
Why Instagram is bad for your health Let&rsquo;s start with the basics. Your brain responds very differently to reading text than it does to looking at images.
It doesn&rsquo;t take more than a quick search to find hundreds of articles and studies about how reading can make you smarter, more empathetic, and stave off cognitive decline by improving brain connectivity. In essence, reading involves a multitude of brain regions including the temporal and frontal lobes. There&rsquo;s still a lot to be discovered about the human brain, but here&rsquo;s what we think we know. The frontal lobes control important cognitive skills like emotional expression, problem solving, memory, language, judgement, and sexual behaviors. The temporal lobes handle important functions such as the encoding of memory, and processing emotions and visual perception.
In other words, reading text - even on social media - stimulates your brain and makes you think about the information you&rsquo;re taking in. To react to words on a page, you first have to read them and form thoughts about them.
Unlike reading, looking at an image has a very different effect on your brain. Here&rsquo;s an infographic about infographics that covers some of these effects. Basically, millennia of evolution have produced human brains hardwired to respond quickly to visual stimuli - in less than 1/10 of a second. As the infographic will literally show you, almost 50% of the brain is involved in visual processing, and 70% of all our sensory receptors are in our eyes. That&rsquo;s a lot of resources devoted to quickly processing visuals. Why could this be bad?
Unlike times past, we&rsquo;re no longer (day-to-day) concerned with spotting a tiger in the bushes about to pounce on us. The near-instant processing time needed to discern if that shivering tree branch is the just wind or impending mortal danger is outdated in our current living arrangements. Our brain, however, doesn&rsquo;t know that. It hasn&rsquo;t evolved faster than our technologies or society. The downside to this is that anyone with a little knowledge of this fundamental flaw in the human mind is able to exploit it.
Advertisers call this exploitation, &ldquo;visual marketing.&rdquo;
These linked articles are stuffed with the same factoids over and over again. &ldquo;The brain processes images 60,000 times faster than text.&rdquo; &ldquo;90% of the information sent to the brain is visual.&rdquo; Whether or not these numbers are accurate, it&rsquo;s clearly provable that visual marketing is on the whole more effective than advertisements without images. There&rsquo;s a reason for it, and it should scare you.
Unlike reading, which involves regions of your brain responsible for comprehension, decision making, and emotional control, images are processed by different areas of the brain. Visual input travels from our eyes through our optic nerves to the thalamus (or LGN, Lateral Geniculate Nucleus) and the superior colliculus. From the thalamus, it proceeds to the visual cortex at the rear of our brains, where the image is processed. Effectively, viewing images does not make us think in the same way that reading does. In other words, it&rsquo;s easy to do.
Let me be clear. This difference in the way words and images are processed is not, in itself, bad. A photo-centric social network is not, in itself, bad. Images and words alike have the power to evoke strong emotions, send powerful messages, spark revolutions, and spur progress. This is good&hellip; if it&rsquo;s used for good.
Instagram, a photo-centric network chock full of product placements, paid sponsorships, and outright advertisements, is a social network primarily designed to bypass your cognitive thinking and sell you stuff.
I don&rsquo;t think Instagram started out with the same motivations it has now. Along with all the photo sharing networks that blossomed when Instagram first got popular, I still believe its initial vision was to make sharing photos with your friends fun and easy.
It just got too popular.
Why centralized networks are bad for your health In the wake of privacy concerns over the last few years, new uproar over algorithm-driven timelines, and the #DeleteFacebook, #DeleteTwitter, and #DeleteInstagram movements, more people today are aware of how networks that make their money on your data are bad for your health. This is in part due to their centralized nature - one hierarchy of authority makes decisions for the whole system, and at the same time, has to support it. It&rsquo;s expensive to support millions of users, so it&rsquo;s no wonder that the network&rsquo;s main concern (and let&rsquo;s just consider the most innocent case) is to remain profitable.
What&rsquo;s a good way to remain profitable?
Take a human desire, preferably one that has been around for a really long time&hellip; Identify that desire and use modern technology to take out steps. &ndash; Evan Williams, co-founder of Twitter and Blogger
Quoted in Wired article, 2013, &ldquo;Twitter Founder Reveals Secret Formula for Getting Rich Online&rdquo;
There&rsquo;s a book called Hooked: How to Build Habit-Forming Products which, if you&rsquo;re ever in the mood for a good horror flick, you should curl up in bed with some popcorn and read.
The book details a simple model for a habit-forming product. The model is cyclical, and has the following key points: a trigger, an action, variable reward, and investment. In summary, if a product can get you to think of it, leading to some action that is easier to do than to think about, give you a reward for that action some of the time, and then compel you to commit or invest in it - you&rsquo;re hooked.
If you&rsquo;re paying attention, you might notice I&rsquo;ve described Instagram. And Twitter. And Facebook. And every other social network.
There&rsquo;s a reason it&rsquo;s easy to use Instagram, easy to post a tweet, easy to browse Facebook. These products have been designed to make it easy for you to use them. They&rsquo;ve been designed to alter your behavior to better suit the product&rsquo;s goals.
This industry employs some of the smartest people, thousands of Ph.D. designers, statisticians, engineers. They go to work every day to get us to do this one thing, to undermine our willpower. &ndash; James Williams, co-founder of Time Well Spent
Quoted in Nautilus article, 2017, &ldquo;Modern Media Is a DoS Attack on Your Free Will&rdquo;
At the heart of the idea of getting you hooked is the concept of a dopamine feedback loop. Dopamine, an organic chemical neurotransmitter in your brain, is thought to be responsible for allowing us to anticipate the reward to an action. It inspires us to get a glass of water when we&rsquo;re thirsty, for example, and may help us to feel good when we take actions towards doing so. Where dopamine is so effectively misused is in the practice of providing variable rewards to drive social media addiction.
Unlike getting a glass of water when you&rsquo;re thirsty, variable rewards are random. It&rsquo;s as if drinking water sometimes, but not always, cured your thirst. This effectively programs your mind to pursue the action that results in the unpredictable reward. Since getting the reward isn&rsquo;t guaranteed, you need to make more attempts to achieve success. Social media is designed to make these variable dopamine hits easy to obtain. It&rsquo;s designed to hijack your intellectual independence in order to keep you on the network.
Especially when the main goal of a centralized social network is to make a profit, that network is exploiting evolutionary flaws in your brain to make that profit from you. You are literally being hacked.
Now combine this information with the knowledge of how a product comprised primarily of images bypasses your cognitive thinking. Not only are you being hacked, but your main defense system is being easily, laughably, circumvented.
Exploiting users is a particularly compelling temptation for any social networks under pressure to make a profit, and this pressure is amplified in organizations with a centralized structure. Not all centralized networks do this, but undoubtedly, the very successful ones do.
Decentralization is by no means a fix for exploitation and greed, but a decentralized social network might have a few things going for it.
Why decentralized networks might be slightly better for your health The main issues present in Twitter, Facebook, and Instagram as pertains to social media addiction do not go away on decentralized networks. I&rsquo;m personally, currently, using both Twitter and Mastodon. The former is centralized and the latter is decentralized, but the the same motivations that could get me in trouble on one platform apply to both. Decentralization does not fix the problem.
It might help.
Unlike a centralized, single-hierarchy, definitely-for-profit social network, decentralization has one thing going for it: more people. Specifically, more instance owners who are in control of their instances.
Running a Mastodon instance is a responsibility, should you choose to accept it. Besides the server itself, instances require their own sets of rules and code of conduct, and like the often adopted mastodon.social code of conduct, it can be collaboratively drafted by the community. Mastodon provides instance owners with moderation tools and provides users with reporting tools, and there&rsquo;s an expectation that they&rsquo;ll both be used. As with other decentralized social networks, it is the responsibility of the instance owner to moderate and foster a social environment that serves the best interests of the instance users.
Instances typically run on donations, and in the grand scheme of things, are inexpensive to support. Decentralization means that instance owners individually have to bear smaller costs. There&rsquo;s no central body being pressured to make a profit in order to run servers that support millions of users. The effect of this many-owners structure is that decisions that concern any particular instance and rules that it might want to adopt are made by that instance&rsquo;s community, or the instance owner. If a user disagrees with the direction taken, they can communicate directly with the instance owner, or simply move to another instance. There&rsquo;s no &ldquo;take it or leave it,&rdquo; and no forced acceptance of terms. Users always have somewhere else to go.
This, in general, means that over many instances, and via many moderators, more people from diverse backgrounds with a collection of both overlapping and contrasting interests are able to have a voice in how the social network evolves.
If instance owners have their users&rsquo; best interests, not addiction, in mind; if moderators act responsibly, and according to their instance rules, moderate for good; and if a wide and varying selection of instances with differing interests, political viewpoints, and topics continue to be available; then decentralized social networks might be better for your health.
Where this leaves PixelFed (and all social networks) All social networks have the potential to do more good than harm, but it is up to those who control them to put in the constant, proactive effort required to make that happen. Twitter has recently been making some steps towards becoming a healthier network, like banning political ads and highlighting manipulated media. I think they&rsquo;re ahead of the curve. With decentralized social networks, there&rsquo;s at least more chances for the possibility that instance owners truly want to do more good than harm with their own little piece of the whole.
While photo sharing networks will, by their essential nature, bypass cognitive thinking and have an advantage over their users that way, there are many design considerations that PixelFed can implement in order to make the network healthier. Features such as comments, likes, timelines, and push notifications can be designed to provide utility more than drive addiction, and there are designers more qualified than I who can tell you how.
These networks will have to constantly resist the temptation to take the easy route. They will have to work to avoid success based on the exploitation of their users&rsquo; desires to chase the easy dopamine hit. They will have to prioritize the ability of the social network to add real value to the lives of its users - at the expense of its own potential to garner mindless, meaningless popularity.
This is in no way a condemnation of PixelFed or any other decentralized photo sharing network. Personally, I sincerely hope they succeed in giving users a healthy, safe, and free-as-in-freedom network for sharing photos with friends, and with the rest of the federated community. It will require considered design with mental health at the forefront; the active, caring effort of moderators and instance owners; and ongoing collaboration from the federated community at large to work together to build for the greater good.
A photo-sharing social media network that does more good than harm? It&rsquo;s possible. But it won&rsquo;t be easy.
`,url:"https://victoria.dev/archive/why-pixelfed-wont-save-us-from-instagram/"},"https://victoria.dev/posts/the-past-ten-years-or-how-to-get-better-at-anything/":{title:"The past ten years, or, how to get better at anything",tags:["life","docs","coding"],content:`If you want to get better at anything:
Solve your own problems, Write about it, Teach others. 1. Searching, a decade ago I was a young graduate with newly-minted freedoms, and I was about to fall in love. I had plenty of imagination, a couple handfuls of tenacity, and no sense of direction at all.
For much of my youth, when I encountered a problem, I just sort of bumped up against it. I tried using whatever was in my head from past experiences or my own imagination to find a solution. For some problems, like managing staff duties at work, my experience was sufficient guidance. For other, more complicated problems, it wasn&rsquo;t.
When you don&rsquo;t have a wealth of experience to draw upon, relying on it is a poor strategy. Like many people at my age then, I thought I knew enough. Like many people at my age now, I recognize how insufficient &ldquo;enough&rdquo; can be. A lack of self-directed momentum meant being dragged in any direction life&rsquo;s currents took me. When falling in love turned out to mean falling from a far greater height than I had anticipated, I tumbled on, complacent. When higher-ups at work handed me further responsibilities, I accepted them without considering if I wanted them at all. When, inevitably, life became more and more complicated, I encountered even more problems I didn&rsquo;t know how to solve. I felt stuck.
Though I was morbidly embarrassed about it at the time, I&rsquo;m not shy to say it now. At one point, it had to be pointed out to me that I could search the Internet for the solution to any of my problems. Anything I wanted to solve - interactions with people at work, a floundering relationship, or the practicalities of filing taxes - I was lucky enough to have the greatest collection of human knowledge ever assembled at my disposal.
Instead of bumbling along in the floatsam of my own trial and error, I started to take advantage of the collective experiences of all those who have been here before me. They weren&rsquo;t always right, and I often found information only somewhat similar to my own experience. Still, it always got me moving in the right direction. Eventually, I started to steer.
There&rsquo;s a learning curve, even when just searching for a problem. Distilling the jumble of confusion in your head to the right search terms is a learned skill. It helped me to understand how search engines like Google work:
We use software known as web crawlers to discover publicly available webpages. Crawlers look at webpages and follow links on those pages, much like you would if you were browsing content on the web. They go from link to link and bring data about those webpages back to Google’s servers&hellip;
When crawlers find a webpage, our systems render the content of the page, just as a browser does. We take note of key signals — from keywords to website freshness — and we keep track of it all in the Search index.
Sometimes, I find what I need by using the right keyword. Other times, I discover the keyword by searching for text that might surround it on the content of the page. For software development, I search for the weirdest word or combination of words attached to what I&rsquo;m trying to learn. I rarely find whole solutions in my search results, but I always find direction for solving the problem myself.
Solving my own problems, even just a few little ones at a time, gave me confidence and built momentum. I began to pursue the experiences I wanted, instead of waiting for experiences to happen to me.
2. Updating the Internet, some years ago I&rsquo;d solved myself out of a doomed relationship and stagnant job. I found myself, rather gleefully, country-hopping with just one backpack of possessions. I met, though I didn&rsquo;t know it at the time, my future husband. I found a new sense of freedom, of having options, that I knew I never wanted to give up. I had to find a means to sustain myself by working remotely.
When I first tried to make a living on the Internet, I felt like a right amateur. Sitting on the bed, hunched over my laptop, I started a crappy Wordpress blog with a modified theme that didn&rsquo;t entirely work. I posted about how I tried and failed to start a dropshipping business. My site was terrible, and I knew it. My first forays into being a &ldquo;real&rdquo; developer were to solve my own problems: how to get my blog working, how to set up a custom domain, how to get and use a security certificate. I found some guidance in blogs and answers that others had written, but much of it was outdated, or not entirely correct. Still, it helped me.
I can&rsquo;t imagine a world in which people did nothing to pass on their knowledge to future generations. Our stories are all we have beyond instinct and determination.
I stopped posting about dropshipping and started writing about the technical problems I was solving. I wrote about what I tried, and ultimately what worked. I started hearing from people who thanked me for explaining the solution they were looking for. Even in posts where all I&rsquo;d done was link to the correct set of instructions on some other website, people thanked me for leading them to it. I still thought my website was terrible, but I realized I was doing something useful. The more problems I solved, the better I got at solving them, and the more I wrote about it in turn.
One day, someone offered me money for one of my solutions. To my great delight, they weren&rsquo;t the last to do so.
As I built up my skills, I started taking on more challenging offers to solve problems. I discovered, as others have before me, that especially in software development, not every solution is out there waiting for you. The most frustrating part of working on an unsolved problem is that, at least to your knowledge, there&rsquo;s no one about to tell you how to solve it. If you&rsquo;re lucky, you&rsquo;ve at least got a heading from someone&rsquo;s cold trail in an old blog post. If you&rsquo;re lucky and tenacious, you&rsquo;ll find a working solution.
Don&rsquo;t leave it scribbled in the corner of a soon-forgotten notepad, never to ease the path of someone who comes along later. Update that old blog post by commenting on it, or sending a note to the author. Put your solution on the Internet, somewhere. Ideally, blog about it yourself in as much detail as you can recall. Some of the people who find your post might have the same problem, and might even be willing to pay you to solve it. And, if my own experience and some scattered stories hold true, one of the people to who&rsquo;ll come along later, looking for that same solution, will be you.
3. Paying it forwards, backwards, and investing; two years ago Already being familiar with how easy it is to stop steering and start drifting, I sought new ways to challenge myself and my skills. I wanted to do more than just sustain my lifestyle. I wanted to offer something to others; something that mattered.
A strange thing started happening when I decided, deliberately, to write an in-depth technical blog about topics I was only beginning to become familiar with. I started to deeply understand some fundamental computer science topics - and trust me, that was strange enough - but odder than that was that others started to see me as a resource. People asked me questions because they thought I had the answers. I didn&rsquo;t, at least, not always - but I knew enough now to not let that stop me. I went to find the answers, to test and understand them, and then I wrote about them to teach those who had asked. I hardly noticed, along the way, that I was learning too.
When someone&rsquo;s outdated blog post leads you to an eventual solution, you can pay them back by posting an update, or blogging about it yourself. When you solve an unsolved problem, you pay it forward by recording that solution for the next person who comes along (sometimes you). In either case, by writing about it - honestly, and with your best effort to be thorough and correct - you end up investing in yourself.
Explaining topics you&rsquo;re interested in to other people helps you find the missing pieces in your own knowledge. It helps you fill those gaps with learning, and integrate the things you learn into a new, greater understanding. Teaching something to others helps you become better at it yourself. Getting better at something - anything - means you have more to offer.
The past decade, and the next decade It&rsquo;s the end of a decade. I went from an aimless drift through life to being captain of my ship. I bettered my environment, learned new skills, made myself a resource, and became a wife to my best friend. I&rsquo;m pretty happy with all of it.
It&rsquo;s the end of 2019. Despite a whole lot of life happening just this year, I&rsquo;ve written one article on this blog for each week since I started in July. That&rsquo;s 23 articles for 23 weeks, plus one Christmas bonus. I hear from people almost every day who tell me that an article I wrote was helpful to them, and it makes me happy and proud to think that I&rsquo;ve been doing something that matters. The first week of January will make this blog two years old.
The past several months have seen me change tack, slightly. I&rsquo;ve become very interested in cybersecurity, and have been lending my skills to the Open Web Application Security Project. I&rsquo;m now an author and maintainer of the Web Security Testing Guide, version 5. I&rsquo;m pretty happy with that, too.
Next year, I&rsquo;ll be posting a little less, though writing even more, as I pursue an old dream of publishing a book, as well as develop my new cybersecurity interests. I aim to get better at quite a few things. Thankfully, I know just how to do it - and now, so do you:
Solve your own problems, Write about it, Teach others. Have a very happy new decade, dear reader.
`,url:"https://victoria.dev/posts/the-past-ten-years-or-how-to-get-better-at-anything/"},"https://victoria.dev/archive/three-healthy-cybersecurity-habits/":{title:"Three healthy cybersecurity habits",tags:["cybersecurity","privacy","life"],content:`In a similar fashion to everyone getting the flu now and again, the risk of catching a cyberattack is a common one. Both a sophisticated social engineering attack or grammatically-lacking email phishing scam can cause real damage. No one who communicates over the Internet is immune.
Like proper hand washing and getting a flu shot, good habits can lower your risk of inadvertently allowing cybergerms to spread. Since the new year is an inspiring time for beginning new habits, I offer a few suggestions for ways to help protect yourself and those around you.
1. Get a follow-up Recognizing a delivery method for cyberattack is getting more difficult. Messages with malicious links do not always come from strangers. They may appear to be routine communications, or seem to originate from someone you know or work with. Attacks use subtle but deeply-engrained cognitive biases to override your common sense. Your natural response ensures you click.
Thankfully, there&rsquo;s a simple low-tech habit you can use to deter these attacks: before you act, follow-up.
You may get an email from a friend that needs help, or from your boss who&rsquo;s about to get on a plane. It could be as enticing and mysterious as a direct message from an acquaintance who sends a link asking, &ldquo;Lol. Is this you?&rdquo; It takes presence of mind to override the panic these attacks prey on, but the deterrent itself is quick and straightforward. Send a text message, pick up the phone and call, or walk down the hall, and ask, &ldquo;Did you send me this?&rdquo;
If the message is genuine, there&rsquo;s no harm in a few extra minutes to double check. If it&rsquo;s not, you&rsquo;ll immediately alert the originating party that they may be compromised, and you may have deterred a cyberattack!
2. Use, and encourage others to use, end-to-end encrypted messaging When individuals in a neighborhood get the flu shot, others in that neighborhood are safer for it. Encryption is similarly beneficial. Encourage your friends, coworkers, and Aunt Matilda to switch to an app like Signal. By doing so, you&rsquo;ll reduce everyone&rsquo;s exposure to more exploitable messaging systems.
This doesn&rsquo;t mean that you must stop using other methods of communication entirely. Instead, think of it as a hierarchy. Use Signal for important messages that should be trusted, like requests for money or making travel arrangements. Use all other methods of messaging, like SMS or social sites, only for &ldquo;unimportant&rdquo; communications. Now, if requests or links that seem important come to you through your unimportant methods, you&rsquo;ll be all the more likely to second-guess them.
3. Don&rsquo;t put that dirty USB plug into your *** You wouldn&rsquo;t brush your teeth with a toothbrush you found on the sidewalk. Why would you plug in a USB device if you don&rsquo;t know where it&rsquo;s been?! While we might ascribe putting a random found USB drive in your computer to a clever exploitation of natural human curiosity, we&rsquo;re no sooner likely to suspect using a public phone-charging station or a USB cable we bought ourselves. Even seemingly-innocuous USB peripherals or rechargeable devices can be a risk.
Unlike email and some file-sharing services that scan and filter files before they reach your computer, plugging in via USB is as direct and unprotected as connection gets. Once this connection is made, the user doesn&rsquo;t need to do anything else for a whole host of bad things to happen. Through USB connections, problems like malware and ransomware can easily infect your computer or phone.
There&rsquo;s no need to swear off the convenience of USB connectivity, or to avoid these devices altogether. Instead of engaging in questionable USB behavior, don&rsquo;t cheap out on USB devices and cables. If it&rsquo;s going to get plugged into your computer, ensure you&rsquo;re being extra cautious. Buy it from the manufacturer (like the Apple Store) or from a reputable company or reseller with supply chain control. When juicing up USB-rechargeables, don&rsquo;t plug them into your computer. Use a wall charger with a USB port instead.
Practice healthy cybersecurity habits Keeping your devices healthy and happy is a matter of practicing good habits. Like battling the flu, good habits can help protect yourself and those around you. Incorporate some conscientious cybersecurity practices in your new year resolutions - or start them right away.
Have a safe and happy holiday!
`,url:"https://victoria.dev/archive/three-healthy-cybersecurity-habits/"},"https://victoria.dev/archive/concurrency-parallelism-and-the-many-threads-of-santa-claus/":{title:"Concurrency, parallelism, and the many threads of Santa Claus 🎅",tags:["computing"],content:`Consider the following: Santa brings toys to all the good girls and boys.
There are 7,713,468,100 people in the world in 2019, around 26.3% of which are under 15 years old. This works out to 2,028,642,110 children (persons under 15 years of age) in the world this year.
Santa doesn&rsquo;t seem to visit children of every religion, so we&rsquo;ll generalize and only include Christians and non-religious folks. Collectively that makes up approximately 44.72% of the population. If we assume that all kids take after their parents, then 907,208,751.6 children would appear to be Santa-eligible.
What percentage of those children are good? It&rsquo;s impossible to know; however, we can work on a few assumptions. One is that Santa Claus functions more on optimism than economics and would likely have prepared for the possibility that every child is a good child in any given year. Thus, he would be prepared to give a toy to every child. Let&rsquo;s assume it&rsquo;s been a great year and that all 907,208,751.6 children are getting toys.
That&rsquo;s a lot of presents, and, as we know, they&rsquo;re all made by Santa&rsquo;s elves at his North China Pole workshop. Given that there are 365 days in a year and one of them is Christmas, let&rsquo;s assume that Santa&rsquo;s elves collectively have 364 days to create and gift wrap 907,208,752 (rounded up) presents. That works out to 2,492,331.74 presents per day.
Almost two-and-a-half million presents per day is a heavy workload for any workshop. Let&rsquo;s look at two paradigms that Santa might employ to hit this goal: concurrency, and parallelism.
A sequential process Suppose that Santa&rsquo;s workshop is staffed by exactly one, very hard working, very tired elf. The production of one present involves four steps:
Cutting wood Assembly and glueing Painting Gift-wrapping With a single elf, only one step for one present can be happening at any instance in time. If the elf were to produce one present at a time from beginning to end, that process would be executed sequentially. It&rsquo;s not the most efficient method for producing two-and-a-half million presents per day; for instance, the elf would have to wait around doing nothing while the glue on the present was drying before moving on to the next step.
Concurrency In order to be more efficient, the elf works on all presents concurrently.
Instead of completing one present at a time, the elf first cuts all the wood for all the toys, one by one. When everything is cut, the elf assembles and glues the toys together, one after the other. This concurrent processing means that the glue from the first toy has time to dry (without needing more attention from the elf) while the remaining toys are glued together. The same goes for painting, one toy at a time, and finally wrapping.
Since one elf can only do one task at a time, a single elf is using the day as efficiently as possible by concurrently producing presents.
Parallelism Hopefully, Santa&rsquo;s workshop has more than just one elf. With more elves, more toys can be built simultaneously over the course of a day. This simultaneous work means that the presents are being produced in parallel. Parallel processing carried out by multiple elves means more work happens at the same time.
Elves working in parallel can also employ concurrency. One elf can still tackle only one task at a time, so it&rsquo;s most efficient to have multiple elves concurrently producing presents.
Of course, if Santa&rsquo;s workshop has, say, two-and-a-half million elves, each elf would only need to finish a maximum of one present per day. In this case, working sequentially doesn&rsquo;t detract from the workshop&rsquo;s efficiency. There would still be 7,668.26 elves left over to fetch coffee and lunch.
Santa Claus, and threading After all the elves&rsquo; hard work is done, it&rsquo;s up to Santa Claus to deliver the presents &ndash; all 907,208,752 of them.
Santa doesn&rsquo;t need to make a visit to every kid; just to the one household tree. So how many trees does Santa need to visit? Again with broad generalization, we&rsquo;ll say that the average number of children per household worldwide is 2.45, based on the year&rsquo;s predicted fertility rates. That makes 370,289,286.4 houses to visit. Let&rsquo;s round that up to 370,289,287.
How long does Santa have? The lore says one night, which means one earthly rotation, and thus 24 hours. NORAD confirms.
This means Santa must visit 370,289,287 households in 24 hours (86,400 seconds), at a rate of 4,285.75 households per second, nevermind the time it takes to put presents under the tree and grab a cookie.
Clearly, Santa doesn&rsquo;t exist in our dimension. This is especially likely given that despite being chubby and plump, he fits down a chimney (with a lit fire, while remaining unhurt) carrying a sack of toys containing presents for all the household&rsquo;s children. We haven&rsquo;t even considered the fact that his sleigh carries enough toys for every believing boy and girl around the world, and flies.
Does Santa exist outside our rules of physics? How could one entity manage to travel around the world, delivering packages, in under 24 hours at a rate of 4,285.75 households per second, and still have time for milk and cookies and kissing mommy?
One thing is certain: Santa uses the Internet. No other technology has yet enabled packages to travel quite so far and quite so quickly. Even so, attempting to reach upwards of four thousand households per second is no small task, even with even the best gigabit Internet hookup the North Pole has to offer. How might Santa increase his efficiency?
There&rsquo;s clearly only one logical conclusion to this mystery: Santa Claus is a multithreaded process.
A single thread Let&rsquo;s work outward. Think of a thread as one particular task, or the most granular sequence of instructions that Santa might execute. One thread might execute the task, put present under tree. A thread is a component of a process, in this case, Santa&rsquo;s process of delivering presents.
If Santa Claus is single-threaded, he, as a process, would only be able to accomplish one task at a time. Since he&rsquo;s old and a bit forgetful, he probably has a set of instructions for delivering presents, as well as a schedule to abide by. These two things guide Santa&rsquo;s thread until his process is complete.
Single-threaded Santa Claus might work something like this:
Land sleigh at Timmy&rsquo;s house Get Timmy&rsquo;s present from sleigh Enter house via chimney Locate Christmas tree Place Timmy&rsquo;s present under Christmas tree Exit house via chimney Take off in sleigh Rinse and repeat&hellip; another 370,289,286 times.
Multithreading Multithreaded Santa Claus, by contrast, is the Doctor Manhattan of the North Pole. There&rsquo;s still only one Santa Claus in the world; however, he has the amazing ability to multiply his consciousness and accomplish multiple instruction sets of tasks simultaneously. These additional task workers, or worker threads, are created and controlled by the main process of Santa delivering presents.
Each worker thread acts independently to complete its instructions. Since they all belong to Santa&rsquo;s consciousness, they share Santa&rsquo;s memory and know everything that Santa knows, including what planet they&rsquo;re running around on, and where to get the presents from.
With this shared knowledge, each thread is able to execute its set of instructions in parallel with the other threads. This multithreaded parallelism makes the one and only Santa Claus as efficient as possible.
If an average present delivery run takes an hour, Santa need only spawn 4,286 worker threads. With each making one delivery trip per hour, Santa will have completed all 370,289,287 trips by the end of the night.
Of course, in theory, Santa could even spawn 370,289,287 worker threads, each flying to one household to deliver presents for all the children in it! That would make Santa&rsquo;s process extremely efficient, and also explain how he manages to consume all those milk-dunked cookies without getting full. 🥛🍪🍪🍪
An efficient and merry multithreaded Christmas Thanks to modern computing, we now finally understand how Santa Claus manages the seemingly-impossible task of delivering toys to good girls and boys the world-over. From my family to yours, I hope you have a wonderful Christmas. Don&rsquo;t forget to hang up your stockings on the router shelf.
Of course, none of this explains how reindeer manage to fly.
`,url:"https://victoria.dev/archive/concurrency-parallelism-and-the-many-threads-of-santa-claus/"},"https://victoria.dev/posts/word-bugs-in-software-documentation-and-how-to-fix-them/":{title:"Word bugs in software documentation and how to fix them",tags:["docs","open source"],content:`I&rsquo;ve been an editor longer than I&rsquo;ve been a developer, so this topic for me is a real root issue. 🥁 When I see a great project with poorly-written docs, it hits close to /home. Okay, okay, I&rsquo;m done.
I help the Open Web Application Security Project (OWASP) with their Web Security Testing Guide (WSTG). I was recently tasked with writing a style guide and article template that show how to write technical instruction for testing software applications.
I thought parts of the guide would benefit more people than just OWASP&rsquo;s contributors, so I&rsquo;m sharing some here.
Many of the projects I participate in are open source. This is a wonderful way for people to share solutions and to build on each others&rsquo; ideas. Unfortunately, it&rsquo;s also a great way for misused and non-existent words to catch on. Here&rsquo;s an excerpt of the guide with some mistakes I&rsquo;ve noticed and how you can fix them in your technical documents.
Use Correct Words The following are frequently misused words and how to correct them.
and/or While sometimes used in legal documents, and/or leads to ambiguity and confusion in technical writing. Instead, use or, which in the English language includes and. For example:
Bad: &ldquo;The code will output an error number and/or description.&rdquo; Good: &ldquo;The code will output an error number or description.&rdquo;
The latter sentence does not exclude the possibility of having both an error number and description.
If you need to specify all possible outcomes, use a list:
&ldquo;The code will output an error number, or a description, or both.&rdquo;
frontend, backend While it&rsquo;s true that the English language evolves over time, these are not yet words.
When referring to nouns, use front end and back end. For example:
Security is equally important on the front end as it is on the back end.
As a descriptive adverb, use the hyphenated front-end and back-end.
Both front-end developers and back-end developers are responsible for application security.
whitebox, blackbox, greybox These are not words.
As nouns, use white box, black box, and grey box. These nouns rarely appear in connection with cybersecurity.
My cat enjoys jumping into that grey box.
As adverbs, use the hyphenated white-box, black-box, and grey-box. Do not use capitalization unless the words are in a title.
While white-box testing involves knowledge of source code, black-box testing does not. A grey-box test is somewhere in-between.
ie, eg These are letters.
The abbreviation i.e. refers to the Latin id est, which means &ldquo;in other words.&rdquo; The abbreviation e.g. is for exempli gratia, translating to &ldquo;for example.&rdquo; To use these in a sentence:
Write using proper English, i.e. correct spelling and grammar. Use common words over uncommon ones, e.g. &ldquo;learn&rdquo; instead of &ldquo;glean.&rdquo;
etc These are also letters.
The Latin phrase et cetera translates to &ldquo;and the rest.&rdquo; It is abbreviated etc. and typically placed at the end of a list that seems redundant to complete:
WSTG authors like rainbow colors, such as red, yellow, green, etc.
In technical writing, the use of etc. is problematic. It assumes the reader knows what you&rsquo;re talking about, and they may not. Violet is one of the colors of the rainbow, but the example above does not explicitly tell you if violet is a color that WSTG authors like.
It is better to be explicit and thorough than to make assumptions of the reader. Only use etc. to avoid completing a list that was given in full earlier in the document.
&hellip; (ellipsis) The ellipsis punctuation mark can indicate that words have been left out of a quote:
Linus Torvalds once said, &ldquo;Once you realize that documentation should be laughed at&hellip; THEN, and only then, have you reached the level where you can safely read it and try to use it to actually implement a driver.&rdquo;
As long as the omission does not change the meaning of the quote, this is acceptable usage of ellipsis in the WSTG.
All other uses of ellipsis, such as to indicate an unfinished thought, are not.
ex While this is a word, it is likely not the word you are looking for. The word ex has particular meaning in the fields of finance and commerce, and may refer to a person if you are discussing your past relationships. None of these topics should appear in the WSTG.
The abbreviation ex. may be used to mean &ldquo;example&rdquo; by lazy writers. Please don&rsquo;t be lazy, and write example instead.
Go forth and write docs If these reminders are helpful, please share them freely and use them when writing your own READMEs and documentation! If there&rsquo;s some I&rsquo;ve missed, I&rsquo;d love to know.
And if you&rsquo;re here for the comments&hellip;
There are none on my blog. You can still @ me.
If you&rsquo;d like to help contribute to the OWASP WSTG, please read the contribution guide. See the full style guide here.
`,url:"https://victoria.dev/posts/word-bugs-in-software-documentation-and-how-to-fix-them/"},"https://victoria.dev/archive/secure-web-forms-for-the-front-end-developer/":{title:"Secure web forms for the front-end developer",tags:["cybersecurity","coding","leadership"],content:`While cybersecurity is often thought of in terms of databases and architecture, much of a strong security posture relies on elements in the domain of the front-end developer. For certain potentially devastating vulnerabilities like SQL injection and Cross-Site Scripting (XSS), a well-considered user interface is the first line of defense.
Here are a few areas of focus for front-end developers who want to help fight the good fight.
Control user input A whole whack of crazy things can happen when developers build a form that fails to control user input. To combat vulnerabilities like injection, it&rsquo;s important to validate or sanitize user input.
Input can be validated by constraining it to known values, such as by using semantic input types or validation-related attributes in forms. Frameworks like Django also help by providing field types for this purpose. Sanitizing data can be done by removing or replacing contextually-dangerous characters, such as by using a whitelist or escaping the input data.
While it may not be intuitive, even data that a user submits to their own area on a site should be validated. One of the fastest viruses to proliferate was the Samy worm on MySpace (yes, I&rsquo;m old), thanks to code that Samy Kamkar was able to inject into his own profile page. Don&rsquo;t directly return any input to your site without thorough validation or santization.
For some further guidance on battling injection attacks, see the OWASP Injection Prevention Cheat Sheet.
Beware hidden fields Adding type=&quot;hidden&quot; is an enticingly convenient way to hide sensitive data in pages and forms, but unfortunately not an effective one. With tools like ZapProxy and even inspection tools in plain ol&rsquo; web browsers, users can easily click to reveal tasty bits of invisible information. Hiding checkboxes can be a neat hack for creating CSS-only switches, but hidden fields do little to contribute to security.
Carefully consider autofill fields When a user chooses to give you their Personally Identifiable Information (PII), it should be a conscious choice. Autofill form fields can be convenient - for both users and attackers. Exploits using hidden fields can harvest PII previously captured by an autocomplete field.
Many users aren&rsquo;t even aware what information their browser&rsquo;s autofill has stored up. Use these fields sparingly, and disable autofilled forms for particularly sensitive data.
It&rsquo;s important to also weigh your risk profile against its trade-offs. If your project must be WCAG compliant, disabling autocomplete can break your input for different modalities. For more, see 1.3.5: Identify Input Purpose in WCAG 2.1.
Keep errors generic While it may seem helpful to let users know whether a piece of data exists, it&rsquo;s also very helpful to attackers. When dealing with accounts, emails, and PII, it&rsquo;s most secure to err (🥁) on the side of less. Instead of returning &ldquo;Your password for this account is incorrect,&rdquo; try the more ambiguous feedback &ldquo;Incorrect login information,&rdquo; and avoid revealing whether the username or email is in the system.
In order to be more helpful, provide a prominent way to contact a human in case an error should arise. Avoid revealing information that isn&rsquo;t necessary. If nothing else, for heaven&rsquo;s sake, don&rsquo;t suggest data that&rsquo;s a close match to the user input.
Be a bad guy When considering security, it&rsquo;s helpful to take a step back, observe the information on display, and ask yourself how a malicious attacker would be able to utilize it. Play devil&rsquo;s advocate. If a bad guy saw this page, what new information would they gain? Does the view show any PII?
Ask yourself if everything on the page is actually necessary for a genuine user. If not, redact or remove it. Less is safer.
Security starts at the front door These days, there&rsquo;s a lot more overlap between coding on the front end and the back end. To create a well-rounded and secure application, it helps to have a general understanding of ways attackers can get their foot in the front door.
`,url:"https://victoria.dev/archive/secure-web-forms-for-the-front-end-developer/"},"https://victoria.dev/posts/the-surprisingly-difficult-task-of-printing-newlines-in-a-terminal/":{title:"The surprisingly difficult task of printing newlines in a terminal",tags:["terminal","cybersecurity"],content:`Surprisingly, getting computers to give humans readable output is no easy feat. With the introduction of standard streams and specifically standard output, programs gained a way to talk to each other using plain text streams; humanizing and displaying stdout is another matter. Technology throughout the computing age has tried to solve this problem, from the use of ASCII characters in video computer displays to modern shell commands like echo and printf.
These advancements have not been seamless. The job of printing output to a terminal is fraught with quirks for programmers to navigate, as exemplified by the deceptively nontrivial task of expanding an escape sequence to print newlines. The expansion of the placeholder \\n can be accomplished in a multitude of ways, each with its own unique history and complications.
Using echo From its appearance in Multics to its modern-day Unix-like system ubiquity, echo remains a familiar tool for getting your terminal to say &ldquo;Hello world!&rdquo; Unfortunately, inconsistent implementations across operating systems make its usage tricky. Where echo on some systems will automatically expand escape sequences, others require the -e option to do the same:
echo &#34;the study of European nerves is \\neurology&#34; # the study of European nerves is \\neurology echo -e &#34;the study of European nerves is \\neurology&#34; # the study of European nerves is # eurology Because of these inconsistencies in implementations, echo is considered non-portable. Additionally, its usage in conjunction with user input is relatively easy to corrupt through shell injection attack using command substitutions.
In modern systems, it is retained only to provide compatibility with the many programs that still use it. The POSIX specification recommends the use of printf in new programs.
Using printf Since 4th Edition Unix, the portable printf command has essentially been the new and better echo. It allows you to use format specifiers to humanize input. To interpret backslash escape sequences, use %b. The character sequence \\n ensures the output ends with a newline:
printf &#34;%b\\n&#34; &#34;Many females in Oble are \\noblewomen&#34; # Many females in Oble are # oblewomen Though printf has further options that make it a far more powerful replacement of echo, this utility is not foolproof and can be vulnerable to an uncontrolled format string attack. It&rsquo;s important for programmers to ensure they carefully handle user input.
Putting newlines in variables In an effort to improve portability amongst compilers, the ANSI C Standard was established in 1983. With ANSI-C quoting using $'...', escape sequences are replaced in output according to the standard.
This allows us to store strings with newlines in variables that are printed with the newlines interpreted. You can do this by setting the variable, then calling it with printf using $:
puns=$&#39;\\number\\narrow\\nether\\nice&#39; printf &#34;%b\\n&#34; &#34;These words started with n but don&#39;t make $puns&#34; # These words started with n but don&#39;t make # umber # arrow # ether # ice The expanded variable is single-quoted, which is passed literally to printf. As always, it is important to properly handle the input.
Bonus round: shell parameter expansion In my article explaining Bash and braces, I covered the magic of shell parameter expansion. We can use one expansion, \${parameter@operator}, to interpret escape sequences, too. We use printf&rsquo;s %s specifier to print as a string, and the E operator will properly expand the escape sequences in our variable:
printf &#34;%s\\n&#34; \${puns@E} # umber # arrow # ether # ice The ongoing challenge of humanizing output String interpolation continues to be a chewy problem for programmers. Besides getting languages and shells to agree on what certain placeholders mean, properly using the correct escape sequences requires an eye for detail.
Poor string interpolation can lead to silly-looking output, as well as introduce security vulnerabilities, such as from injection attacks. Until the next evolution of the terminal has us talking in emojis, we&rsquo;d best pay attention when printing output for humans.
`,url:"https://victoria.dev/posts/the-surprisingly-difficult-task-of-printing-newlines-in-a-terminal/"},"https://victoria.dev/archive/the-care-and-feeding-of-an-iot-device/":{title:"The care and feeding of an IoT device",tags:["cybersecurity","privacy","life"],content:`Giving someone a puppy for Christmas might work really well in a movie, but in real life often comes hitched to a multitude of responsibilities that the giftee may not be fully prepared to take on. The same is true for Internet of Things (IoT) devices, including Amazon&rsquo;s Alexa-enabled devices, Google Home, and other Internet-connected appliances like cameras, lightbulbs, and toasters. Yes, they have those now.
Like puppies, IoT devices are still young. Many contain known vulnerabilities that remote attackers can use to gain access to device owners&rsquo; networks. These attacks are sometimes as laughably simple as using a default username and password that the device owner cannot change.
Does all this mean you shouldn&rsquo;t give Grandma Mabel a new app-enabled coffee maker or Ring doorbell for Christmas? Probably, although not necessarily. Like puppies, properly-maintained IoT devices are capable of warming your heart without causing too much havoc; but they take a lot of work to care for. Here are a few responsibilities to keep in mind for the care and feeding of an IoT device.
Immature security Many manufacturers of IoT devices have not made security a priority. There aren&rsquo;t yet any enforced security requirements for this industry, which leaves the protection of your device and the network it&rsquo;s connected to in the hands of the manufacturer.
It&rsquo;s not just obscure no-name toasters, either; malicious third-party apps have snuck onto Amazon&rsquo;s and Google&rsquo;s more reputable devices and enabled attackers to eavesdrop on unsuspecting owners.
Until security regulations are put in place and enforced, it&rsquo;s buyer beware for both devices and third-party applications. To the extent possible, potential owners must do ample research to weed out vulnerable devices and untrustworthy apps.
Protecting your network If you think hackers aren&rsquo;t likely to find your device in the vast expanse of the Internet, you might be wrong. These days, obscurity doesn&rsquo;t provide security. It&rsquo;s no longer left up to a potential attacker&rsquo;s fallible human eyes to find your insecure front door camera in a cacophony of wireless traffic; IoT search engines like Shodan will do that for them. Thankfully, these search engines are also used for good, enabling white hat hackers and penetration testers to find and fix insecure devices.
Just like locking your own front door, IoT owners are responsible for locking down access to their devices. This may mean searching through device settings to make sure default credentials are changed, or checking to make sure that a device used on your private home network doesn&rsquo;t by default have public Internet access.
Where the options are available, HTTPS and multifactor authentication should be enabled. The use of a VPN can also keep your devices from being found.
Keeping them patched Unlike puppies, many IoT devices are &ldquo;headless&rdquo; and have no inherent way of interfacing with a human. An app-controlled lightbulb, for example, may be all but useless without the software that makes it shine. As convenient as it may be to have your 1500K mood lighting come on automatically at dusk, it also means automatically ceding control of the device to its software developers.
When vulnerabilities in your phone&rsquo;s operating system are discovered and patched, it&rsquo;s likely that automatic updates are pushed and installed overnight, possibly without you even knowing. Your IoT device, on the other hand, may have no such support. In those cases, it&rsquo;s completely up to the user to discover that an update is needed, find and download the patch, then correctly update their device. Even for owners with some technical expertise, this process takes significant effort. Many device owners aren&rsquo;t even aware that their software is dangerously outdated.
In practical terms, this means that users without the time, knowledge, or willingness to keep their devices updated should reconsider owning them. Alternatively, some research can help prospective owners choose devices that receive automatic push updates from their (hopefully responsible) manufacturers over WiFi.
Being responsible Raising a healthy and happy IoT device is no small task, especially for potential owners with little time or willingness to put in the required effort. With the proper attention and maintenance, your Internet-connected appliance can bring joy and convenience to your life; but without, it introduces a potential security risk and a whole lot of trouble.
Before getting or giving IoT, be sure the potential owner is up to the task of caring for it.
You can learn more about basic cybersecurity for IoT (as a user or maker) by reading NIST&rsquo;s draft guidelines publication.
`,url:"https://victoria.dev/archive/the-care-and-feeding-of-an-iot-device/"},"https://victoria.dev/posts/bash-and-shell-expansions-lazy-list-making/":{title:"Bash and shell expansions: lazy list-making",tags:["terminal","linux"],content:`It&rsquo;s that time of year again! When stores start putting up colourful sparkly lit-up plastic bits, we all begin to feel a little festive, and by festive I mean let&rsquo;s go shopping. Specifically, holiday gift shopping! (Gifts for yourself are still gifts, technically.)
Just so this doesn&rsquo;t all go completely madcap, you ought to make some gift lists. Bash can help.
Brace expansion These are not braces: ()
Neither are these: []
These are braces: {}
Braces tell Bash to do something with the arbitrary string or strings it finds between them. Multiple strings are comma-separated: {a,b,c}. You can also add an optional preamble and postscript to be attached to each expanded result. Mostly, this can save some typing, such as with common file paths and extensions.
Let&rsquo;s make some lists for each person we want to give stuff to. The following commands are equivalent:
touch /home/me/gift-lists/Amy.txt /home/me/gift-lists/Bryan.txt /home/me/gift-lists/Charlie.txt touch /home/me/gift-lists/{Amy,Bryan,Charlie}.txt tree gift-lists /home/me/gift-lists ├── Amy.txt ├── Bryan.txt └── Charlie.txt Oh darn, &ldquo;Bryan&rdquo; spells his name with an &ldquo;i.&rdquo; I can fix that.
mv /home/me/gift-lists/{Bryan,Brian}.txt renamed &#39;/home/me/gift-lists/Bryan.txt&#39; -&gt; &#39;/home/me/gift-lists/Brian.txt&#39; Shell parameter expansions Shell parameter expansion allows us to make all sorts of changes to parameters enclosed in braces, like manipulate and substitute text.
There are a few stocking stuffers that all our giftees deserve. Let&rsquo;s make that a variable:
STUFF=$&#39;socks\\nlump of coal\\nwhite chocolate&#39; echo &#34;$STUFF&#34; socks lump of coal white chocolate Now to add these items to each of our lists with some help from the tee command to get echo and expansions to play nice.
echo &#34;$STUFF&#34; | tee {Amy,Brian,Charlie}.txt cat {Amy,Brian,Charlie}.txt socks lump of coal white chocolate socks lump of coal white chocolate socks lump of coal white chocolate Pattern match substitution On second thought, maybe the lump of coal isn&rsquo;t such a nice gift. You can replace it with something better using a pattern match substitution in the form of \${parameter/pattern/string}:
echo &#34;\${STUFF/lump of coal/candy cane}&#34; | tee {Amy,Brian,Charlie}.txt cat {Amy,Brian,Charlie}.txt socks candy cane white chocolate socks candy cane white chocolate socks candy cane white chocolate This replaces the first instance of &ldquo;lump of coal&rdquo; with &ldquo;candy cane.&rdquo; To replace all instances (if there were multiple), use \${parameter//pattern/string}. This doesn&rsquo;t change our $STUFF variable, so we can still reuse the original list for someone naughty later.
Substrings While we&rsquo;re improving things, our giftees may not all like white chocolate. We&rsquo;d better add some regular chocolate to our lists just in case. Since I&rsquo;m super lazy, I&rsquo;m just going to hit the up arrow and modify a previous Bash command. Luckily, the last word in the $STUFF variable is &ldquo;chocolate,&rdquo; which is nine characters long, so I&rsquo;ll tell Bash to keep just that part using \${parameter:offset}. I&rsquo;ll use tee&rsquo;s -a flag to append to my existing lists:
echo &#34;\${STUFF: -9}&#34; | tee -a {Amy,Brian,Charlie}.txt cat {Amy,Brian,Charlie}.txt socks candy cane white chocolate chocolate socks candy cane white chocolate chocolate socks candy cane white chocolate chocolate You can also:
Do this With this Get substring from n characters onwards \${parameter:n} Get substring for x characters starting at n \${parameter:n:x} There! Now our base lists are finished. Let&rsquo;s have some eggnog.
Testing variables You know, it may be the eggnog, but I think I started a list for Amy yesterday and stored it in a variable that I might have called amy. Let&rsquo;s see if I did. I&rsquo;ll use the \${parameter:?word} expansion. It&rsquo;ll write word to standard error and exit if there&rsquo;s no amy parameter.
echo &#34;\${amy:?no such}&#34; bash: amy: no such I guess not. Maybe it was Brian instead?
echo &#34;\${brian:?no such}&#34; Lederhosen You can also:
Do this With this Substitute word if parameter is unset or null \${parameter:-word} Substitute word if parameter is not unset or null \${parameter:+word} Assign word to parameter if parameter is unset or null \${parameter:=word} Changing case That&rsquo;s right! Brian said he wanted some lederhosen and so I made myself a note. This is pretty important, so I&rsquo;ll add it to Brian&rsquo;s list in capital letters with the \${parameter^^pattern} expansion. The pattern part is optional. We&rsquo;re only writing to Brian&rsquo;s list, so I&rsquo;ll just use &gt;&gt; instead of tee -a.
echo &#34;\${brian^^}&#34; &gt;&gt; Brian.txt cat Brian.txt socks candy cane white chocolate chocolate LEDERHOSEN You can also:
Do this With this Capitalize the first letter \${parameter^pattern} Lowercase the first letter \${parameter,pattern} Lowercase all letters \${parameter,,pattern} Expanding arrays You know what, all this gift-listing business is a lot of work. I&rsquo;m just going to make an array of things I saw at the store:
gifts=(sweater gameboy wagon pillows chestnuts hairbrush) I can use substring expansion in the form of \${parameter:offset:length} to make this simple. I&rsquo;ll add the first two to Amy&rsquo;s list, the middle two to Brian&rsquo;s, and the last two to Charlie&rsquo;s. I&rsquo;ll use printf to help with newlines.
printf &#39;%s\\n&#39; &#34;\${gifts[@]:0:2}&#34; &gt;&gt; Amy.txt printf &#39;%s\\n&#39; &#34;\${gifts[@]:2:2}&#34; &gt;&gt; Brian.txt printf &#39;%s\\n&#39; &#34;\${gifts[@]: -2}&#34; &gt;&gt; Charlie.txt cat Amy.txt socks candy cane white chocolate chocolate sweater gameboy cat Brian.txt socks candy cane white chocolate chocolate LEDERHOSEN wagon pillows cat Charlie.txt socks candy cane white chocolate chocolate chestnuts hairbrush There! Now we&rsquo;ve got a comprehensive set of super personalized gift lists. Thanks Bash! Too bad it can&rsquo;t do the shopping for us, too.
`,url:"https://victoria.dev/posts/bash-and-shell-expansions-lazy-list-making/"},"https://victoria.dev/archive/a-cron-job-that-could-save-you-from-a-ransomware-attack/":{title:"A cron job that could save you from a ransomware attack",tags:["cybersecurity","data","terminal","aws"],content:`It&rsquo;s 2019, and ransomware has become a thing.
Systems that interact with the public, like companies, educational institutions, and public services, are most susceptible. While delivery methods for ransomware vary from the physical realm to communication via social sites and email, all methods only require one person to make one mistake in order for ransomware to proliferate.
Ransomware, as you may have heard, is a malicious program that encrypts your files, rendering them unreadable and useless to you. It can include instructions for paying a ransom, usually by sending cryptocurrency, in order to obtain the decryption key. Successful ransomware attacks typically exploit vital, time-sensitive systems. Victims like public services and medical facilities are more likely to have poor or zero recovery processes, leaving governments or insurance providers to reward attackers with ransom payments.
Individuals, especially less-than-tech-savvy ones, are no less at risk. Ransomware can occlude personal documents and family photos that may only exist on one machine.
Thankfully, a fairly low-tech solution exists for rendering ransomware inept: back up your data!
You could achieve this with a straightforward system like plugging in an external hard drive and dragging files over once a day, but this method has a few hurdles. Manually transferring files may be slow or incomplete, and besides, you&rsquo;ll first have to remember to do it.
In my constant pursuit of automating all the things, there&rsquo;s one tool I often return to for its simplicity and reliability: cron. Cron does one thing, and does it well: it runs commands on a schedule.
I first used it a few months shy of three years ago (Have I really been blogging that long?!) to create custom desktop notifications on Linux. Using the crontab configuration file, which you can edit by running crontab -e, you can specify a schedule for running any commands you like. Here&rsquo;s what the scheduling syntax looks like, from the Wikipedia cron page:
# ┌───────────── minute (0 - 59) # │ ┌───────────── hour (0 - 23) # │ │ ┌───────────── day of the month (1 - 31) # │ │ │ ┌───────────── month (1 - 12) # │ │ │ │ ┌───────────── day of the week (0 - 6) # │ │ │ │ │ # │ │ │ │ │ # │ │ │ │ │ # * * * * * command to execute For example, a cron job that runs every day at 00:00 would look like:
0 0 * * * To run a job every twelve hours, the syntax is:
0 */12 * * * This great tool can help you wrap your head around the cron scheduling syntax.
What&rsquo;s a scheduler have to do with backing up? By itself, not much. The simple beauty of cron is that it runs commands - any shell commands, and any scripts that you&rsquo;d normally run on the command line. As you may have gleaned from my other posts, I&rsquo;m of the strong opinion that you can do just about anything on the command line, including backing up your files. Options for storage in this area are plentiful, from near-to-free local and cloud options, as well as paid managed services too numerous to list. For CLI tooling, we have utilitarian classics like rsync, and CLI tools for specific cloud providers like AWS.
Backing up with rsync The rsync utility is a classic choice, and can back up your files to an external hard drive or remote server while making intelligent determinations about which files to update. It uses file size and modification times to recognize file changes, and then only transfers changed files, saving time and bandwidth.
The rsync syntax can be a little nuanced; for example, a trailing forward slash will copy just the contents of the directory, instead of the directory itself. I found examples to be helpful in understanding the usage and syntax.
Here&rsquo;s one for backing up a local directory to a local destination, such as an external hard drive:
rsync -a /home/user/directory /media/user/destination The first argument is the source, and the second is the destination. Reversing these in the above example would copy files from the mounted drive to the local home directory.
The a flag for archive mode is one of rsync&rsquo;s superpowers. Equivalent to flags -rlptgoD, it:
Syncs files recursively through directories (r); Preserves symlinks (l), permissions (p), modification times (t), groups (g), and owner (o); and Copies device and special files (D). Here&rsquo;s another example, this time for backing up the contents of a local directory to a directory on a remote server using SSH:
rsync -avze ssh /home/user/directory/ user@remote.host.net:home/user/directory The v flag turns on verbose output, which is helpful if you like realtime feedback on which files are being transferred. During large transfers, however, it can tend to slow things down. The z flag can help with that, as it indicates that files should be compressed during transfer.
The e flag, followed by ssh, tells rsync to use SSH according to the destination instructions provided in the final argument.
Backing up with AWS CLI Amazon Web Services offers a command line interface tool for doing just about everything with your AWS set up, including a straightforward s3 sync command for recursively copying new and updated files to your S3 storage buckets. As a storage method for back up data, S3 is a stable and inexpensive choice. You can even turn on versioning in your bucket.
The syntax for interacting with directories is fairly straightforward, and you can directly indicate your S3 bucket as an S3Uri argument in the form of s3://mybucket/mykey. To back up a local directory to your S3 bucket, the command is:
aws s3 sync /home/user/directory s3://mybucket Similar to rsync, reversing the source and destination would download files from the S3 bucket.
The sync command is intuitive by default. It will guess the mime type of uploaded files, as well as include files discovered by following symlinks. A variety of options exist to control these and other defaults, even including flags to specify the server-side encryption to be used.
Setting up your cronjob back up You can edit your machine&rsquo;s cron file by running:
crontab -e Intuitive as it may be, it&rsquo;s worth mentioning that your back up commands will only run when your computer is turned on and the cron daemon is running. With this in mind, choose a schedule for your cronjob that aligns with times when your machine is powered on, and maybe not overloaded with other work.
To back up to an S3 bucket every day at 8AM, for example, you&rsquo;d put a line in your crontab that looks like:
0 8 * * * aws s3 sync /home/user/directory s3://mybucket If you&rsquo;re curious whether your cron job is currently running, find the PID of cron with:
pstree -ap | grep cron Then run pstree -ap &lt;PID&gt;.
This rabbit hole goes deeper; a quick search can reveal different ways of organizing and scheduling cronjobs, or help you find different utilities to run cronjobs when your computer is asleep. To protect against the possibility of ransomware-affected files being transferred to your back up, incrementally separated archives are a good idea. In essence, however, this basic set up is all you really need to create a reliable, automatic back up system.
Don&rsquo;t feed the trolls Humans are fallible; that&rsquo;s why cyberattacks work. The success of a ransomware attack depends on the victim having no choice but to pay up in order to return to business as usual. A highly accessible recent back up undermines attackers who depend on us being unprepared. By blowing away a system and restoring from yesterday&rsquo;s back up, we may lose a day of progress; ransomers, however, gain nothing at all.
For further resources on ransomware defense for users and organizations, check out CISA&rsquo;s advice on ransomware.
`,url:"https://victoria.dev/archive/a-cron-job-that-could-save-you-from-a-ransomware-attack/"},"https://victoria.dev/archive/publishing-github-event-data-with-github-actions-and-pages/":{title:"Publishing GitHub event data with GitHub Actions and Pages",tags:["data","ci/cd","api","git","websites"],content:`Teams who work on GitHub rely on event data to collaborate. The data recorded as issues, pull requests, and comments, become vital to understanding the project.
With the general availability of GitHub Actions, we have a chance to programmatically access and preserve GitHub event data in our repository. Making the data part of the repository itself is a way of preserving it outside of GitHub, and also gives us the ability to feature the data on a front-facing website, such as with GitHub Pages, through an automated process that&rsquo;s part of our CI/CD pipeline.
And, if you&rsquo;re like me, you can turn GitHub issue comments into an awesome 90s guestbook page.
No matter the usage, the principle concepts are the same. We can use Actions to access, preserve, and display GitHub event data - with just one workflow file. To illustrate the process, I&rsquo;ll take you through the workflow code that makes my guestbook shine on.
For an introductory look at GitHub Actions including how workflows are triggered, see A lightweight, tool-agnostic CI/CD flow with GitHub Actions.
Accessing GitHub event data An Action workflow runs in an environment with some default environment variables. A lot of convenient information is available here, including event data. The most complete way to access the event data is using the $GITHUB_EVENT_PATH variable, the path of the file with the complete JSON event payload.
The expanded path looks like /home/runner/work/_temp/_github_workflow/event.json and its data corresponds to its webhook event. You can find the documentation for webhook event data in GitHub REST API Event Types and Payloads. To make the JSON data available in the workflow environment, you can use a tool like jq to parse the event data and put it in an environment variable.
Below, I grab the comment ID from an issue comment event:
ID=&#34;$(jq &#39;.comment.id&#39; $GITHUB_EVENT_PATH)&#34; Most event data is also available via the github.event context variable without needing to parse JSON. The fields are accessed using dot notation, as in the example below where I grab the same comment ID:
ID=\${{ github.event.comment.id }} For my guestbook, I want to display entries with the user&rsquo;s handle, and the date and time. I can capture this event data like so:
AUTHOR=\${{ github.event.comment.user.login }} DATE=\${{ github.event.comment.created_at }} Shell variables are handy for accessing data, however, they&rsquo;re ephemeral. The workflow environment is created anew each run, and even shell variables set in one step do not persist to other steps. To persist the captured data, you have two options: use artifacts, or commit it to the repository.
Preserving event data: using artifacts Using artifacts, you can persist data between workflow jobs without committing it to your repository. This is handy when, for example, you wish to transform or incorporate the data before putting it somewhere more permanent.
Two actions assist with using artifacts: upload-artifact and download-artifact. You can use these actions to make files available to other jobs in the same workflow. For a full example, see passing data between jobs in a workflow.
The upload-artifact action&rsquo;s action.yml contains an explanation of the keywords. The uploaded files are saved in .zip format. Another job in the same workflow run can use the download-artifact action to utilize the data in another step.
You can also manually download the archive on the workflow run page, under the repository&rsquo;s Actions tab.
Persisting workflow data between jobs does not make any changes to the repository files, as the artifacts generated live only in the workflow environment. Personally, being comfortable working in a shell environment, I see a narrow use case for artifacts, though I&rsquo;d have been remiss not to mention them. Besides passing data between jobs, they could be useful for creating .zip format archives of, say, test output data. In the case of my guestbook example, I simply ran all the necessary steps in one job, negating any need for passing data between jobs.
Preserving event data: pushing workflow files to the repository To preserve data captured in the workflow in the repository itself, it is necessary to add and push this data to the Git repository. You can do this in the workflow by creating new files with the data, or by appending data to existing files, using shell commands.
Creating files in the workflow To work with the repository files in the workflow, use the checkout action to first get a copy to work with:
- uses: actions/checkout@master with: fetch-depth: 1 To add comments to my guestbook, I turn the event data captured in shell variables into proper files, using substitutions in shell parameter expansion to sanitize user input and translate newlines to paragraphs. I wrote previously about why user input should be treated carefully.
- name: Turn comment into file run: | ID=\${{ github.event.comment.id }} AUTHOR=\${{ github.event.comment.user.login }} DATE=\${{ github.event.comment.created_at }} COMMENT=$(echo &#34;\${{ github.event.comment.body }}&#34;) NO_TAGS=\${COMMENT//[&lt;&gt;]/\\\`} FOLDER=comments printf &#39;%b\\n&#39; &#34;&lt;div class=\\&#34;comment\\&#34;&gt;&lt;p&gt;\${AUTHOR} says:&lt;/p&gt;&lt;p&gt;\${NO_TAGS//$&#39;\\n&#39;/\\&lt;\\/p\\&gt;\\&lt;p\\&gt;}&lt;/p&gt;&lt;p&gt;\${DATE}&lt;/p&gt;&lt;/div&gt;\\r\\n&#34; &gt; \${FOLDER}/\${ID}.html By using printf and directing its output with &gt; to a new file, the event data is transformed into an HTML file, named with the comment ID number, that contains the captured event data. Formatted, it looks like:
&lt;div class=&#34;comment&#34;&gt; &lt;p&gt;victoriadrake says:&lt;/p&gt; &lt;p&gt;This is a comment!&lt;/p&gt; &lt;p&gt;2019-11-04T00:28:36Z&lt;/p&gt; &lt;/div&gt; When working with comments, one effect of naming files using the comment ID is that a new file with the same ID will overwrite the previous. This is handy for a guestbook, as it allows any edits to a comment to replace the original comment file.
If you&rsquo;re using a static site generator like Hugo, you could build a Markdown format file, stick it in your content/ folder, and the regular site build will take care of the rest. In the case of my simplistic guestbook, I have an extra step to consolidate the individual comment files into a page. Each time it runs, it overwrites the existing index.html with the header.html portion (&gt;), then finds and appends (&gt;&gt;) all the comment files&rsquo; contents in descending order, and lastly appends the footer.html portion to end the page.
- name: Assemble page run: | cat header.html &gt; index.html find comments/ -name &#34;*.html&#34; | sort -r | xargs -I % cat % &gt;&gt; index.html cat footer.html &gt;&gt; index.html Committing changes to the repository Since the checkout action is not quite the same as cloning the repository, at time of writing, there are some issues still to work around. A couple extra steps are necessary to pull, checkout, and successfully push changes back to the master branch, but this is pretty trivially done in the shell.
Below is the step that adds, commits, and pushes changes made by the workflow back to the repository&rsquo;s master branch.
- name: Push changes to repo run: | REMOTE=https://\${{ secrets.GITHUB_TOKEN }}@github.com/\${{ github.repository }} git config user.email &#34;\${{ github.actor }}@users.noreply.github.com&#34; git config user.name &#34;\${{ github.actor }}&#34; git pull \${REMOTE} git checkout master git add . git status git commit -am &#34;Add new comment&#34; git push \${REMOTE} master The remote, in fact, our repository, is specified using the github.repository context variable. For our workflow to be allowed to push to master, we give the remote URL using the default secrets.GITHUB_TOKEN variable.
Since the workflow environment is shiny and newborn, we need to configure Git. In the above example, I&rsquo;ve used the github.actor context variable to input the username of the account initiating the workflow. The email is similarly configured using the default noreply GitHub email address.
Displaying event data If you&rsquo;re using GitHub Pages with the default secrets.GITHUB_TOKEN variable and without a site generator, pushing changes to the repository in the workflow will only update the repository files. The GitHub Pages build will fail with an error, &ldquo;Your site is having problems building: Page build failed.&rdquo;
To enable Actions to trigger a Pages site build, you&rsquo;ll need to create a Personal Access Token. This token can be stored as a secret in the repository settings and passed into the workflow in place of the default secrets.GITHUB_TOKEN variable. I wrote more about Actions environment and variables in this post.
With the use of a Personal Access Token, a push initiated by the Actions workflow will also update the Pages site. You can see it for yourself by leaving a comment in my guestbook! The comment creation event triggers the workflow, which then takes around 30 seconds to run and update the guestbook page.
Where a site build is necessary for changes to be published, such as when using Hugo, an Action can do this too. However, in order to avoid creating unintended loops, one Action workflow will not trigger another (see what will). Instead, it&rsquo;s extremely convenient to handle the process of building the site with a Makefile, which any workflow can then run. Simply add running the Makefile as the final step in your workflow job, with the repository token where necessary:
- name: Run Makefile env: TOKEN: \${{ secrets.GITHUB_TOKEN }} run: make all This ensures that the final step of your workflow builds and deploys the updated site.
No more event data horizon GitHub Actions provides a neat way to capture and utilize event data so that it&rsquo;s not only available within GitHub. The possibilities are only as limited as your imagination! Here are a few ideas for things this lets us create:
A public-facing issues board, where customers without GitHub accounts can view and give feedback on project issues. An automatically-updating RSS feed of new issues, comments, or PRs for any repository. A comments system for static sites, utilizing GitHub issue comments as an input method. An awesome 90s guestbook page. Did I mention I made a 90s guestbook page? My inner-Geocities-nerd is a little excited.
`,url:"https://victoria.dev/archive/publishing-github-event-data-with-github-actions-and-pages/"},"https://victoria.dev/posts/a-lightweight-tool-agnostic-ci/cd-flow-with-github-actions/":{title:"A lightweight, tool-agnostic CI/CD flow with GitHub Actions",tags:["ci/cd","terminal","websites"],content:`Agnostic tooling is the clever notion that you should be able to run your code in various environments. With many continuous integration and continuous development (CI/CD) apps available, agnostic tooling gives developers a big advantage: portability.
Of course, having your CI/CD work everywhere is a tall order. Popular CI apps for GitHub repositories alone use a multitude of configuration languages spanning Groovy, YAML, TOML, JSON, and more&hellip; all with differing syntax, of course. Porting workflows from one tool to another is more than a one-cup-of-coffee process.
The introduction of GitHub Actions has the potential to add yet another tool to the mix; or, for the right set up, greatly simplify a CI/CD workflow.
Prior to this article, I accomplished my CD flow with several lashed-together apps. I used AWS Lambda to trigger site builds on a schedule. I had Netlify build on push triggers, as well as run image optimization, and then push my site to the public Pages repository. I used Travis CI in the public repository to test the HTML. All this worked in conjunction with GitHub Pages, which actually hosts the site.
I&rsquo;m now using the GitHub Actions beta to accomplish all the same tasks, with one portable Makefile of build instructions, and without any other CI/CD apps.
Appreciating the shell What do most CI/CD tools have in common? They run your workflow instructions in a shell environment! This is wonderful, because that means that most CI/CD tools can do anything that you can do in a terminal&hellip; and you can do pretty much anything in a terminal.
Especially for a contained use case like building my static site with a generator like Hugo, running it all in a shell is a no-brainer. To tell the magic box what to do, we just need to write instructions.
While a shell script is certainly the most portable option, I use the still-very-portable Make to write my process instructions. This provides me with some advantages over simple shell scripting, like the use of variables and macros, and the modularity of rules.
I got into the nitty-gritty of my Makefile in my last post. Let&rsquo;s look at how to get GitHub Actions to run it.
Using a Makefile with GitHub Actions To our point on portability, my magic Makefile is stored right in the repository root. Since it&rsquo;s included with the code, I can run the Makefile locally on any system where I can clone the repository, provided I set the environment variables. Using GitHub Actions as my CI/CD tool is as straightforward as making Make go worky-worky.
I found the GitHub Actions workflow syntax guide to be pretty straightforward, though also lengthy on options. Here&rsquo;s the necessary set up for getting the Makefile to run.
The workflow file at .github/workflows/make-master.yml contains the following:
name: make-master on: push: branches: - master schedule: - cron: &#39;20 13 * * *&#39; jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@master with: fetch-depth: 1 - name: Run Makefile env: TOKEN: \${{ secrets.TOKEN }} run: make all I&rsquo;ll explain the components that make this work.
Triggering the workflow Actions support multiple triggers for a workflow. Using the on syntax, I&rsquo;ve defined two triggers for mine: a push event to the master branch only, and a scheduled cron job.
Once the make-master.yml file is in your repository, either of your triggers will cause Actions to run your Makefile. To see how the last run went, you can also add a fun badge to the README.
One hacky thing Because the Makefile runs on every push to master, I sometimes would get errors when the site build had no changes. When Git, via my Makefile, attempted to commit to the Pages repository, no changes were detected and the commit would fail annoyingly:
nothing to commit, working tree clean On branch master Your branch is up to date with &#39;origin/master&#39;. nothing to commit, working tree clean Makefile:62: recipe for target &#39;deploy&#39; failed make: *** [deploy] Error 1 ##[error]Process completed with exit code 2. I came across some solutions that proposed using diff to check if a commit should be made, but this may not work for reasons. As a workaround, I simply added the current UTC time to my index page so that every build would contain a change to be committed.
Environment and variables You can define the virtual environment for your workflow to run in using the runs-on syntax. The obvious best choice one I chose is Ubuntu. Using ubuntu-latest gets me the most updated version, whatever that happens to be when you&rsquo;re reading this.
GitHub sets some default environment variables for workflows. The actions/checkout action with fetch-depth: 1 creates a copy of just the most recent commit your repository in the GITHUB_WORKSPACE variable. This allows the workflow to access the Makefile at GITHUB_WORKSPACE/Makefile. Without using the checkout action, the Makefile won&rsquo;t be found, and I get an error that looks like this:
make: *** No rule to make target &#39;all&#39;. Stop. Running Makefile ##[error]Process completed with exit code 2. While there is a default GITHUB_TOKEN secret, this is not the one I used. The default is only locally scoped to the current repository. To be able to push to my separate GitHub Pages repository, I created a personal access token scoped to public_repo and pass it in as the secrets.TOKEN encrypted variable. For a step-by-step, see Creating and using encrypted secrets.
Portable tooling The nice thing about using a simple Makefile to define the bulk of my CI/CD process is that it&rsquo;s completely portable. I can run a Makefile anywhere I have access to an environment, which is most CI/CD apps, virtual instances, and, of course, on my local machine.
One of the reasons I like GitHub Actions is that getting my Makefile to run was pretty straightforward. I think the syntax is well done - easy to read, and intuitive when it comes to finding an option you&rsquo;re looking for. For someone already using GitHub Pages, Actions provides a pretty seamless CD experience; and if that should ever change, I can run my Makefile elsewhere. ¯\\_(ツ)_/¯
`,url:"https://victoria.dev/posts/a-lightweight-tool-agnostic-ci/cd-flow-with-github-actions/"},"https://victoria.dev/posts/a-portable-makefile-for-continuous-delivery-with-hugo-and-github-pages/":{title:"A portable Makefile for continuous delivery with Hugo and GitHub Pages",tags:["ci/cd","linux","terminal","websites"],content:`Fun fact: I first launched this GitHub Pages site 1,018 days ago.
Since then, we&rsquo;ve grown together. From early cringe-worthy commit messages, through eighty-six versions of Hugo, and up until last week, a less-than-streamlined multi-app continuous integration and deployment (CI/CD) workflow.
If you know me at all, you know I love to automate things. I&rsquo;ve been using a combination of AWS Lambda, Netlify, and Travis CI to automatically build and publish this site. My workflow for the task includes:
Build with Hugo on push to master, and on a schedule (Netlify and Lambda); Optimize and resize images (Netlify); Test with HTMLProofer (Travis CI); and Deploy to my separate, public, GitHub Pages repository (Netlify). Thanks to the introduction of GitHub Actions, I&rsquo;m able to do all the above with just one portable Makefile.
Next week I&rsquo;ll cover my Actions set up; today, I&rsquo;ll take you through the nitty-gritty of my Makefile so you can write your own.
Makefile portability POSIX-standard-flavour Make runs on every Unix-like system out there. Make derivatives, such as GNU Make and several flavours of BSD Make also run on Unix-like systems, though their particular use requires installing the respective program. To write a truly portable Makefile, mine follows the POSIX standard. (For a more thorough summation of POSIX-compatible Makefiles, I found this article helpful: A Tutorial on Portable Makefiles.) I run Ubuntu, so I&rsquo;ve tested the portability aspect using the BSD Make programs bmake, pmake, and fmake. Compatibility with non-Unix-like systems is a little more complicated, since shell commands differ. With derivatives such as Nmake, it&rsquo;s better to write a separate Makefile with appropriate Windows commands.
While much of my particular use case could be achieved with shell scripting, I find Make offers some worthwhile advantages. I enjoy the ease of using variables and macros, and the modularity of rules when it comes to organizing my steps.
The writing of rules mostly comes down to shell commands, which is the main reason Makefiles are as portable as they are. The best part is that you can do pretty much anything in a terminal, and certainly handle all the workflow steps listed above.
My continuous deployment Makefile Here&rsquo;s the portable Makefile that handles my workflow. Yes, I put emojis in there. I&rsquo;m a monster.
.POSIX: DESTDIR=public HUGO_VERSION=0.58.3 OPTIMIZE = find $(DESTDIR) -not -path &#34;*/static/*&#34; \\( -name &#39;*.png&#39; -o -name &#39;*.jpg&#39; -o -name &#39;*.jpeg&#39; \\) -print0 | \\ xargs -0 -P8 -n2 mogrify -strip -thumbnail &#39;1000&gt;&#39; .PHONY: all all: get_repository clean get build test deploy .PHONY: get_repository get_repository: @echo &#34;🛎 Getting Pages repository&#34; git clone https://github.com/victoriadrake/victoriadrake.github.io.git $(DESTDIR) .PHONY: clean clean: @echo &#34;🧹 Cleaning old build&#34; cd $(DESTDIR) &amp;&amp; rm -rf * .PHONY: get get: @echo &#34;❓ Checking for hugo&#34; @if ! [ -x &#34;$$(command -v hugo)&#34; ]; then\\ echo &#34;🤵 Getting Hugo&#34;;\\ wget -q -P tmp/ https://github.com/gohugoio/hugo/releases/download/v$(HUGO_VERSION)/hugo_extended_$(HUGO_VERSION)_Linux-64bit.tar.gz;\\ tar xf tmp/hugo_extended_$(HUGO_VERSION)_Linux-64bit.tar.gz -C tmp/;\\ sudo mv -f tmp/hugo /usr/bin/;\\ rm -rf tmp/;\\ hugo version;\\ fi .PHONY: build build: @echo &#34;🍳 Generating site&#34; hugo --gc --minify -d $(DESTDIR) @echo &#34;🧂 Optimizing images&#34; $(OPTIMIZE) .PHONY: test test: @echo &#34;🍜 Testing HTML&#34; docker run -v $(GITHUB_WORKSPACE)/$(DESTDIR)/:/mnt 18fgsa/html-proofer mnt --disable-external .PHONY: deploy deploy: @echo &#34;🎁 Preparing commit&#34; @cd $(DESTDIR) \\ &amp;&amp; git config user.email &#34;hello@victoria.dev&#34; \\ &amp;&amp; git config user.name &#34;Victoria via GitHub Actions&#34; \\ &amp;&amp; git add . \\ &amp;&amp; git status \\ &amp;&amp; git commit -m &#34;🤖 CD bot is helping&#34; \\ &amp;&amp; git push -f -q https://$(TOKEN)@github.com/victoriadrake/victoriadrake.github.io.git master @echo &#34;🚀 Site is deployed!&#34; Sequentially, this workflow:
Clones the public Pages repository; Cleans (deletes) the previous build files; Downloads and installs the specified version of Hugo, if Hugo is not already present; Builds the site; Optimizes images; Tests the built site with HTMLProofer, and Prepares a new commit and pushes to the public Pages repository. If you&rsquo;re familiar with command line, most of this may look familiar. Here are a couple bits that might warrant a little explanation.
Checking if a program is already installed I think this bit is pretty tidy:
if ! [ -x &#34;$$(command -v hugo)&#34; ]; then\\ ... fi I use a negated if conditional in conjunction with command -v to check if an executable (-x) called hugo exists. If one is not present, the script gets the specified version of Hugo and installs it. This Stack Overflow answer has a nice summation of why command -v is a more portable choice than which.
Image optimization My Makefile uses mogrify to batch resize and compress images in particular folders. It finds them automatically using the file extension, and only modifies images that are larger than the target size of 1000px in any dimension. I wrote more about the batch-processing one-liner in this post.
There are a few different ways to achieve this same task, one of which, theoretically, is to take advantage of Make&rsquo;s suffix rules to run commands only on image files. I find the shell script to be more readable.
Using Dockerized HTMLProofer HTMLProofer is installed with gem, and uses Ruby and Nokogiri, which adds up to a lot of installation time for a CI workflow. Thankfully, 18F has a Dockerized version that is much faster to implement. Its usage requires starting the container with the built site directory mounted as a data volume, which is easily achieved by appending to the docker run command.
docker run -v /absolute/path/to/site/:/mounted-site 18fgsa/html-proofer /mounted-site In my Makefile, I specify the absolute site path using the default environment variable GITHUB_WORKSPACE. I&rsquo;ll dive into this and other GitHub Actions features in the next post.
In the meantime, happy Making!
`,url:"https://victoria.dev/posts/a-portable-makefile-for-continuous-delivery-with-hugo-and-github-pages/"},"https://victoria.dev/archive/how-to-quickly-batch-resize-compress-and-convert-images-with-a-bash-one-liner/":{title:"How to quickly batch resize, compress, and convert images with a Bash one-liner",tags:["terminal","ci/cd"],content:`Part of my Hugo site continuous deployment workflow is the processing of 210 images, at time of writing.
Here&rsquo;s my one-liner:
find public/ -not -path &#34;*/static/*&#34; \\( -name &#39;*.png&#39; -o -name &#39;*.jpg&#39; -o -name &#39;*.jpeg&#39; \\) -print0 | xargs -0 -P8 -n2 mogrify -strip -thumbnail &#39;1000&gt;&#39; -format jpg I use find to target only certain image file formats in certain directories. With mogrify, part of ImageMagick, I resize only the images that are larger than a certain dimension, compress them, and strip the metadata. I tack on the format flag to create jpg copies of the images.
Here&rsquo;s the one-liner again (broken up for better reading):
# Look in the public/ directory find public/ \\ # Ignore directories called &#34;static&#34; regardless of location -not -path &#34;*/static/*&#34; \\ # Print the file paths of all files ending with any of these extensions \\( -name &#39;*.png&#39; -o -name &#39;*.jpg&#39; -o -name &#39;*.jpeg&#39; \\) -print0 \\ # Pipe the file paths to xargs and use 8 parallel workers to process 2 arguments | xargs -0 -P8 -n2 \\ # Tell mogrify to strip metadata, and... mogrify -strip \\ # ...compress and resize any images larger than the target size (1000px in either dimension) -thumbnail &#39;1000&gt;&#39; \\ # Convert the files to jpg format -format jpg That&rsquo;s it. That&rsquo;s the post.
`,url:"https://victoria.dev/archive/how-to-quickly-batch-resize-compress-and-convert-images-with-a-bash-one-liner/"},"https://victoria.dev/posts/secure-application-architecture-basics-separation-configuration-and-access/":{title:"Secure application architecture basics: separation, configuration, and access",tags:["cybersecurity","coding","data"],content:`Software developers today are encouraged to focus on building, and that&rsquo;s a great thing. There&rsquo;s the benefit of maker culture, an attitude of &ldquo;always be shipping,&rdquo; open source collaboration, and a bevy of apps that help you prioritize and execute with maximum efficiency. It&rsquo;s in an environment of constant creation, where both teams and solo entrepreneurs can be maximally productive.
Sometimes, this breakneck-speed productivity shows its downsides.
A lack of awareness of security seems to lead to a lack of prioritization of tasks that don&rsquo;t directly support bringing the product to launch. The market seems to have made it more important to launch a usable product than a secure one, with the prevailing attitude being, &ldquo;we can do the security stuff later.&rdquo;
Cobbling together a foundation based more on expediency than longevity is a bad way to build applications and a great way to build security debt. Security debt, like technical debt, amasses when developers make decisions that can make it more difficult to secure the application later on. If you&rsquo;re familiar with the concept of &ldquo;pushing left&rdquo; (or if you read my article about sensitive data exposure), you&rsquo;ll know that when it comes to security, sometimes there isn&rsquo;t a version of &ldquo;later&rdquo; that isn&rsquo;t too late. It&rsquo;s a shame, especially since following some basic security practices with high benefit yield early on in the development process doesn&rsquo;t take significantly more time than not following them. Often, it comes down to having some basic but important knowledge that enables making the more secure decision.
While application architecture specifics vary, there are a few basic principles you can commonly apply. This article will provide a high-level overview of areas that I hope will help point developers in the right direction.
There must be a reason you call it application &ldquo;architecture.&rdquo; I like to think it&rsquo;s because the architecture of software is similar in some basic ways to the architecture of a building. (Or at least, in my absolute zero building-building expertise, how I imagine a building to be built.) Here&rsquo;s how I like to summarize three basic points of secure application architecture:
Separated storage Customized configuration Controlled access and user scope This is only a jumping-off point meant to get you started on the right foot; a complete picture of a fully-realized application&rsquo;s security posture includes areas outside the scope of this article, including authentication, logging and monitoring, testing, and perhaps meeting compliance standards.
1. Separated storage From a security standpoint, the concept of separation refers to storing files that serve different purposes in different places. When you&rsquo;re constructing your building and deciding where all the rooms go, you similarly create the lobby on the ground floor and place administrative offices on higher floors, perhaps off the main path. While both are rooms, you understand that they serve different purposes, have different functional needs, and possibly very different security requirements.
When it comes to your files, the benefit is perhaps easiest to understand if you consider a simple file structure:
application/ ├───html/ │ └───index.html ├───assets/ │ ├───images/ │ │ ├───rainbows.jpg │ │ └───unicorns.jpg │ └───style.css └───super-secret-configurations/ └───master-keys.txt In this simplified example, let&rsquo;s say that all your application&rsquo;s images are stored in the application/assets/images/ directory. When one of your users creates a profile and uploads their picture to it, this picture is also stored in this folder. Makes sense, right? It&rsquo;s an image, and that&rsquo;s where the images go. What&rsquo;s the issue?
If you&rsquo;re familiar with navigating a file structure in a terminal, you may have seen this syntax before: ../../. The two dots are a handy way of saying, &ldquo;go up one directory.&rdquo; If you execute the command cd ../../ in the images/ directory of the simple file structure above, you&rsquo;d go up into assets/, then up again to the root directory, application/. This is a problem because of a wee little vulnerability dubbed path traversal.
While the dot syntax saves you some typing, it also introduces the interesting advantage of not actually needing to know what the parent directory is called in order to go to it. Consider an attack payload script, delivered into the images/ folder of your insecure application via an uploaded file, that went up one directory using cd ../ and then sent everything it found to the attacker, on repeat. Eventually, it would reach the root application directory and access the super-secret-configurations/ folder. Not good.
While other measures should be in place to prevent path traversal and related user upload vulnerabilities, the simplest prevention by far is a separation of storage. Core application files and assets should not be combined with other data, and especially not with user input. It&rsquo;s best to keep user-uploaded files and activity logs (which may contain juicy data and can be vulnerable to injection attacks) separate from the main application.
Separation can be achieved in a few ways, such as by using a different server, different instance, separate IP range, or separate domain.
2. Customized configuration Security misconfiguration is listed in the OWASP Top 10. A surprising number of very preventable security incidents occur because a server, firewall, or administrative account is running in production with default settings. Upon the opening of your new building, you&rsquo;d hopefully be more careful to ensure you haven&rsquo;t left any keys in the locks.
Usually, the victims of attacks related to default settings aren&rsquo;t specifically targeted. Rather, they are found by automated scanning tools that attackers run over many possible targets, effectively prodding at many different systems to see if any roll over and expose some useful exploit. The automated nature of this attack means that it&rsquo;s important for you to review settings for every piece of your architecture. Even if an individual piece doesn&rsquo;t seem significant, it may provide a vulnerability that allows an attacker to use it as a gateway to your larger application.
In particular, examine architecture components for unattended areas such as:
Default accounts, especially with default passwords, left in service; Example web pages, tutorial applications, or sample data left in the application; Unnecessary ports left in service, or ports left open to the Internet; Unrestricted permitted HTTP methods; Sensitive information stored in automated logs; Default configured permissions in managed services; and, Directory listings, or sensitive file types, left accessible by default. This list isn&rsquo;t exhaustive. Specific architecture components, such as cloud storage or web servers, will have other configurable features that should be reviewed. In general, reduce the application&rsquo;s attack surface by using minimal architecture components. If you use fewer components or don&rsquo;t install modules you don&rsquo;t need, you&rsquo;ll have fewer possible attack entry points to configure and safeguard.
3. Controlled access and user scope One of the more difficult security problems to test in an application is misconfigured access control. Automated testing tools have limited capability to find areas of an application that one user shouldn&rsquo;t be able to access. Thus, this is often left to manual testing or source code review to discover. By considering this vulnerability early on in the software development lifecycle when architectural decisions are being made, you reduce the risk that it becomes a problem that&rsquo;s harder to fix later. After all, you wouldn&rsquo;t simply leave your master keys out of reach on a high ledge and hope no one comes along with a ladder.
Broken access control is listed in the OWASP Top 10, which goes into more detail on its various forms. As a simple example, consider an application with two levels of access: administrators and users. You want to build a new feature - the ability to moderate or ban users - with the intention that only administrators would be allowed to use it.
If you&rsquo;re aware of the possibility of access control misconfigurations or exploits, you may decide to build the moderation feature in a completely separate area from the user-accessible space, such as on a different domain, or as part of a model that users don&rsquo;t share. This greatly reduces the risk that an access control misconfiguration or elevation of privilege vulnerability might allow a user to improperly access the moderation feature later on.
Of course, robust access control in your application needs more support to be effective. Consider factors such as sensitive tokens, or keys passed as URL parameters, or whether a control fails securely or insecurely. Nevertheless, by considering authorization at the architectural stage, you can set yourself up to make further reinforcements easier to implement.
Security basics for maximum benefit Similar to avoiding racking up technical debt by choosing a well-vetted framework, developers can avoid security debt by becoming more aware of common vulnerabilities and the simple architectural decisions you can make to help mitigate them. For a much more detailed resource on how to bake security into your applications from the start, the OWASP Application Security Verification Standard is a robust guide.
`,url:"https://victoria.dev/posts/secure-application-architecture-basics-separation-configuration-and-access/"},"https://victoria.dev/archive/migrating-to-the-cloud-but-without-screwing-it-up-or-how-to-move-house/":{title:"Migrating to the cloud but without screwing it up, or how to move house",tags:["aws","coding","data","websites"],content:`For an application that&rsquo;s ready to scale, not using managed cloud architecture these days is like insisting on digging your own well for water. It&rsquo;s far more labour-intensive, requires buying all your own equipment, takes a lot more time, and there&rsquo;s a higher chance you&rsquo;re going to get it wrong because you don&rsquo;t personally have a whole lot of experience digging wells, anyway.
That said - let&rsquo;s just get this out of the way first - there is no cloud. It&rsquo;s just someone else&rsquo;s computer.
Of course, these days, cloud services go far beyond the utility we&rsquo;d expect from a single computer. Besides being able to quickly set up and utilize the kind of computing power that previously required a new office lease agreement to house, there are now a multitude of monitoring, management, and analysis tools at our giddy fingertips. While it&rsquo;s important to understand that the cloud isn&rsquo;t a better option in every case, for applications that can take advantage of it, we can do more, do it faster, and do it for less money than if we were to insist on building our own on-premises infrastructure.
That&rsquo;s all great, and easily said; moving to the cloud, however, can look from the outset like a pretty daunting task. How, exactly, do we go about shifting what may be years of on-premises data and built-up systems to someone else&rsquo;s computer? You know, without being able to see it, touch it, and without completely screwing up our stuff.
While it probably takes less work and money than setting up or maintaining the same architecture on-premise, it does take some work to move to the cloud initially. It&rsquo;s important that our application is prepared to migrate, and capable of using the benefits of cloud services once it gets there. To accomplish this, and a smooth transition, preparation is key. In fact, it&rsquo;s a whole lot like moving to a new house.
In this article, we&rsquo;ll take a high-level look at the general stages of taking an on-premise or self-hosted application and moving it to the cloud. This guide is meant to serve as a starting point for designing the appropriate process for your particular situation, and to enable you to better understand the cloud migration process. While cloud migration may not be the best choice for some applications - such as ones without scalable architecture or where very high computing resources are needed - a majority of modular and modern applications stand to benefit from a move to the cloud.
It&rsquo;s certainly possible, as I discovered at a recent event put on by Amazon Web Services (AWS) Solutions Architects, to migrate smoothly and efficiently, with near-zero loss of availability to customers. I&rsquo;ll specifically reference some services provided by AWS, however, similar functionality can be found with other cloud providers. I&rsquo;ve found the offerings from AWS to be pleasantly modular in scope, which is why I use them myself and why they make good examples for discussing general concepts.
To have our move go as smoothly as possible, here are the things we&rsquo;ll want to consider:
The type of move we&rsquo;re making; The things we&rsquo;ll take, and the things we&rsquo;ll clean up; How to choose the right type and size for the infrastructure we&rsquo;re moving into; and How to do test runs to practice for the big day. The type of move we&rsquo;re making While it&rsquo;s important to understand why we&rsquo;re moving our application to cloud services, we should also have an idea of what we&rsquo;d like it to look like when it gets there. There are three main ways to move to the cloud: re-host, re-platform, or re-factor.
Re-host A re-host scenario is the the most straightforward type of move. It involves no change to the way our application is built or how it runs. For example, if we currently have Python code, use PostgreSQL, and serve our application with Apache, a re-host move would mean we use all the same components, combined in just the same way, only now they&rsquo;re in the cloud. It&rsquo;s a lot like moving into a new house that has the exact same floor plan as the current one. All the furniture goes into the same room it&rsquo;s in now, and it&rsquo;s going to feel pretty familiar when we get there.
The main draw of a re-host move is that it may offer the least amount of complication necessary in order to take advantage of going to the cloud. Scalable applications, for example, can gain the ability to automatically manage necessary application resources.
While re-hosting makes scaling more automatic, it&rsquo;s important to note that it won&rsquo;t in itself make an application scalable. If the application infrastructure is not organized in such a way that gives it the ability to scale, a re-factor may be necessary instead.
Re-platform If a component of our current application set up isn&rsquo;t working out well for us, we&rsquo;re probably going to want to re-platform. In this case, we&rsquo;re making a change to at least one component of our architecture; for example, switching our database from Oracle to MySQL on Amazon Relational Database Service (RDS).
Like moving from a small apartment in Tokyo to an equally small apartment in New York, a re-platform doesn&rsquo;t change the basic nature of our application, but does change its appearance and environment. In the database change example, we&rsquo;ll have all the same data, just organized or formatted a little differently. In most cases, we won&rsquo;t have to make these changes manually. A tool such as Amazon Database Migration Service (DMS) can help to seamlessly shift our data over to the new database.
We might re-platform in order to enable us to better meet a business demand in the future, such as scaling up, integrating with other technological components, or choosing a more modern technology stack.
Re-factor A move in which we re-factor our application is necessarily more complicated than our other options, however, it may provide the most overall benefit for companies or applications that have reason to make this type of move. As with code, refactoring is done when fundamental changes need to be made in order for our application to meet a business need. The specifics necessarily differ case-by-case, but typically involve changes to architectural components or how those components relate to one another. This type of move may also involve changing application code in order to optimize the application&rsquo;s performance in a cloud environment. We can think of it like moving out from our parent&rsquo;s basement in the suburbs and getting a nice townhouse in the city. There&rsquo;s no way we&rsquo;re taking that ancient hand-me-down sofa, so we&rsquo;ll need some new furniture, and for our neighbour&rsquo;s sake, probably window dressings.
Refactoring may enable us to modernize a dated application, or make it more efficient in general. With greater efficiency, we can better take advantage of services that cloud providers typically offer, like bursting resources or attaining deep analytical insight.
If a re-factor is necessary but time is scarce, it may be better to re-host or re-platform first, then re-factor later. That way, we&rsquo;ll have a job well done later instead of a hasty, botched migration (and more problems) sooner.
What to take, and what to clean up Over the years of living in one place, stuff tends to pile up unnoticed in nooks and crannies. When moving house, it&rsquo;s usually a great opportunity to sort everything out and decide what is useful enough to keep, and what should be discarded or given away. Moving to the cloud is a similarly great opportunity to do the same when it comes to our application.
While cloud storage is inexpensive nowadays, there may be some things that don&rsquo;t make sense to store any longer, or at least not keep stored with our primary application. If data cannot be discarded due to policy or regulations, we may choose a different storage class to house data that we don&rsquo;t expect to need anytime soon outside of our main application.
In the case of Amazon&rsquo;s Simple Storage Service (S3), we can choose to use different storage classes that accomplish this goal. While the data that our business relies on every day can take advantage of the Standard class 99.99% availability, data meant for long-term cold storage such as archival backups can be put into the Glacier class, which has longer retrieval time and lower cost.
The right type and size Choosing the type and size of cloud infrastructure appropriate for our business is usually the part that can be the most confusing. How should we predict, in a new environment or for a growing company, the computing power we&rsquo;ll need?
Part of the beauty of not procuring hardware on our own is that won&rsquo;t have to make predictions like these. Using cloud storage and instances, expanding or scaling back resources can be done in a matter of minutes, sometimes seconds. With managed services, it can even be done automatically for us. With the proper support for scalability in our application, it&rsquo;s like having a magical house that instantly generates any type of room and amenity we need at that moment. The ability to continually ensure that we&rsquo;re using appropriate, cost-effective resources is at our fingertips, and often clearly visualized in charts and dashboards.
For applications new to the cloud, some leeway for experimentation may be necessary. While cloud services enables us to quickly spin up and try out different architectures, there&rsquo;s no guarantee that all of those set ups will work well for our application. For example, running a single instance may be less expensive than going serverless, but we&rsquo;d be hard pressed to know this until we tried it out.
As a starting point, we simply need enough storage and computing power to support the application as it is currently running, today. For example, in the case of storage, consider the size of the current database - the actual database data, not the total storage capacity of hardware on-premises. For a detailed cost exploration, AWS even offers a Simple Monthly Calculator with use case samples to help guide expectations.
Do test runs before the big day Running a trial cloud migration may be an odd concept, but it is an essential component to ensuring that the move goes as planned with minimal service interruption. Imagine the time and energy that would be saved in the moving house example if we could automate test runs! Invariably, some box or still-hung picture is forgotten and left out of the main truck, necessitating additional trips in other vehicles. With multiple chances to ensure we&rsquo;ve got it down pat, we minimize the possibility that our move causes any break in normal day-to-day business.
Generally, to do a test run, we create a duplicate version of our application. The more we can duplicate, the more thorough the test run will be, especially if our data is especially large. Though duplication may seem tedious, working with the actual components we intend to migrate is essential to ensuring the migration goes as planned. After all, if we only did a moving-house test run with one box, it wouldn&rsquo;t be very representative.
Test runs can help to validate our migration plan against any challenges we may encounter. These challenges might include:
Downtime restrictions; Encrypting data in transit and immediately when at rest on the target; Schema conversion to a new target schema (the AWS Schema Conversion Tool can also help); Access to databases, such as through firewalls or VPNs; Developing a process to ensure that all the data successfully migrated, such as by using a hash function. Test runs also help to give us a more accurate picture of the overall time that a migration will take, as well as affording us the opportunity to fine-tune it. Factors that may affect the overall speed of a migration include:
The sizes of the source and target instances; Available bandwidth for moving data; Schema configurations; and Transaction pressure on the source, such as changes to the data and the volume of incoming transactions. Once the duplicate application has been migrated via one or more options, we test the heck out of the application that&rsquo;s now running in the cloud to ensure it performs as expected. Ideally, on the big day, we&rsquo;d follow this same general process to move up-to-date duplicate data, and then seamlessly point the &ldquo;real&rdquo; application or web address to the new location in the cloud. This means that our customers experience near-zero downtime; essentially, only the amount of time that the change in location-pointing would need to propagate to their device.
In the case of very large or complex applications with many components or many teams working together at the same time, a more gradual approach may be more appropriate than the &ldquo;Big Bang&rdquo; approach, and may help to mitigate risk of any interruptions. This means migrating in stages, component by component, and running tests between stages to ensure that all parts of the application are communicating with each other as expected.
Preparation is essential to a smooth migration I hope this article has enabled a more practical understanding of how cloud migration can be achieved. With thorough preparation, it&rsquo;s possible to take advantage of all the cloud has to offer, with minimal hassle to get there.
My thanks to the AWS Solutions Architects who presented at Pop-Up Loft and shared their knowledge on these topics, in particular: Chandra Kapireddy, Stephen Moon, John Franklin, Michael Alpaugh, and Priyanka Mahankali.
One last nugget of wisdom, courtesy of John: &ldquo;Friends don&rsquo;t let friends use DMS to create schema objects.&rdquo;
`,url:"https://victoria.dev/archive/migrating-to-the-cloud-but-without-screwing-it-up-or-how-to-move-house/"},"https://victoria.dev/archive/how-users-and-applications-stay-safe-on-the-internet-its-proxy-servers-all-the-way-down/":{title:"How users and applications stay safe on the Internet: it's proxy servers all the way down",tags:["cybersecurity","websites"],content:`Both Internet users and Internet-connected applications can benefit from investing in cybersecurity. One core aspect of online privacy is the use of a proxy server, though this basic building block may not be initially visible underneath its more recognizable forms. Proxy servers are a useful thing to know about nowadays, for developers, software product owners, as well as the average dog on the Internet. Let&rsquo;s explore what makes proxy servers an important piece of cybersecurity support.
&ldquo;On the Internet, nobody knows you&rsquo;re a dog.&rdquo;
When Peter Steiner&rsquo;s caption was first published in The New Yorker in 1993, it reportedly went largely unnoticed. Only later did the ominous and slightly frightening allusion to online anonymity touch the public consciousness with the icy fingers of the unknown. As Internet usage became more popular, users became concerned that other people could represent themselves online in any manner they chose, without anyone else knowing who they truly were.
This, to make a gross understatement, is no longer the case. Thanks to tracking cookies, browser fingerprinting, Internet Service Providers (ISPs) selling our browsing logs to advertisers, and our own inexplicable inclination to put our names and faces on social networks, online anonymity is out like last year&rsquo;s LaCroix flavours. While your next-door neighbor may not know how to find you online (well, except for through that location-based secondhand marketplace app you&rsquo;re using), you can be certain that at least one large advertising company has a series of zeroes and ones somewhere that represent you, the specific details of your market demographic, and all your online habits, including your preferred flavour of LaCroix.
There are ways to add some layers of obscurity, like using a corporate firewall that hides your IP, or using Tor. The underlying mechanism of both these methods is the same. Like being enshrouded in the layers of an onion, we&rsquo;re using one or more proxy servers to shield our slightly sulfuric selves from third-party tracking.
What&rsquo;s a proxy server, anyway A proxy, in the traditional English definition, is the &ldquo;authority or power to act for another.&rdquo; (Merriam-Webster) A proxy server, in the computing context, is a server that acts on behalf of another server, or a user&rsquo;s machine.
By using a proxy to browse the Internet, for example, a user can defer being personally identifiable. All of the user&rsquo;s Internet traffic appears to come from the proxy server instead of their machine.
Proxy servers are for users There are a few ways that we, as the client, can use a proxy server to conceal our identity when we go online. It&rsquo;s important to know that these methods offer differing levels of anonymity, and that no single method will really provide true anonymity; if others are actively seeking to find you on the Internet, for whatever reason, further steps should be taken to make your activity truly difficult to identify. (Those steps are beyond the scope of this article, but you can get started with the Electronic Frontier Foundation&rsquo;s (EFF) Surveillance Self-Defense resource.) For the average user, however, here is a small menu of options ranging from least to most anonymous.
Use a proxy in your web browser Certain web browsers, including Firefox and Safari on Mac, allow us to configure them to send our Internet traffic through a proxy server. The proxy server attempts to anonymize our requests by replacing our originating IP address with the proxy server&rsquo;s own IP. This provides us with some anonymity, as the website we&rsquo;re trying to reach will not see our originating IP address; however, the proxy server that we choose to use will know exactly who originated the request. This method also doesn&rsquo;t necessarily encrypt traffic, block cookies, or stop social media and cross-site trackers from following us around; on the upside, it&rsquo;s the method least likely to prevent websites that use cookies from functioning properly.
Public proxy servers are out there, and deciding whether or not we should use any one of them is on par with deciding whether we should eat a piece of candy handed to us by a smiling stranger. If your academic institution or company provides a proxy server address, it is (hopefully) a private server with some security in place. My preferred method, if we have a little time and a few monthly dollars to invest in our security, is to set up our own virtual instance with a company such as Amazon Web Services or Digital Ocean and use this as our proxy server.
To use a proxy through our browser, we can edit our Connection Settings in Firefox, or set up a proxy server using Safari on Mac.
In regards to choosing a browser, I would happily recommend Firefox to any Internet user who wants to beef up the security of their browsing experience right out of the box. Mozilla has been a champion of privacy-first since I&rsquo;ve heard of them, and recently made some well-received changes to Enhanced Tracking Protection in Firefox Browser that blocks social media trackers, cross-site tracking cookies, fingerprinters, and cryptominers by default.
Use a VPN on your device In order to take advantage of a proxy server for all our Internet usage instead of just through one browser, we can use a Virtual Private Network (VPN). A VPN is a service, usually paid, that sends our Internet traffic through their servers, thus acting as a proxy. A VPN can be used on our laptop as well as phone and tablet devices, and since it encompasses all our Internet traffic, it doesn&rsquo;t require much extra effort to use other than ensuring our device is connected. Using a VPN is an effective way to keep nosy ISPs from snooping on our requests.
To use a paid, third-party VPN service, we&rsquo;d usually sign up on their website and download their app. It&rsquo;s important to keep in mind that whichever provider we choose, we&rsquo;re entrusting them with our data. VPN providers anonymize our activity from the Internet, but can themselves see all our requests. Providers vary in terms of their privacy policies and the data they choose to log, so a little research may be necessary to determine which, if any, we are comfortable trusting.
We can also roll our own VPN service by using a virtual instance and OpenVPN. OpenVPN is an open source VPN protocol, and can be used with a few virtual instance providers, such as Amazon VPC, Microsoft Azure, Google Cloud, and Digital Ocean Droplets. I previously wrote a tutorial on setting up your own personal VPN service with AWS using an EC2 instance. I&rsquo;ve been running this solution personally for about a month, and it&rsquo;s cost me almost $4 USD in total, which is a price I&rsquo;m quite comfortable paying for some peace of mind.
Use Tor Tor takes the anonymity offered by a proxy server and compounds it by forwarding our requests through a relay network of other servers, each called a &ldquo;node.&rdquo; Our traffic passes through three nodes on its way to a destination: the guard, middle, and exit nodes. At each step, the request is encrypted and anonymized such that the current node only knows where to send it, and nothing more about what the request contains. This separation of knowledge means that, of the options discussed, Tor provides the most complete version of anonymity. (For a more complete explanation, see Robert Heaton&rsquo;s article on how Tor works, which is so excellently done that I wish I&rsquo;d written it myself.)
That said, this level of anonymity comes with its own cost. Not monetary, as Tor Browser is free to download and use. It is, however, slower than using a VPN or simple proxy server through a browser, due to the circuitous route our requests take.
Proxy servers are for servers too We&rsquo;re now familiar with proxy servers in the context of protecting users as they surf the web, but proxies aren&rsquo;t just for clients. Websites and Internet-connected applications can use reverse proxy servers for obfuscation too. The &ldquo;reverse&rdquo; part just means that the proxy is acting on behalf of the server, instead of the client.
Why would a web server care about anonymity? Generally, they don&rsquo;t, at least not in the same way some users do. Web servers can benefit from using a proxy for a few different reasons; for example, they typically offer faster service to users by caching or compressing content to optimize delivery. From a cybersecurity perspective, however, a reverse proxy can improve an application&rsquo;s security posture by obfuscating the underlying infrastructure.
Basically, by placing another web server (the &ldquo;proxy&rdquo;) in front of the web server that directly accesses all the files and assets, we make it more difficult for an attacker to pinpoint our &ldquo;real&rdquo; web server and mess with our stuff. Like when you want to see the store manager and the clerk you&rsquo;re talking to says, &ldquo;I speak for the manager,&rdquo; and you&rsquo;re not really sure there even is a manager, anyway, but you successfully exchange the hot pink My Little Pony they sold you for a fuchsia one, thankyouverymuch, so now you&rsquo;re no longer concerned with who the manager is and whether or not they really exist, and if you passed them on the street you would not be able to stop them and call them out for passing off hot pink as fuchsia, and the manager is just fine with that.
Some common web servers can also act as reverse proxies, often with just a minimal and straightforward configuration change. While the best choice for your particular architecture is unknown to me, I will offer a couple common examples here.
Using NGINX as a reverse proxy NGINX uses the proxy_pass directive in its configuration file (nginx.conf by default) to turn itself into a reverse proxy server. The set up requires the following lines to be placed in the configuration file:
location /requested/path/ { proxy_pass http://www.example.com/target/path/; } This specifies that all requests for the path /requested/path/ are forwarded to http://www.example.com/target/path/. The target can be a domain name or an IP address, the latter with or without a port.
The full guide to using NGINX as a reverse proxy is part of the NGINX documentation.
Using Apache httpd as a reverse proxy Apache httpd similarly requires some straightforward configuration to act as a reverse proxy server. In the configuration file, usually httpd.conf, set the following directives:
ProxyPass &#34;/requested/path/&#34; &#34;http://www.example.com/target/path/&#34; ProxyPassReverse &#34;/requested/path/&#34; &#34;http://www.example.com/target/path/&#34; The ProxyPass directive ensures that all requests for the path /requested/path/ are forwarded to http://www.example.com/target/path/. The ProxyPassReverse directive ensures that the headers sent by the web server are modified to point to the reverse proxy server instead.
The full reverse proxy guide for Apache HTTP server is available in their documentation.
Proxy servers most of the way down I concede that my title is a little facetious, as cybersecurity best practices aren&rsquo;t really some eternal infinite-regression mystery (though they may sometimes seem to be). Regardless, I hope this post has helped in your understanding of what proxy servers are, how they contribute to online anonymity for both clients and servers, and that they are an integral building block of cybersecurity practices.
If you&rsquo;d like to learn more about personal best practices for online security, I highly recommend exploring the articles and resources provided by EFF. For a guide to securing web sites and applications, the OWASP Cheat Sheet Series is a fantastic resource.
`,url:"https://victoria.dev/archive/how-users-and-applications-stay-safe-on-the-internet-its-proxy-servers-all-the-way-down/"},"https://victoria.dev/archive/hackers-are-googling-your-plain-text-passwords-preventing-sensitive-data-exposure/":{title:"Hackers are Googling your plain text passwords: preventing sensitive data exposure",tags:["cybersecurity","privacy","data","websites"],content:`Last week, I wrote about the importance of properly handling user input in our websites and applications. I alluded to an overarching security lesson that I hope to make explicit today: the security of our software, application, and customer data is built from the ground up, long before the product goes live.
The OWASP Top 10 is a comprehensive guide to web application security risks. It is relied upon by technology professionals, corporations, and those interested in cybersecurity or information security. The most recent publication lists Sensitive Data Exposure as the third most critical web application security risk. Here&rsquo;s how the risk is described:
Many web applications and APIs do not properly protect sensitive data, such as financial, healthcare, and PII. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data may be compromised without extra protection, such as encryption at rest or in transit, and requires special precautions when exchanged with the browser.
&ldquo;Sensitive Data Exposure&rdquo; is a sort of catch-all category for leaked data resulting from many sources, ranging from weak cryptographic algorithms to unenforced encryption. The simplest source of this security risk, however, takes far fewer syllables to describe: people.
The phrase &ldquo;an ounce of prevention is worth a pound of cure,&rdquo; applies to medicine as well as secure software development. In the world of the latter, this is referred to as &ldquo;pushing left,&rdquo; a rather unintuitive term for establishing security best practices earlier, rather than later, in the software development life cycle (SDLC). Establishing procedures &ldquo;to the left&rdquo; of the SDLC can help ensure that the people involved in creating a software product are properly taking care of sensitive data from day one.
Unfortunately, a good amount of security testing often seems to occur much farther to the right side of the SDLC; too late for some security issues, such as sensitive data leakage, to be prevented.
I&rsquo;m one of the authors contributing to the upcoming OWASP Testing Guide and recently expanded a section on search engine discovery reconnaissance, or what the kids these days call &ldquo;Google dorking.&rdquo; This is one method, and arguably the most accessible method, by which a security tester (or black hat hacker) could find exposed sensitive data on the Internet. Here&rsquo;s an excerpt from that section (currently a work in progress on GitHub, to be released in v5):
Search Operators A search operator is a special keyword that extends the capabilities of regular search queries, and can help obtain more specific results. They generally take the form of operator:query. Here are some commonly supported search operators:
site: will limit the search to the provided URL. inurl: will only return results that include the keyword in the URL. intitle: will only return results that have the keyword in the page title. intext: or inbody: will only search for the keyword in the body of pages. filetype: will match only a specific filetype, i.e. png, or php. For example, to find the web content of owasp.org as indexed by a typical search engine, the syntax required is:
site:owasp.org
&hellip; Searching with operators can be a very effective discovery reconnaissance technique when combined with the creativity of the tester. Operators can be chained to effectively discover specific kinds of sensitive files and information. This technique, called Google hacking or Google dorking, is also possible using other search engines, as long as the search operators are supported.
A database of dorks, such as Google Hacking Database, is a useful resource that can help uncover specific information.
Regularly reviewing search engine results can be a fruitful task for security testers. However, when a search for site:myapp.com passwords turns up no results, it may still be a little too early to break for lunch. Here are a couple other places a security tester might like to look for sensitive data exposed in the wild.
Pastebin The self-declared &ldquo;#1 paste tool since 2002,&rdquo; Pastebin allows users to temporarily store any kind of text. It&rsquo;s mostly used for sharing information with others, or retrieving your own &ldquo;paste&rdquo; on another machine, perhaps in another location. Pastebin makes it easy to share large amounts of complicated text, like error logs, source code, configuration files, tokens, api keys&hellip; what&rsquo;s that? Oh, yes, it&rsquo;s public by default.
Here are some screenshots of a little dorking I did for a public bug bounty program.
API keys in plain view.
Log-in details out in the open.
Thanks in part to the convenience of using Pastebin and similar websites, it would appear that some people fail to think twice before making sensitive data publicly available.
But why Granted, non-technical employees with access to the application may not have an understanding of which items should or should not be freely shared. Someone unfamiliar with what encrypted data is or what it looks like may not realize the difference between an encrypted string and an unencrypted token made up of many random letters and numbers. Even technical staff can miss things, make mistakes, or act carelessly after a hard day at work. It may be easy to call this a training problem and move on; however, none of these rationalizations address the root cause of the issue.
When people turn to outside solutions for an issue they face, it&rsquo;s usually because they haven&rsquo;t been provided with an equally-appealing internal solution, or are unaware that one exists. Employees using pastes to share or move sensitive data do so because they don&rsquo;t have an easier, more convenient, and secure internal solution to use instead.
Mitigation Everyone involved in the creation and maintenance of a web application should be briefed on a few basic things in regards to sensitive data protection:
what constitutes sensitive data, the difference between plain text and encrypted data, and how to properly transmit and store sensitive data. When it comes to third-party services, ensure people are aware that some transmission may not be encrypted, or may be publicly searchable. If there is no system currently in place for safely sharing and storing sensitive data internally, this is a good place to start. The security of application data is in the hands of everyone on the team, from administrative staff to C-level executives. Ensure people have the tools they need to work securely.
Public repositories Developers are notorious for leaving sensitive information hanging out where it doesn&rsquo;t belong (yes, I&rsquo;ve done it too!). Without a strong push-left approach in place for handling tokens, secrets, and keys, these little gems can end up in full public view on sites like GitHub, GitLab, and Bitbucket (to name a few). A 2019 study found that thousands of new, unique secrets are leaked every day on GitHub alone.
GitHub has implemented measures like token scanning, and GitLab 11.9 introduced secret detection. While these tools aim to reduce the chances that a secret might accidentally be committed, to put it bluntly, it&rsquo;s really not their job. Secret scanning won&rsquo;t stop developers from committing the data in the first place.
But why Without an obvious process in place for managing secrets, developers may tend too much towards their innate sense of just-get-it-done-ness. Sometimes this leads to the expedient but irresponsible practice of storing keys as unencrypted variables within the program, perhaps with the intention of it being temporary. Nonetheless, these variables inevitably fall from front of mind and end up in a commit.
Mitigation Having a strong push-left culture means ensuring that sensitive data is properly stored and can be securely retrieved long before anyone is ready to make a commit. Tools and strategies for doing so are readily available for those who seek them. Here are some examples of tools that can support a push-left approach:
Use a management tool to store and control access to keys and secrets, such as Amazon Key Management Service or Microsoft&rsquo;s Azure Key Vault. Make use of encrypted environment variables in CI tools, such as Netlify&rsquo;s environment variables or virtual environments in GitHub Actions. Craft a robust .gitignore file that everyone on the team can contribute to and use. We also need not rely entirely on the public repository to catch those mistakes that may still slip through. It&rsquo;s possible to set up Git pre-commit hooks that scan for committed secrets using regular expressions. There are some open-source programs available for this, such as Talisman from ThoughtWorks and git-secrets from AWS Labs.
Pushing left to prevent sensitive data exposure A little perspective can go a long way in demonstrating why it&rsquo;s important to begin managing sensitive data even before any sensitive data exists. By establishing security best practices on the left of the SDLC, we give our people the best chance to increase the odds that any future dorking on our software product looks more like this.
Another great resource for checking up on the security of our data is Troy Hunt&rsquo;s Have I Been Pwned, a service that compares your data (such as your email) to data that has been leaked in previous data breaches.
To learn about more ways we can be proactive with our application security, the OWASP Proactive Controls publication is a great resource. There&rsquo;s also more about creating a push-left approach to security in the upcoming OWASP Testing Guide. If these topics interest you, I encourage you to read, learn, and contribute so more people will make it harder for sensitive data to be found.
`,url:"https://victoria.dev/archive/hackers-are-googling-your-plain-text-passwords-preventing-sensitive-data-exposure/"},"https://victoria.dev/archive/sql-injection-and-xss-what-white-hat-hackers-know-about-trusting-user-input/":{title:"SQL injection and XSS: what white hat hackers know about trusting user input",tags:["cybersecurity","data","websites"],content:`Software developers have a lot on their minds. There are are myriad of questions to ask when it comes to creating a website or application: What technologies will we use? How will the architecture be set up? What functions do we need? What will the UI look like? Especially in a software market where shipping new apps seems more like a race for reputation than a well-considered process, one of the most important questions often falls to the bottom of the &ldquo;Urgent&rdquo; column: how will our product be secured?
If you&rsquo;re using a robust, open-source framework for building your product (and if one is applicable and available, why wouldn&rsquo;t you?) then some basic security concerns, like CSRF tokens and password encryption, may already be handled for you. Still, fast-moving developers would be well served to brush up on their knowledge of common threats and pitfalls, if only to avoid some embarrass ing rookie mistakes. Usually, the weakest point in the security of your software is you.
I&rsquo;ve recently become more interested in information security in general, and practicing ethical hacking in particular. An ethical hacker, sometimes called &ldquo;white hat&rdquo; hacker, and sometimes just &ldquo;hacker,&rdquo; is someone who searches for possible security vulnerabilities and responsibly (privately) reports them to project owners. By contrast, a malicious or &ldquo;black hat&rdquo; hacker, also called a &ldquo;cracker,&rdquo; is someone who exploits these vulnerabilities for amusement or personal gain. Both white hat and black hat hackers might use the same tools and resources, and generally try to get into places they aren&rsquo;t supposed to be; however, white hats do this with permission, and with the intention of fortifying defences instead of destroying them. Black hats are the bad guys.
When it comes to learning how to find security vulnerabilities, it should come as no surprise that I&rsquo;ve been devouring whatever information I can get my hands on; this post is a distillation of some key areas that are specifically helpful to developers when handling user input. These lessons have been collectively gleaned from these excellent resources:
The Open Web Application Security Project guides The Hacker101 playlist from HackerOne&rsquo;s YouTube channel Web Hacking 101 by Peter Yaworski Brute Logic&rsquo;s blog The Computerphile YouTube channel Videos featuring Jason Haddix (@jhaddix) and Tom Hudson (@tomnomnom) (two accomplished ethical hackers with different, but both effective, methodologies) You may be familiar with the catchphrase, &ldquo;sanitize your inputs!&rdquo; However, as I hope this post demonstrates, developing an application with robust security isn&rsquo;t quite so straightforward. I suggest an alternate phrase: pay attention to your inputs. Let&rsquo;s elaborate by examining the most common attacks that take advantage of vulnerabilities in this area: SQL injection and cross site scripting.
SQL injection attacks If you&rsquo;re not yet familiar with SQL (Structured Query Language) injection attacks, or SQLi, here is a great explain-like-I&rsquo;m-five video on SQLi. You may already know of this attack from xkcd&rsquo;s Little Bobby Tables. Essentially, malicious actors may be able to send SQL commands that affect your application through some input on your site, like a search box that pulls results from your database. Sites coded in PHP can be especially susceptible to these, and a successful SQL attack can be devastating for software that relies on a database (as in, your Users table is now a pot of petunias).
You have no chance to survive make your time.
You can test your own site to see if you&rsquo;re susceptible to this kind of attack. (Please only test sites that you own, since running SQL injections where you don&rsquo;t have permission to be doing so is, possibly, illegal in your locality; and definitely, universally, not very funny.) The following payloads can be used to test inputs:
' OR 1='1 evaluates to a constant true, and when successful, returns all rows in the table. ' AND 0='1 evaluates to a constant false, and when successful, returns no rows. This video demonstrates the above tests, and does a great job of showing how impactful an SQL injection attack can be.
Thankfully, there are ways to mitigate SQL injection attacks, and they all boil down to one basic concept: don&rsquo;t trust user input.
SQL injection mitigation In order to effectively mitigate SQL injections, developers must prevent users from being able to successfully submit raw SQL commands to any part of the site.
Some frameworks will do most of the heavy lifting for you. For example, Django implements the concept of Object-Relational Mapping, or ORM, with its use of QuerySets. We can think of these as wrapper functions that help your application query the database using pre-defined methods that avoid the use of raw SQL.
Being able to use a framework, however, is never a guarantee. When dealing directly with a database, there are other methods we can use to safely abstract our SQL queries from user input, though they vary in efficacy. These are, by order of most to least preferred, and with links to relevant examples:
Prepared statements with variable binding (or parameterized queries), Stored procedures; and Whitelisting or escaping user input. If you want to implement the above techniques, the linked cheatsheets are a great starting point for digging deeper. Suffice to say, the use of these techniques to obtain data instead of using raw SQL queries helps to minimize the chances that SQL will be processed by any part of your application that takes input from users, thus mitigating SQL injection attacks.
The battle, however, is only half won&hellip;
Cross Site Scripting (XSS) attacks If you&rsquo;re a malicious coder, JavaScript is pretty much your best friend. The right commands will do anything a legitimate user could do (and even some things they aren&rsquo;t supposed to be able to) on a web page, sometimes without any interaction on the part of an actual user. Cross Site Scripting attacks, or XSS, occur when JavaScript code is injected into a web page and changes that page&rsquo;s behavior. Its effects can range from prank nuisance occurrences to more severe authentication bypasses or credential stealing.
The annual DOM dance-off receives an unexpected guest);
XSS can occur on the server or on the client side, and generally comes in three flavors: DOM (Document Object Model) based, stored, and reflected XSS. The differences amount to where the attack payload is injected into the application.
DOM-based XSS DOM-based XSS occurs when a JavaScript payload affects the structure, behavior, or content of the web page the user has loaded in their browser. These are most commonly executed through modified URLs, such as in phishing.
To see how easy it would be for injected JavaScript to manipulate a page, we can create a working example with an HTML web page. Try creating a file on your local system called xss-test.html (or whatever you like) with the following HTML and JavaScript code:
&lt;html&gt; &lt;head&gt; &lt;title&gt;My XSS Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1 id=&#34;greeting&#34;&gt;Hello there!&lt;/h1&gt; &lt;script&gt; var name = new URLSearchParams(document.location.search).get(&#39;name&#39;); if (name !== &#39;null&#39;) { document.getElementById(&#39;greeting&#39;).innerHTML = &#39;Hello &#39; + name + &#39;!&#39;; } &lt;/script&gt; &lt;/h1&gt; &lt;/html&gt; This web page will display the title &ldquo;Hello there!&rdquo; unless it receives a URL parameter from a query string with a value for name. To see the script work, open the page in a browser with an appended URL parameter, like so:
file:///path/to/file/xss-test.html?name=Victoria
Fun, right? Our insecure (in the safety sense, not the emotional one) page takes the URL parameter value for name and displays it in the DOM. The page is expecting the value to be a nice friendly string, but what if we change it to something else? Since the page is owned by us and only exists on our local system, we can test it all we like. What happens if we change the name parameter to, say, &lt;img+src+onerror=alert(&quot;pwned&quot;)&gt;?
This is just one example, largely based on one from Brute&rsquo;s post, that demonstrates how an XSS attack could be executed. Funny pop-up alerts may be amusing, but JavaScript can do a lot of harm, including helping malicious attackers steal passwords and personal information.
Stored and reflected XSS Stored XSS occurs when the attack payload is stored on the server, such as in a database. The attack affects a victim whenever that stored data is retrieved and rendered in the browser. For example, instead of using a URL query string, an attacker might update their profile page on a social site to include a hidden script in, say, their &ldquo;About Me&rdquo; section. The script, improperly stored on the site&rsquo;s server, would successfully execute at a later time when another user views the attacker&rsquo;s profile.
One of the most famous examples of this is the Samy worm that all but took over MySpace in 2005. It propagated by sending HTTP requests that replicated it onto a victim&rsquo;s profile page whenever an infected profile was viewed. Within just 20 hours, it had spread to over a million users.
Reflected XSS similarly occurs when the injected payload travels to the server, however, the malicious code does not end up stored in a database. It is instead immediately returned to the browser by the web application. An attack like this might be executed by luring the victim to click a malicious link that sends a request to the vulnerable website&rsquo;s server. The server would then send a response to the attacker as well as the victim, which may result in the attacker being able to obtain passwords, or perpetrate actions that appear to originate from the victim.
XSS attack mitigation In all of these cases, XSS attacks can be mitigated with two key strategies: validating form fields, and avoiding the direct injection of user input on the web page.
Validating form fields Frameworks can again help us out when it comes to making sure that user-submitted forms are on the up-and-up. One example is Django&rsquo;s built-in Field classes, which provide fields that validate to some commonly used types and also specify sane defaults. Django&rsquo;s EmailField, for instance, uses a set of rules to determine if the input provided is a valid email. If the submitted string has characters in it that are not typically present in email addresses, or if it doesn&rsquo;t imitate the common format of an email address, then Django won&rsquo;t consider the field valid and the form will not be submitted.
If relying on a framework isn&rsquo;t an option, we can implement our own input validation. This can be accomplished with a few different techniques, including type conversion, for example, ensuring that a number is of type int(); checking minimum and maximum range values for numbers and lengths for strings; using a pre-defined array of choices that avoids arbitrary input, for example, months of the year; and checking data against strict regular expressions.
Thankfully, we needn&rsquo;t start from scratch. Open source resources are available to help, such as the OWASP Validation Regex Repository, which provides patterns to match against for some common forms of data. Many programming languages offer validation libraries specific to their syntax, and we can find plenty of these on GitHub. Additionally, the XSS Filter Evasion Cheat Sheet has a couple suggestions for test payloads we can use to test our existing applications.
While it may seem tedious, properly implemented input validation can protect our application from being susceptible to XSS.
Avoiding direct injection Elements of an application that directly return user input to the browser may not, on a casual inspection, be obvious. We can determine areas of our application that may be at risk by exploring a few questions:
How does data flow through our application? What does a user expect to happen when they interact with this input? Where on our page does data appear? Does it become embedded in a string or an attribute? Here are some sample payloads that we can play with in order to test inputs on our site (again, only our own site!) courtesy of Hacker101. The successful execution of any of these samples can indicate a possible XSS vulnerability due to direct injection.
&quot;&gt;&lt;h1&gt;test&lt;/h1&gt; '+alert(1)+' &quot;onmouserover=&quot;alert(1) http://&quot;onmouseover=&quot;alert(1) As a general rule, if you are able to design around directly injecting input, do so. Alternatively, be sure to completely understand the effect of the methods you choose; for example, using innerText instead of innerHTML in JavaScript will ensure that content will be set as plain text instead of (potentially vulnerable) HTML.
Pay attention to your inputs Software developers are at a marked disadvantage when it comes to competing with black hat, or malicious, hackers. For all the work we do to secure each and every input that could potentially compromise our application, an attacker need only find the one we missed. It&rsquo;s like installing deadbolts on all the doors, but leaving a window open!
By learning to think along the same lines as an attacker, however, we can better prepare our software to stand up against bad actors. Exciting as it may be to ship features as quickly as possible, we&rsquo;ll avoid racking up a lot of security debt if we take the time beforehand to think through our application&rsquo;s flow, follow the data, and pay attention to our inputs.
`,url:"https://victoria.dev/archive/sql-injection-and-xss-what-white-hat-hackers-know-about-trusting-user-input/"},"https://victoria.dev/archive/how-to-set-up-openvpn-on-aws-ec2-and-fix-dns-leaks-on-ubuntu-18.04-lts/":{title:"How to set up OpenVPN on AWS EC2 and fix DNS leaks on Ubuntu 18.04 LTS",tags:["aws","cybersecurity","linux"],content:`There&rsquo;s no better way to strive for maximum privacy than a VPN service you control, configure, and maintain yourself. Here&rsquo;s a step-by-step tutorial for setting up your own OpenVPN on AWS EC2, and how to check for and fix DNS leaks.
For a VPN that also blocks ads and trackers, you can set up a Pi-hole VPN on an AWS Lightsail instance instead.
Set up OpenVPN on AWS EC2 This post will cover how to set up the OpenVPN Access Server product on AWS Marketplace, running on an Amazon EC2 instance. Then, you&rsquo;ll look at how to fix a known NetworkManager bug in Ubuntu 18.04 that might cause DNS leaks. The whole process should take about fifteen minutes, so grab a ☕ and let&rsquo;s be configuration superheroes.
Note: IDs and IP addresses shown for demonstration in this tutorial are invalid.
1. Launch the OpenVPN Access Server on AWS Marketplace The OpenVPN Access Server is available on AWS Marketplace. The Bring Your Own License (BYOL) model doesn&rsquo;t actually require a license for up to two connected devices; to connect more clients, you can get bundled billing for five, ten, or twenty-five clients, or purchase a minimum of ten OpenVPN licenses a la carte for $15/device/year. For most of us, the two free connected devices will suffice; and if using an EC2 Micro instance, your set up will be AWS Free Tier eligible as well.
Start by clicking Continue to Subscribe for the OpenVPN Access Server, which will bring you to a page that looks like this:
Click Continue to Configuration.
You may notice that the EC2 instance type in the right side bar (and consequently, the Monthly Estimate) isn&rsquo;t the one you want - that&rsquo;s okay, you can change it soon. Just ensure that the Region chosen is where you want the instance to be located. Generally, the closer it is to the physical location of your client (your laptop, in this case), the faster your VPN will be. Click Continue to Launch.
On this page, you&rsquo;ll change three things:
1. The EC2 Instance type Different types of EC2 (Elastic Compute Cloud) instances will offer you different levels of computing power. If you plan to use your instance for something more than just this VPN, you may want to choose something with higher memory or storage capacity, depending on how you plan to use it. You can view each instance offering on the Amazon EC2 Instance Types page.
For simple VPN use, the t2.nano or t2.micro instances are likely sufficient. Only the Micro instance is Free Tier eligible.
2. The Security Group settings A Security Group is a profile, or collection of settings, that Amazon uses to control access to your instance. If you&rsquo;ve set up other AWS products before, you may already have some groups with their own rules defined. You should be careful to understand the reasons for your Security Group settings, as these define how public or private your instance is, and consequently, who has access to it.
If you click Create New Based on Seller Settings, the OpenVPN server defines some recommended settings for a default Security Group.
The default recommended settings are all 0.0.0.0/0 for TCP ports 22, 943, 443, and 945, and UDP port 1194. OpenVPN offers an explanation of how the ports are used on their website. With the default settings, all these ports are left open to support various features of the OpenVPN server. You may wish to restrict access to these ports to a specific IP address or block of addresses (like that of your own ISP) to increase the security of your instance. However, if your IP address frequently changes (like when you travel and connect to a different WiFi network), restricting the ports may not be as helpful as you hope.
In any case, your instance will require SSH keys to connect to, and the OpenVPN server will be password protected. Unless you have other specific security goals, it&rsquo;s fine to accept the default settings for now.
Let&rsquo;s give the Security Group a name and brief description, so you know what it&rsquo;s for. Then click Save.
3. The Key Pair settings The aforementioned SSH keys are access credentials that you&rsquo;ll use to connect to your instance. You can create a key pair in this section, or you can choose a key pair you may already be using with AWS.
To create a new set of access credentials, click Create a key pair in EC2 to open a new window. Then, click the Create Key Pair blue button. Once you give your key pair a name, it will be created and the private key will automatically download to your machine. It&rsquo;s a file ending with the extension .pem. Store this key in a secure place on your computer. You&rsquo;ll need to refer to it when you connect to your new EC2 instance.
You can return to the previous window to select the key pair you just created. If it doesn&rsquo;t show up, hit the little &ldquo;refresh&rdquo; icon next to the drop-down. Once it&rsquo;s selected, hit the shiny yellow Launch button.
You should see a message like this:
Great stuff! Now that your instance exists, let&rsquo;s make sure you can access it and start up your VPN. For a shortcut to the next step, click on the &ldquo;EC2 Console&rdquo; link in the success message.
2. Associate an Elastic IP Amazon&rsquo;s Elastic IP Addresses provides you with a public IPv4 address controlled by your account, unlike the public IP address tied to your EC2 instance. It&rsquo;s considered a best practice to create one and associate it with your VPN instance. If anything should go wrong with your instance, or if you want to use a new instance for your VPN in the future, the Elastic IP can be disassociated from the current instance and reassociated with your new one. This makes the transition seamless for your connected clients. Think of the Elastic IP like a web domain address that you register - you can point it at whatever you choose.
We can create a new Elastic IP address on the Amazon EC2 Console. If you clicked the link from the success message above, we&rsquo;re already there.
If you have more than one instance, take note of the Instance ID of the one you&rsquo;ve just launched.
In the left sidebar under Network &amp; Security, choose Elastic IPs. Then click the blue Allocate new address button.
Choose Amazon Pool, then click Allocate.
Success! Click Close to return to the Elastic IP console.
Now that you have an Elastic IP, let&rsquo;s associate it with your instance. Select the IP address, then click Actions, and choose Associate address.
Ensure the Instance option is selected, then click the drop-down menu. You should see your EC2 instance ID there. Select it, then click Associate.
Success! Now that you&rsquo;ll be able to access your VPN instance, let&rsquo;s get your VPN service up and running.
3. Initialize OpenVPN on the EC2 server First, you&rsquo;ll need to connect to the EC2 instance via your terminal. You&rsquo;ll use the private key you created earlier.
Open a new terminal window and navigate to the directory containing the private key .pem file. You&rsquo;ll need to set its permissions with:
sudo chmod 400 &lt;name&gt;.pem Be sure to substitute &lt;name&gt; with the name of your key.
This sets the file permissions to -r-------- so that it can only be read by the user (you). It may help to protect the private key from read and write operations by other users, but more importantly, will prevent AWS from throwing an error when you try to connect to your instance.
We can now do just that by running:
ssh -i &lt;name&gt;.pem openvpnas@&lt;elastic ip&gt; The user openvpnas is set up by the OpenVPN Access Server to allow you to connect to your instance. Replace &lt;elastic ip&gt; with the Elastic IP address you just associated.
We may get a message saying that the authenticity of your host can&rsquo;t be established. As long as you&rsquo;ve typed the Elastic IP correctly, go ahead and answer yes to the prompt.
Upon the initial connection to the OpenVPN instance, a set up wizard called Initial Configuration Tool should automatically run. (If, for some reason, it doesn&rsquo;t, or you panic-mashed a button, you can restart it with sudo ovpn-init –ec2.) You&rsquo;ll be asked to accept the agreement, then the wizard will help to walk you through some configuration settings for your VPN server.
You may generally accept the default settings, however, there are a couple questions you may like to answer knowledgeably. They are:
Should client traffic be routed by default through the VPN?
Should client DNS traffic be routed by default through the VPN?
These answers depend on your privacy goals for your VPN.
When asked for your OpenVPN-AS license key, you can leave it blank to use the VPN with up to two clients. If you&rsquo;ve purchased a key, enter it here.
Once the configuration wizard finishes running, you should see the message &ldquo;Initial Configuration Complete!&rdquo; Before you move on, you should set a password for your server&rsquo;s administration account. To do this, run:
sudo passwd openvpn Then enter your chosen password twice. Now we&rsquo;re ready to get connected!
To close the SSH connection, type exit.
4. Connect the client to the VPN To connect your client (in this case, your laptop) to the VPN and start reaping the benefits, you&rsquo;ll need to do two things; first, obtain your connection profile; second, install the openvpn daemon.
1. Get your .ovpn connection profile You&rsquo;ll need to download a connection profile; this is like a personal configuration file with information, including keys, that the VPN server will need to allow your connection. You can do this by logging in with the password you just set at your Elastic IP address, port 943. This looks like:
https://&lt;elastic ip&gt;:943/ The https part is important; without it, the instance won&rsquo;t send any data.
When you go to this URL, you may see a page warning you that this site&rsquo;s certificate issuer is unknown or invalid. As long as you&rsquo;ve typed your Elastic IP correctly, it&rsquo;s safe to proceed. If you&rsquo;re using Firefox, click Advanced, and then Accept the Risk and Continue. In Chrome, click Advanced, then Proceed to the elastic IP.
Log in with the username openvpn and the password you just set. You&rsquo;ll now be presented with a link to download your user-locked connection profile:
When you click the link, a file named client.ovpn will download.
2. Install and start openvpn on your Ubuntu 18.04 client The openvpn daemon will allow your client to connect to your VPN server. It can be installed through the default Ubuntu repositories. Run:
sudo apt install openvpn In order for OpenVPN to automatically start when you boot up your computer, you&rsquo;ll need to rename and move the connection profile file. I suggest using a symlink to accomplish this, as it leaves your original file more easily accessible for editing, and allows you to store it in any directory you choose. You can create a symlink by running this command in the directory where your file is located:
sudo ln -s client.ovpn /etc/openvpn/&lt;name&gt;.conf This creates a symbolic link for the connection profile in the appropriate folder for systemd to find it. The &lt;name&gt; can be anything. When the Linux kernel has booted, systemd is used to initialize the services and daemons that the user has set up to run; one of these will now be OpenVPN. Renaming the file with the extension .conf will let the openvpn daemon know to use it as your connection file.
For now, you can manually start and connect to OpenVPN by running:
sudo openvpn --config client.ovpn You&rsquo;ll be asked for a username and password, which will be the same credentials you used before. Once the service finishes starting up, you&rsquo;ll see &ldquo;Initialization Sequence Complete.&rdquo; If you now visit the DNS leak test website, you should see the Elastic IP and the location of your EC2 server. Yay!
If you&rsquo;re on a later version of Ubuntu, you may check for DNS leaks by clicking on one of the test buttons. If all the ISPs shown are Amazon and none are your own service provider&rsquo;s, congratulations! No leaks! You can move on to Step 3 in the second section below, after which, you&rsquo;ll be finished.
If you&rsquo;re using Ubuntu 18.04 LTS, however, we&rsquo;re not yet done.
What a DNS leak looks like Sites like the DNS leak test website can help you check your configuration and see if the Internet knows more about your location than you&rsquo;d like. On the main page you&rsquo;ll see a big hello, your IP address, and your location, so far as can be determined.
If you have a DNS leak, you can see what it looks like by clicking on one of the test buttons on the the DNS leak test page. When you do, you&rsquo;ll see not only your Amazon.com IP addresses, but also your own ISP and location.
You can also see the leak by running systemd-resolve --status in your terminal. Your results will contain two lines under different interfaces that both have entries for DNS Servers. It&rsquo;ll look something like this:
Link 7 (tun0) Current Scopes: DNS LLMNR setting: yes MulticastDNS setting: no DNSSEC setting: no DNSSEC supported: no DNS Servers: 172.31.0.2 DNS Domain: ~. Link 3 (wlp4s0) Current Scopes: none LLMNR setting: yes MulticastDNS setting: no DNSSEC setting: no DNSSEC supported: no DNS Servers: 192.168.0.1 DNS Domain: ~. The DNS leak problem in Ubuntu 18.04 stems from Ubuntu&rsquo;s DNS resolver, systemd-resolved, failing to properly handle your OpenVPN configuration. In order to try and be a good, efficient DNS resolver, systemd-resolved will send DNS lookup requests in parallel to each interface that has a DNS server configuration, and then utilizes the fastest response. In your case, you only want to use your VPN&rsquo;s DNS servers. Sorry, systemd-resolved. You tried.
How to fix OpenVPN DNS leak on Ubuntu 18.04 Luckily, there is a fix that you can implement. You&rsquo;ll need to install a few helpers from the Ubuntu repositories, update your configuration file, then set up OpenVPN using NetworkManager. Let&rsquo;s do it!
1. Install some helpers To properly integrate OpenVPN with systemd-resolved, you&rsquo;ll need a bit more help. In a terminal, run:
sudo apt install -y openvpn-systemd-resolved network-manager-openvpn network-manager-openvpn-gnome This will install a helper script that integrates OpenVPN and systemd-resolved, a NetworkManager plugin for OpenVPN, and its GUI counterpart for GNOME desktop environment.
2. Add DNS implementation to your connection profile You&rsquo;ll need to edit the connection profile file you downloaded earlier. Since it&rsquo;s symbolically linked, you can accomplish this by changing the .ovpn file, wherever it&rsquo;s stored. Run vim &lt;name&gt;.ovpn to open it in Vim, then add the following lines at the bottom. Explanation in the comments:
# Allow OpenVPN to call user-defined scripts script-security 2 # Tell systemd-resolved to send all DNS queries over the VPN dhcp-option DOMAIN-ROUTE . # Use the update-systemd-resolved script when TUN/TAP device is opened, # and also run the script on restarts and before the TUN/TAP device is closed up /etc/openvpn/update-systemd-resolved up-restart down /etc/openvpn/update-systemd-resolved down-pre For the full list of OpenVPN options, see OpenVPN Scripting and Environment Variables. You may also like more information about TUN/TAP.
3. Set up OpenVPN as NetworkManager system connection Use the GUI to set up your VPN with NetworkManager. Open up Network Settings, which should look something like this:
Then click the plus sign (+) button. On the window that pops up, counterintuitively, choose Import from file&hellip; instead of the OpenVPN option.
Navigate to, and then select, your .ovpn file. You should now see something like this:
Add your username and password for the server (openvpn and the password you set in the first section&rsquo;s Step 3), and your user key password (the same one again, if you&rsquo;ve followed this tutorial), then click the &ldquo;Add&rdquo; button.
4. Edit your OpenVPN NetworkManager configuration Nearly there! Now that you&rsquo;ve added the VPN as a NetworkManager connection, you&rsquo;ll need to make a quick change to it. You can see a list of NetworkManager connections by running:
ls -la /etc/NetworkManager/system-connections/* The one for your VPN is probably called openvpn, so let&rsquo;s edit it by running:
sudo vim /etc/NetworkManager/system-connections/openvpn Under [ipv4], you&rsquo;ll need to add the line dns-priority=-42. It should end up looking like this:
Setting a negative number is a workaround that prioritizes this DNS server. The actual number is arbitrary (-1 should also work) but I like 42. ¯\\_(ツ)_/¯
5. Restart, connect, profit In a terminal, run:
sudo service network-manager restart Then in the Network Settings, click the magic button that turns on the VPN:
Finally, visit the DNS leak test website and click on Extended test to verify the fix. If everything&rsquo;s working properly, you should now see a list containing only your VPN ISP.
And we&rsquo;re done! Congratulations on rolling your very own VPN server and stopping DNS leaks with OpenVPN. Enjoy surfing in (relative) privacy. Now your only worry at the local coffeeshop is who&rsquo;s watching you surf from the seat behind you.
If you enjoyed this post, there&rsquo;s a lot more where it came from! I write about computing, cybersecurity, and leading great technical teams. Subscribe on victoria.dev to see new articles first.
`,url:"https://victoria.dev/archive/how-to-set-up-openvpn-on-aws-ec2-and-fix-dns-leaks-on-ubuntu-18.04-lts/"},"https://victoria.dev/archive/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/":{title:"How to do twice as much with half the keystrokes using `.bashrc`",tags:["linux","terminal","git"],content:`In my recent post about setting up Ubuntu with Bash scripts, I briefly alluded to the magic of .bashrc. This didn&rsquo;t really do it justice, so here&rsquo;s a quick post that offers a bit more detail about what the Bash configuration file can do.
My current configuration hugely improves my workflow, and saves me well over 50% of the keystrokes I would have to employ without it! Let&rsquo;s look at some examples of aliases, functions, and prompt configurations that can improve our workflow by helping us be more efficient with fewer key presses.
Bash aliases A smartly written .bashrc can save a whole lot of keystrokes. You can take advantage of this in the literal sense by using bash aliases, or strings that expand to larger commands. For an indicative example, here is a Bash alias for copying files in the terminal:
# Always copy contents of directories (r)ecursively and explain (v) what was done alias cp=&#39;cp -rv&#39; The alias command defines the string you&rsquo;ll type, followed by what that string will expand to. You can override existing commands like cp above. On its own, the cp command will only copy files, not directories, and succeeds silently. With this alias, you need not remember to pass those two flags, nor cd or ls the location of our copied file to confirm that it&rsquo;s there! Now, just those two key presses (for c and d) will do all of that for us.
Here are a few more .bashrc aliases for passing flags with common functions.
# List contents with colors for file types, (A)lmost all hidden files (without . and ..), in (C)olumns, with class indicators (F) alias ls=&#39;ls --color=auto -ACF&#39; # List contents with colors for file types, (a)ll hidden entries (including . and ..), use (l)ong listing format, with class indicators (F) alias ll=&#39;ls --color=auto -alF&#39; # Explain (v) what was done when moving a file alias mv=&#39;mv -v&#39; # Create any non-existent (p)arent directories and explain (v) what was done alias mkdir=&#39;mkdir -pv&#39; # Always try to (c)ontinue getting a partially-downloaded file alias wget=&#39;wget -c&#39; Aliases come in handy when you want to avoid typing long commands, too. Here are a few I use when working with Python environments:
alias pym=&#39;python3 manage.py&#39; alias mkenv=&#39;python3 -m venv env&#39; alias startenv=&#39;source env/bin/activate &amp;&amp; which python3&#39; alias stopenv=&#39;deactivate&#39; For further inspiration on ways Bash aliases can save time, I highly recommend the examples in this article.
Bash functions One downside of the aliases above is that they&rsquo;re rather static - they&rsquo;ll always expand to exactly the text declared. For a Bash alias that takes arguments, you&rsquo;ll need to create a function. You can do this like so:
# Show contents of the directory after changing to it function cd () { builtin cd &#34;$1&#34; ls -ACF } I can&rsquo;t begin to tally how many times I&rsquo;ve typed cd and then ls immediately after to see the contents of the directory I&rsquo;m now in. With this function set up, it all happens with just those two letters! The function takes the first argument, $1, as the location to change directory to, then prints the contents of that directory in nicely formatted columns with file type indicators. The builtin part is necessary to get Bash to allow us to override this default command.
Bash functions are very useful when it comes to downloading or upgrading software, too.
Bash function for downloading extended Hugo Thanks to the static site generator Hugo&rsquo;s excellent ship frequency, I previously spent at least a few minutes every couple weeks downloading the new extended version. With a Bash function, I only need to pass in the version number, and the upgrade happens in a few seconds.
# Hugo install or upgrade function gethugo () { wget -q -P tmp/ https://github.com/gohugoio/hugo/releases/download/v&#34;$@&#34;/hugo_extended_&#34;$@&#34;_Linux-64bit.tar.gz tar xf tmp/hugo_extended_&#34;$@&#34;_Linux-64bit.tar.gz -C tmp/ sudo mv -f tmp/hugo /usr/local/bin/ rm -rf tmp/ hugo version } The $@ notation simply takes all the arguments given, replacing its spot in the function. To run the above function and download Hugo version 0.57.2, you use the command gethugo 0.57.2.
Bash function for downloading a specific Go version I&rsquo;ve got one for Golang, too:
function getgolang () { sudo rm -rf /usr/local/go wget -q -P tmp/ https://dl.google.com/go/go&#34;$@&#34;.linux-amd64.tar.gz sudo tar -C /usr/local -xzf tmp/go&#34;$@&#34;.linux-amd64.tar.gz rm -rf tmp/ go version } Bash function for adding a GitLab remote Or how about a function that adds a remote origin URL for GitLab to the current repository?
function glab () { git remote set-url origin --add git@gitlab.com:&#34;$@&#34;/&#34;\${PWD##*/}&#34;.git git remote -v } With glab username, you can create a new origin URL for the current Git repository with our username on GitLab.com. Pushing to a new remote URL automatically creates a new private GitLab repository, so this is a useful shortcut for creating backups!
Bash functions are really only limited by the possibilities of scripting, of which there are, practically, few limits. If there&rsquo;s anything you do on a frequent basis that requires typing a few lines into a terminal, you can probably create a Bash function for it!
Bash prompt Besides directory contents, it&rsquo;s also useful to see the full path of the directory we&rsquo;re in. The Bash prompt can show us this path, along with other useful information like our current Git branch. To make it more readable, you can define colours for each part of the prompt. Here&rsquo;s how you can set up our prompt in .bashrc to accomplish this:
# Colour codes are cumbersome, so let&#39;s name them txtcyn=&#39;\\[\\e[0;96m\\]&#39; # Cyan txtpur=&#39;\\[\\e[0;35m\\]&#39; # Purple txtwht=&#39;\\[\\e[0;37m\\]&#39; # White txtrst=&#39;\\[\\e[0m\\]&#39; # Text Reset # Which (C)olour for what part of the prompt? pathC=&#34;\${txtcyn}&#34; gitC=&#34;\${txtpur}&#34; pointerC=&#34;\${txtwht}&#34; normalC=&#34;\${txtrst}&#34; # Get the name of our branch and put parenthesis around it gitBranch() { git branch 2&gt; /dev/null | sed -e &#39;/^[^*]/d&#39; -e &#39;s/* \\(.*\\)/(\\1)/&#39; } # Build the prompt export PS1=&#34;\${pathC}\\w \${gitC}\\$(gitBranch) \${pointerC}\\$\${normalC} &#34; Result:
~/github/myrepo (master) $ Naming the colours helps to easily identify where one colour starts and stops, and where the next one begins. The prompt that you see in our terminal is defined by the string following export PS1, with each component of the prompt set with an escape sequence. Let&rsquo;s break that down:
\\w displays the current working directory, \\$(gitBranch) calls the gitBranch function defined above, which displays the current Git branch, \\$ will display a &ldquo;$&rdquo; if you are a normal user or in normal user mode, and a &ldquo;#&rdquo; if you are root. The full list of Bash escape sequences can help us display many more bits of information, including even the time and date! Bash prompts are highly customizable and individual, so feel free to set it up any way you please.
Here are a few options that put information front and centre and can help us to work more efficiently.
For the procrastination-averse Username and current time with seconds, in 24-hour HH:MM:SS format:
export PS1=&#34;\${userC}\\u \${normalC}at \\t &gt;&#34; user at 09:35:55 &gt; For those who always like to know where they stand Full file path on a separate line, and username:
export PS1=&#34;\${pathC}\\w\${normalC}\\n\\u:&#34; ~/github/myrepo user: For the minimalist export PS1=&#34;&gt;&#34; &gt; We can build many practical prompts with just the basic escape sequences; once you start to integrate functions with prompts, as in the Git branch example, things can get really complicated. Whether this amount of complication is an addition or a detriment to your productivity, only you can know for sure!
Many fancy Bash prompts are possible with programs readily available with a quick search. I&rsquo;ve intentionally not provided samples here because, well, if you can tend to get as excited about this stuff as I can, it might be a couple hours before you get back to what you were doing before you started reading this post, and I just can&rsquo;t have that on my conscience. 🥺
We&rsquo;ve hopefully struck a nice balance now between time invested and usefulness gained from our Bash configuration file! I hope you use your newly-recovered keystroke capacity for good.
`,url:"https://victoria.dev/archive/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/"},"https://victoria.dev/archive/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/":{title:"How to set up a fresh Ubuntu desktop using only dotfiles and bash scripts",tags:["linux","terminal","hardware"],content:`One of my most favorite things about open source files on GitHub is the ability to see how others do (what some people might call) mundane things, like set up their .bashrc and other dotfiles. While I&rsquo;m not as enthusiastic about ricing as I was when I first came to the Linux side, I still get pretty excited when I find a config setting that makes things prettier and faster, and thus, better.
I recently came across a few such things, particularly in Tom Hudson&rsquo;s dotfiles. Tom seems to like to script things, and some of those things include automatically setting up symlinks, and installing Ubuntu repository applications and other programs. This got me thinking. Could I automate the set up of a new machine to replicate my current one?
Being someone generally inclined to take things apart in order to see how they work, I know I&rsquo;ve messed up my laptop on occasion. (Usually when I&rsquo;m away from home, and my backup hard drive isn&rsquo;t.) On those rare but really inconvenient situations when my computer becomes a shell of its former self, (ba-dum-ching) it&rsquo;d be quite nice to have a fast, simple way of putting Humpty Dumpty back together again, just the way I like.
In contrast to creating a disk image and restoring it later, a collection of bash scripts is easier to create, maintain, and move around. They require no special utilities, only an external transportation method. It&rsquo;s like passing along the recipe, instead of the whole bundt cake. (Mmm, cake.)
Additionally, functionality like this would be super useful when setting up a virtual machine, or VM, or even just a virtual private server, or VPS. (Both of which, now that I write this, would probably make more forgiving targets for my more destructive experiments&hellip; live and learn!)
Well, after some grepping and Googling and digging around, I now have a suite of scripts that can do this:
This is the tail end of a test run of the set up scripts on a fresh Ubuntu desktop, loaded off a bootable USB. It had all my programs and settings restored in under three minutes!
This post will cover how to achieve the automatic set up of a computer running Ubuntu Desktop using bash scripts. This exact process was last used on Ubuntu 19.10; see my dotfiles master branch for the latest configuration. The majority of the information covered is applicable to all the Linux desktop flavours, though some syntax may differ. The bash scripts cover three main areas: linking dotfiles, installing software from Ubuntu and elsewhere, and setting up the desktop environment. We&rsquo;ll cover each of these areas and go over the important bits so that you can begin to craft your own scripts.
Dotfiles Dotfiles are what most Linux enthusiasts call configuration files. They typically live in the user&rsquo;s home directory (denoted in bash scripts with the builtin variable $HOME) and control the appearance and behavior of all kinds of programs. The file names begin with ., which denotes hidden files in Linux (hence &ldquo;dot&rdquo; files). Here are some common dotfiles and ways in which they&rsquo;re useful.
.bashrc The .bashrc file is a list of commands executed at startup by interactive, non-login shells. Interactive vs non-interactive shells can be a little confusing, but aren&rsquo;t necessary for us to worry about here. For our purposes, any time you open a new terminal, see a prompt, and can type commands into it, your .bashrc was executed.
Lines in this file can help improve your workflow by creating aliases that reduce keystrokes, or by displaying a helpful prompt with useful information. It can even run user-created programs, like Eddie. For more ideas, you can have a look at my .bashrc file on GitHub.
.vimrc The .vimrc dotfile configures the champion of all text editors, Vim. (If you haven&rsquo;t yet wielded the powers of the keyboard shortcuts, I highly recommend a fun game to learn Vim with.)
In .vimrc, we can set editor preferences such as display settings, colours, and custom keyboard shortcuts. You can take a look at my .vimrc on GitHub.
Other dotfiles may be useful depending on the programs you use, such as .gitconfig or .tmux.conf. Exploring dotfiles on GitHub is a great way to get a sense of what&rsquo;s available and useful to you!
Linking dotfiles We can use a script to create symbolic links, or symlinks for all our dotfiles. This allows us to keep all the files in a central repository, where they can easily be managed, while also providing a sort of placeholder in the spot that our programs expect the configuration file to be found. This is typically, but not always, the user home directory. For example, since I store my dotfiles on GitHub, I keep them in a directory with a path like ~/github/dotfiles/ while the files themselves are symlinked, resulting in a path like ~/.vimrc.
To programmatically check for and handle any existing files and symlinks, then create new ones, we can use this elegant shell script. I compliment it only because I blatantly stole the core of it from Tom&rsquo;s setup script, so I can&rsquo;t take the credit for how lovely it is.
The symlink.sh script works by attempting to create symlinks for each dotfile in our $HOME. It first checks to see if a symlink already exists, or if a regular file or directory with the same name exists. In the former case, the symlink is removed and remade; in the latter, the file or directory is renamed, then the symlink is made.
Installing software One of the beautiful things about exploring shell scripts is discovering how much can be achieved using only the command line. As someone whose first exposure to computers was through a graphical operating system, I find working in the terminal to be refreshingly fast.
With Ubuntu, most programs we likely require are available through the default Ubuntu software repositories. We typically search for these with the command apt search &lt;program&gt; and install them with sudo apt install &lt;program&gt;. Some software we&rsquo;d like may not be in the default repositories, or may not be offered there in the most current version. In these cases, we can still install these programs in Ubuntu using a PPA, or Personal Package Archive. We&rsquo;ll just have to be careful that the PPAs we choose are from the official sources.
If a program we&rsquo;d like doesn&rsquo;t appear in the default repositories or doesn&rsquo;t seem to have a PPA, we may still be able to install it via command line. A quick search for &ldquo; installation command line&rdquo; should get some answers.
Since bash scripts are just a collection of commands that we could run individually in the terminal, creating a script to install all our desired programs is as straightforward as putting all the commands into a script file. I chose to organize my installation scripts between the default repositories, which are installed by my aptinstall.sh script, and programs that involve external sources, handled with my programs.sh script.
Setting up the desktop environment On the recent occasions when I&rsquo;ve gotten a fresh desktop (intentionally or otherwise) I always seem to forget how long it takes to remember, find, and then change all the desktop environment settings. Keyboard shortcuts, workspaces, sound settings, night mode&hellip; it adds up!
Thankfully, all these settings have to be stored somewhere in a non-graphical format, which means that if we can discover how that&rsquo;s done, we can likely find a way to easily manipulate the settings with a bash script. Lo and behold the terminal command, gsettings list-recursively.
There are a heck of a lot of settings for GNOME desktop environment. We can make the list easier to scroll through (if, like me, you&rsquo;re sometimes the type of person to say &ldquo;Just let me look at everything and figure out what I want!&rdquo;) by piping to less: gsettings list-recursively | less. Alternatively, if we have an inkling as to what we might be looking for, we can use grep: gsettings list-recursively | grep 'keyboard'.
We can manipulate our settings with the gsettings set command. It can sometimes be difficult to find the syntax for the setting we want, so when we&rsquo;re first building our script, I recommend using the GUI to make the changes, then finding the gsettings line we changed and recording its value.
For some inspiration, you can view my desktop.sh settings script on GitHub.
Putting it all together Having modular scripts (one for symlinks, two for installing programs, another for desktop settings) is useful for both keeping things organized and for being able to run some but not all of the automated set up. For instance, if I were to set up a VPS in which I only use the command line, I wouldn&rsquo;t need to bother with installing graphical programs or desktop settings.
In cases where I do want to run all the scripts, however, doing so one-by-one is a little tedious. Thankfully, since bash scripts can themselves be run by terminal commands, we can simply write another master script to run them all!
Here&rsquo;s my master script to handle the set up of a new Ubuntu desktop machine:
#!/bin/bash ./symlink.sh ./aptinstall.sh ./programs.sh ./desktop.sh ## Get all upgrades sudo apt upgrade -y ## See our bash changes source ~/.bashrc ## Fun hello figlet &#34;... and we&#39;re back!&#34; | lolcat I threw in the upgrade line for good measure. It will make sure that the programs installed on our fresh desktop have the latest updates. Now a simple, single bash command will take care of everything!
You may have noticed that, while our desktop now looks and runs familiarly, these scripts don&rsquo;t cover one very important area: our files. Hopefully, you have a back up method for those that involves some form of reliable external hardware. If not, and if you tend to put your work in external repository hosts like GitHub or GitLab, I do have a way to automatically clone and back up your GitHub repositories with bash one-liners.
Relying on external repository hosts doesn&rsquo;t offer 100% coverage, however. Files that you wouldn&rsquo;t put in an externally hosted repository (private or otherwise) consequently can&rsquo;t be pulled. Git ignored objects that can&rsquo;t be generated from included files, like private keys and secrets, will not be recreated. Those files, however, are likely small enough that you could fit a whole bunch on a couple encrypted USB flash drives (and if you don&rsquo;t have private key backups, maybe you ought to do that first?).
That said, I hope this post has given you at least some inspiration as to how dotfiles and bash scripts can help to automate setting up a fresh desktop. If you come up with some settings you find useful, please help others discover them by sharing your dotfiles, too!
`,url:"https://victoria.dev/archive/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/"},"https://victoria.dev/posts/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/":{title:"How to write Bash one-liners for cloning and managing GitHub and GitLab repositories",tags:["terminal","linux","git","data"],content:`Few things are more satisfying to me than one elegant line of Bash that automates hours of tedious work. As part of some recent explorations into automatically re-creating my laptop with Bash scripts, I wanted to find a way to easily clone my GitHub-hosted repositories to a new machine. After a bit of digging around, I wrote a one-liner that did just that. Then, in the spirit of not putting all our eggs in the same basket, I wrote another one-liner to automatically create and push to GitLab-hosted backups as well. Here they are.
A Bash one-liner to clone all your GitHub repositories Caveat: you&rsquo;ll need a list of the GitHub repositories you want to clone. The good thing about that is it gives you full agency to choose just the repositories you want on your machine, instead of going in whole-hog.
You can easily clone GitHub repositories without entering your password each time by using HTTPS with your 15-minute cached credentials or, my preferred method, by connecting to GitHub with SSH. For brevity I&rsquo;ll assume we&rsquo;re going with the latter, and our SSH keys are set up.
Given a list of GitHub URLs in the file gh-repos.txt, like this:
git@github.com:username/first-repository.git git@github.com:username/second-repository.git git@github.com:username/third-repository.git We run:
xargs -n1 git clone &lt; gh-repos.txt This clones all the repositories on the list into the current folder. This same one-liner works for GitLab repositories as well, if you substitute the appropriate URLs.
What&rsquo;s going on here There are two halves to this one-liner: the input, counterintuitively on the right side, and the part that makes stuff happen, on the left. We could make the order of these parts more intuitive (maybe?) by writing the same command like this:
&lt;gh-repos.txt xargs -n1 git clone To run a command for each line of our input, gh-repos.txt, we use xargs -n1. The tool xargs reads items from input and executes any commands it finds (it will echo if it doesn&rsquo;t find any). By default, it assumes that items are separated by spaces; new lines also works and makes our list easier to read. The flag -n1 tells xargs to use 1 argument, or in our case, one line, per command. We build our command with git clone, which xargs then executes for each line. Ta-da.
A Bash one-liner to create and push many repositories on GitLab GitLab, unlike GitHub, lets us do this nifty thing where we don&rsquo;t have to use the website to make a new repository first. We can create a new GitLab repository from our terminal. The newly created repository defaults to being set as Private, so if we want to make it Public on GitLab, we&rsquo;ll have to do that manually later.
The GitLab docs tell us to push to create a new project using git push --set-upstream, but I don&rsquo;t find this to be very convenient for using GitLab as a backup. As I work with my repositories in the future, I&rsquo;d like to run one command that pushes to both GitHub and GitLab without additional effort on my part.
To make this Bash one-liner work, we&rsquo;ll also need a list of repository URLs for GitLab (ones that don&rsquo;t exist yet). We can easily do this by copying our GitHub repository list, opening it up with Vim, and doing a search-and-replace:
cp gh-repos.txt gl-repos.txt vim gl-repos.txt :%s/\\&lt;github\\&gt;/gitlab/g :wq This produces gl-repos.txt, which looks like:
git@gitlab.com:username/first-repository.git git@gitlab.com:username/second-repository.git git@gitlab.com:username/third-repository.git We can create these repositories on GitLab, add the URLs as remotes, and push our code to the new repositories by running:
awk -F&#39;\\/|(\\.git)&#39; &#39;{system(&#34;cd ~/FULL/PATH/&#34; $2 &#34; &amp;&amp; git remote set-url origin --add &#34; $0 &#34; &amp;&amp; git push&#34;)}&#39; gl-repos.txt Hang tight and I&rsquo;ll explain it; for now, take note that ~/FULL/PATH/ should be the full path to the directory containing our GitHub repositories.
We do have to make note of a couple assumptions:
The name of the directory on your local machine that contains the repository is the same as the name of the repository in the URL (this will be the case if it was cloned with the one-liner above); Each repository is currently checked out to the branch you want pushed, ie. master. The one-liner could be expanded to handle these assumptions, but it is the humble opinion of the author that at that point, we really ought to be writing a Bash script.
What&rsquo;s going on here Our Bash one-liner uses each line (or URL) in the gl-repos.txt file as input. With awk, it splits off the name of the directory containing the repository on our local machine, and uses these pieces of information to build our larger command. If we were to print the output of awk, we&rsquo;d see:
cd ~/FULL/PATH/first-repository &amp;&amp; git remote set-url origin --add git@gitlab.com:username/first-repository.git &amp;&amp; git push cd ~/FULL/PATH/second-repository &amp;&amp; git remote set-url origin --add git@gitlab.com:username/second-repository.git &amp;&amp; git push cd ~/FULL/PATH/third-repository &amp;&amp; git remote set-url origin --add git@gitlab.com:username/third-repository.git &amp;&amp; git push Let&rsquo;s look at how we build this command.
Splitting strings with awk The tool awk can split input based on field separators. The default separator is a whitespace character, but we can change this by passing the -F flag. Besides single characters, we can also use a regular expression field separator. Since our repository URLs have a set format, we can grab the repository names by asking for the substring between the slash character / and the end of the URL, .git.
One way to accomplish this is with our regex \\/|(\\.git):
\\/ is an escaped / character; | means &ldquo;or&rdquo;, telling awk to match either expression; (\\.git) is the capture group at the end of our URL that matches &ldquo;.git&rdquo;, with an escaped . character. This is a bit of a cheat, as &ldquo;.git&rdquo; isn&rsquo;t strictly splitting anything (there&rsquo;s nothing on the other side) but it&rsquo;s an easy way for us to take this bit off. Once we&rsquo;ve told awk where to split, we can grab the right substring with the field operator. We refer to our fields with a $ character, then by the field&rsquo;s column number. In our example, we want the second field, $2. Here&rsquo;s what all the substrings look like:
1: git@gitlab.com:username 2: first-repository To use the whole string, or in our case, the whole URL, we use the field operator $0. To write the command, we just substitute the field operators for the repository name and URL. Running this with print as we&rsquo;re building it can help to make sure we&rsquo;ve got all the spaces right.
awk -F&#39;\\/|(\\.git)&#39; &#39;{print &#34;cd ~/FULL/PATH/&#34; $2 &#34; &amp;&amp; git remote set-url origin --add &#34; $0 &#34; &amp;&amp; git push&#34;}&#39; gl-repos.txt Running the command We build our command inside the parenthesis of system(). By using this as the output of awk, each command will run as soon as it is built and output. The system() function creates a child process that executes our command, then returns once the command is completed. In plain English, this lets us perform the Git commands on each repository, one-by-one, without breaking from our main process in which awk is doing things with our input file. Here&rsquo;s our final command again, all put together.
awk -F&#39;\\/|(\\.git)&#39; &#39;{system(&#34;cd ~/FULL/PATH/&#34; $2 &#34; &amp;&amp; git remote set-url origin --add &#34; $0 &#34; &amp;&amp; git push&#34;)}&#39; gl-repos.txt Using our backups By adding the GitLab URLs as remotes, we&rsquo;ve simplified the process of pushing to both externally hosted repositories. If we run git remote -v in one of our repository directories, we&rsquo;ll see:
origin git@github.com:username/first-repository.git (fetch) origin git@github.com:username/first-repository.git (push) origin git@gitlab.com:username/first-repository.git (push) Now, simply running git push without arguments will push the current branch to both remote repositories.
We should also note that git pull will generally only try to pull from the remote repository you originally cloned from (the URL marked (fetch) in our example above). Pulling from multiple Git repositories at the same time is possible, but complicated, and beyond the scope of this post. Here&rsquo;s an explanation of pushing and pulling to multiple remotes to help get you started, if you&rsquo;re curious. The Git documentation on remotes may also be helpful.
To elaborate on the succinctness of Bash one-liners Bash one-liners, when understood, can be fun and handy shortcuts. At the very least, being aware of tools like xargs and awk can help to automate and alleviate a lot of tediousness in our work. However, there are some downsides.
In terms of an easy-to-understand, maintainable, and approachable tool, Bash one-liners suck. They&rsquo;re usually more complicated to write than a Bash script using if or while loops, and certainly more complicated to read. It&rsquo;s likely that when we write them, we&rsquo;ll miss a single quote or closing parenthesis somewhere; and as I hope this post demonstrates, they can take quite a bit of explaining, too. So why use them?
Imagine reading a recipe for baking a cake, step by step. You understand the methods and ingredients, and gather your supplies. Then, as you think about it, you begin to realize that if you just throw all the ingredients at the oven in precisely the right order, a cake will instantly materialize. You try it, and it works!
That would be pretty satisfying, wouldn&rsquo;t it?
`,url:"https://victoria.dev/posts/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/"},"https://victoria.dev/posts/a-quick-guide-to-changing-your-github-username/":{title:"A quick guide to changing your GitHub username",tags:["websites","linux","terminal"],content:`This being the 2,38947234th and probably last time I&rsquo;ll change my username, (marriage is permanent, right?) I thought I&rsquo;d better write a quick post on how this transition can be achieved as smoothly as possible. You can read official instructions on how to change your GitHub username here, and they will tell you how to do it and what happens. The following is a quick guide to some things to consider afterwards.
Where to make changes Change username in GitHub account settings. If using GitHub Pages, change name of your &ldquo;username.github.io&rdquo; repository. If using other services that point to your &ldquo;username.github.io&rdquo; repository address, update them. If using Netlify, you may want to sign in and reconnect your repositories. (Mine still worked, but due to a possibly unrelated issue, I&rsquo;m not positive.) Sign in to Travis CI and other integrations (find them in your repository Settings tab -&gt; Integrations &amp; services). This will update your username there. Update your local files and repository links with very carefully executed find and sed commands, and push back changes to GitHub. Redeploy any websites you may have with your updated GitHub link. Fix any links around the web to your profile, your repositories, or Gists you may have shared. Local file updates Here are some suggestions for strings to search and replace your username in.
github.com/username (References to your GitHub page in READMEs or in website copy) username.github.io (Links to your GitHub Page) git@github.com:username (Git config remote ssh urls) travis-ci.com/username (Travis badges in READMEs) shields.io/github/.../username (Shields badges in READMEs, types include contributors, stars, tags, and more) You can quickly identify where the above strings are located using this command for each string:
grep -rnw -e 'foobar'
This will recursively (r) search all files for strings matching the whole (w) pattern (e) provided and prefix results with the line numbers (n) so you can easily find them.
Using find and sed can make these changes much faster. See this article on search and replace.
Enjoy your new handle! (I hope it sticks.)
`,url:"https://victoria.dev/posts/a-quick-guide-to-changing-your-github-username/"},"https://victoria.dev/archive/two-ways-to-deploy-a-public-github-pages-site-from-a-private-hugo-repository/":{title:"Two ways to deploy a public GitHub Pages site from a private Hugo repository",tags:["ci/cd","git"],content:`Tools like Travis CI and Netlify offer some pretty nifty features, like seamlessly deploying your GitHub Pages site when changes are pushed to its repository. Along with a static site generator like Hugo, keeping a blog up to date is pretty painless.
I&rsquo;ve used Hugo to build my site for years, but until this past week I&rsquo;d never hooked up my Pages repository to any deployment service. Why? Because using a tool that built my site before deploying it seemed to require having the whole recipe in one place - and if you&rsquo;re using GitHub Pages with the free version of GitHub, that place is public. That means that all my three-in-the-morning bright ideas and messy unfinished (and unfunny) drafts would be publicly available - and no amount of continuous convenience was going to convince me to do that.
So I kept things separated, with Hugo&rsquo;s messy behind-the-scenes stuff in a local Git repository, and the generated public/ folder pushing to my GitHub Pages remote repository. Each time I wanted to deploy my site, I&rsquo;d have to get on my laptop and hugo to build my site, then cd public/ &amp;&amp; git add . &amp;&amp; git commit&hellip; etc etc. And all was well, except for the nagging feeling that there was a better way to do this.
I wrote another article a little while back about using GitHub and Working Copy to make changes to my repositories on my iPad whenever I&rsquo;m out and about. It seemed off to me that I could do everything except deploy my site from my iPad, so I set out to change that.
A couple three-in-the-morning bright ideas and a revoked access token later (oops), I now have not one but two ways to deploy to my public GitHub Pages repository from an entirely separated, private GitHub repository. In this post, I&rsquo;ll take you through achieving this with Travis CI or using Netlify and Make.
There&rsquo;s nothing hackish about it - my public GitHub Pages repository still looks the same as it does when I pushed to it locally from my terminal. Only now, I&rsquo;m able to take advantage of a couple great deployment tools to have the site update whenever I push to my private repo, whether I&rsquo;m on my laptop or out and about with my iPad.
#YouDidNotPushFromThere
This article assumes you have working knowledge of Git and GitHub Pages. If not, you may like to spin off some browser tabs from my articles on using GitHub and Working Copy and building a site with Hugo and GitHub Pages first.
Let&rsquo;s do it!
Private-to-public GitHub Pages deployment with Travis CI Travis CI has the built-in ability (♪) to deploy to GitHub Pages following a successful build. They do a decent job in the docs of explaining how to add this feature, especially if you&rsquo;ve used Travis CI before&hellip; which I haven&rsquo;t. Don&rsquo;t worry, I did the bulk of the figuring-things-out for you.
Travis CI gets all its instructions from a configuration file in the root of your repository called .travis.yml You need to provide a GitHub personal access token as a secure encrypted variable, which you can generate using travis on the command line Once your script successfully finishes doing what you&rsquo;ve told it to do (not necessarily what you want it to do but that&rsquo;s a whole other blog post), Travis will deploy your build directory to a repository you can specify with the repo configuration variable. Setting up the Travis configuration file Create a new configuration file for Travis with the filename .travis.yml (note the leading &ldquo;.&rdquo;). These scripts are very customizable and I struggled to find a relevant example to use as a starting point - luckily, you don&rsquo;t have that problem!
Here&rsquo;s my basic .travis.yml:
git: depth: false env: global: - HUGO_VERSION=&#34;0.54.0&#34; matrix: - YOUR_ENCRYPTED_VARIABLE install: - wget -q https://github.com/gohugoio/hugo/releases/download/v\${HUGO_VERSION}/hugo_\${HUGO_VERSION}_Linux-64bit.tar.gz - tar xf hugo_\${HUGO_VERSION}_Linux-64bit.tar.gz - mv hugo ~/bin/ script: - hugo --gc --minify deploy: provider: pages skip-cleanup: true github-token: $GITHUB_TOKEN keep-history: true local-dir: public repo: gh-username/gh-username.github.io target-branch: master verbose: true on: branch: master This script downloads and installs Hugo, builds the site with the garbage collection and minify flags, then deploys the public/ directory to the specified repo - in this example, your public GitHub Pages repository. You can read about each of the deploy configuration options here.
To add the GitHub personal access token as an encrypted variable, you don&rsquo;t need to manually edit your .travis.yml. The travis gem commands below will encrypt and add the variable for you when you run them in your repository directory.
First, install travis with sudo gem install travis.
Then generate your GitHub personal access token, copy it (it only shows up once!) and run the commands below in your repository root, substituting your token for the kisses:
travis login --pro --github-token xxxxxxxxxxxxxxxxxxxxxxxxxxx travis encrypt GITHUB_TOKEN=xxxxxxxxxxxxxxxxxxxxxxxxxxx --add env.matrix Your encrypted token magically appears in the file. Once you&rsquo;ve committed .travis.yml to your private Hugo repository, Travis CI will run the script and if the build succeeds, will deploy your site to your public GitHub Pages repo. Magic!
Travis will always run a build each time you push to your private repository. If you don&rsquo;t want to trigger this behavior with a particular commit, add the skip command to your commit message.
Yo that&rsquo;s cool but I like Netlify.
Okay fine.
Deploying to a separate repository with Netlify and Make We can get Netlify to do our bidding by using a Makefile, which we&rsquo;ll run with Netlify&rsquo;s build command.
Here&rsquo;s what our Makefile looks like:
SHELL:=/bin/bash BASEDIR=$(CURDIR) OUTPUTDIR=public .PHONY: all all: clean get_repository build deploy .PHONY: clean clean: @echo &#34;Removing public directory&#34; rm -rf $(BASEDIR)/$(OUTPUTDIR) .PHONY: get_repository get_repository: @echo &#34;Getting public repository&#34; git clone https://github.com/gh-username/gh-username.github.io.git public .PHONY: build build: @echo &#34;Generating site&#34; hugo --gc --minify .PHONY: deploy deploy: @echo &#34;Preparing commit&#34; @cd $(OUTPUTDIR) \\ &amp;&amp; git config user.email &#34;you@youremail.com&#34; \\ &amp;&amp; git config user.name &#34;Your Name&#34; \\ &amp;&amp; git add . \\ &amp;&amp; git status \\ &amp;&amp; git commit -m &#34;Deploy via Makefile&#34; \\ &amp;&amp; git push -f -q https://$(GITHUB_TOKEN)@github.com/gh-username/gh-username.github.io.git master @echo &#34;Pushed to remote&#34; To preserve the Git history of our separate GitHub Pages repository, we&rsquo;ll first clone it, build our new Hugo site to it, and then push it back to the Pages repository. This script first removes any existing public/ folder that might contain files or a Git history. It then clones our Pages repository to public/, builds our Hugo site (essentially updating the files in public/), then takes care of committing the new site to the Pages repository.
In the deploy section, you&rsquo;ll notice lines starting with &amp;&amp;. These are chained commands. Since Make invokes a new sub-shell for each line, it starts over with every new line from our root directory. To get our cd to stick and avoid running our Git commands in the project root directory, we&rsquo;re chaining the commands and using the backslash character to break long lines for readability.
By chaining our commands, we&rsquo;re able to configure our Git identity, add all our updated files, and create a commit for our Pages repository.
Similarly to using Travis CI, we&rsquo;ll need to pass in a GitHub personal access token to push to our public GitHub Pages repository - only Netlify doesn&rsquo;t provide a straightforward way to encrypt the token in our Makefile.
Instead, we&rsquo;ll use Netlify&rsquo;s Build Environment Variables, which live safely in our site settings in the Netlify app. We can then call our token variable in the Makefile. We use it to push (quietly, to avoid printing the token in logs) to our Pages repository by passing it in the remote URL.
To avoid printing the token in Netlify&rsquo;s logs, we suppress recipe echoing for that line with the leading @ character.
With your Makefile in the root of your private GitHub repository, you can set up Netlify to run it for you.
Setting up Netlify Getting set up with Netlify via the web UI is straightforward. Once you sign in with GitHub, choose the private GitHub repository where your Hugo site lives. The next page Netlify takes you to lets you enter deploy settings:
You can specify the build command that will run your Makefile (make all for this example). The branch to deploy and the publish directory don&rsquo;t matter too much in our specific case, since we&rsquo;re only concerned with pushing to a separate repository. You can enter the typical master deploy branch and public publish directory.
Under &ldquo;Advanced build settings&rdquo; click &ldquo;New variable&rdquo; to add your GitHub personal access token as a Build Environment Variable. In our example, the variable name is GITHUB_TOKEN. Click &ldquo;Deploy site&rdquo; to make the magic happen.
If you&rsquo;ve already previously set up your repository with Netlify, find the settings for Continuous Deployment under Settings &gt; Build &amp; deploy.
Netlify will build your site each time you push to the private repository. If you don&rsquo;t want a particular commit to trigger a build, add [skip ci] in your Git commit message.
Same same but different One effect of using Netlify this way is that your site will be built in two places: one is the separate, public GitHub Pages repository that the Makefile pushes to, and the other is your Netlify site that deploys on their CDN from your linked private GitHub repository. The latter is useful if you&rsquo;re going to play with Deploy Previews and other Netlify features, but those are outside the scope of this post.
The main point is that your GitHub Pages site is now updated in your public repo. Yay!
Go forth and deploy fearlessly I hope the effect of this new information is that you feel more able to update your sites, wherever you happen to be. The possibilities are endless - at home on your couch with your laptop, out cafe-hopping with your iPad, or in the middle of a first date on your phone. Endless!
Don&rsquo;t do stuff on your phone when you&rsquo;re on a date. Not if you want a second one, anyway.
`,url:"https://victoria.dev/archive/two-ways-to-deploy-a-public-github-pages-site-from-a-private-hugo-repository/"},"https://victoria.dev/archive/a-remote-sync-solution-for-ios-and-linux-git-and-working-copy/":{title:"A remote sync solution for iOS and Linux: Git and Working Copy",tags:["linux","git"],content:`I&rsquo;m always looking for pockets of time in which I can be productive. If you add up the minutes you spend in limbo while waiting in line, commuting, or waiting for food delivery (just me?), you may just find an extra hour or two in your day.
To take full advantage of these bits of time, I needed a solution that let me pick up work on my Git repositories wherever I happen to be. That means a remote sync solution that bridges my iOS devices (iPad and iPhone) and my Linux machine.
After a lot of trial and error, I&rsquo;ve found one that works really well. With synced Git repositories on iOS, I can seamlessly pick up work for any of my repositories on the go.
Components Working Copy app ($15.99 one-time pro-unlock and well worth it) iA Writer app ($8.99 one-time purchase for iOS, also available on Mac, Windows, and Android) GitHub repositories Get set up Here are the steps to setting up that I&rsquo;ll walk you through in this article.
Create your remote repository Clone repository to iPad with Working Copy Open and edit files with iA Writer Push changes back to remote Pull changes from repository on your computer This system is straightforward to set up whether you&rsquo;re a command line whiz or just getting into Git. Let&rsquo;s do it!
Create your remote repository Create a public or private repository on GitHub.
If you&rsquo;re creating a new repository, you can follow GitHub&rsquo;s instructions to push some files to it from your computer, or you can add files later from your iOS device.
Clone repository to iOS with Working Copy Download Working Copy from the App Store. It&rsquo;s a fantastic app. Developer Anders Borum has a steady track record of frequent updates and incorporating the latest features for iOS apps, like drag and drop on iPad. I think he&rsquo;s fairly priced his product in light of the work he puts into maintaining and enhancing it.
In Working Copy, find the gear icon in the top left corner and touch to open Settings.
Tap on SSH Keys, and you&rsquo;ll see this screen:
SSH keys, or Secure Shell keys, are access credentials used in the SSH protocol. Your key is a password that your device will use to securely connect with your remote repository host - GitHub, in this example. Since anyone with your SSH keys can potentially pretend to be you and gain access to your files, it&rsquo;s important not to share them accidentally, like in a screenshot on a blog post.
Tap on the second line that looks like WorkingCopy@iPad-xxxxxxxx to get this screen:
Working Copy supports easy connection to GitHub. Tap Connect With GitHub to bring up some familiar sign-in screens that will authorize Working Copy to access your account(s).
Once connected, tap the + symbol in the top right of the side bar to add a new repository. Choose Clone repository to bring up this screen:
Here, you can either manually input the remote URL, or simply choose from the list of repositories that Working Copy fetches from your connected account. When you make your choice, the app clones the repository to your device and it will show up in the sidebar. You&rsquo;re connected!
Open and edit files with iA Writer One of the (many) reasons I adore iA Writer is its ability to select your freshly cloned remote repository as a Library Location. To enable this, first open your Files app. On the Browse screen, tap the overflow menu (three dots) in the top right and choose Edit.
Turn on Working Copy as a location option:
Then in the iA Writer app:
From the main Library list, in the top right of the sidebar, tap Edit. Tap Add Location&hellip;. A helpful popup appears. Tap OK. From the Working Copy location, tap Select in the top right, then choose the repository folder. Tap Open, then Done. Your remote repository now appears as a Location in the sidebar. Tap on it to work within this directory.
While inside this location, new files you create (by tapping the pencil-and-paper icon in the top right corner) will be saved to this folder locally. As you work, iA Writer automatically saves your progress. Next, we&rsquo;ll look at pushing those files and changes back to your remote.
Push changes back to remote Once you&rsquo;ve made changes to your files, open Working Copy again. You should see a yellow dot on your changed repository.
Tap on your repository name, then on Repository Status and Configuration at the top of the sidebar. Your changed files will be indicated by yellow dots or green + symbols. These mean that you&rsquo;ve modified or added files, respectively.
Working Copy is a sweet iOS Git client, and you can tap on your files to see additional information including a comparison of changes (&ldquo;diff&rdquo;) as well as status and Git history. You can even edit files right within the app, with syntax highlighting for its many supported languages. For now, we&rsquo;ll look at how to push your changed work to your remote repository.
On the Repository Status and Configuration page, you&rsquo;ll see right at the top that there are changes to be committed. If you&rsquo;re new to Git, this is like &ldquo;saving your changes&rdquo; to your Git history, something typically done with the terminal command git commit. You can think of this as saving the files that we&rsquo;ll want to send to the GitHub repository. Tap Commit changes.
Enter your commit message, and select the files you want to add. Toggle the Push switch to send everything to your remote repository when you commit the files. Then tap Commit.
You&rsquo;ll see a progress bar as your files are uploaded, and then a confirmation message on the status screen.
Congratulations! Your changes are now present in your remote repository on GitHub. You&rsquo;ve successfully synced your files remotely!
Pull changes from repository on your computer To bring your updated files full circle to your computer, you pull them from the GitHub repository. I prefer to use the terminal for this as it&rsquo;s quick and easy, but GitHub also offers a graphical client if terminal commands seem a little alien for now.
If you started with the GitHub repository, you can clone it to a folder on your computer by following these instructions.
Staying in sync When you update your work on your computer, you&rsquo;ll use Git to push your changes to the remote repository. To do this, you can use GitHub&rsquo;s graphical client, or follow these instructions.
On your iOS device, Working Copy makes pulling and pushing as simple as a single tap. On the Repository Status and Configuration page, tap on the remote name under Remotes.
Then tap Synchronize. Working Copy will take care of the details of pushing your committed changes and/or pulling any new changes it finds from the remote repository.
Work anywhere For a Git-based developer and work-anywhere-aholic like me, this set up couldn&rsquo;t be more convenient. Working Copy really makes staying in sync with my remote repositories seamless, nevermind the ability to work with any of my GitHub repos on the go.
I most recently used this set up to get some writing done while hanging out in the atrium of Washington DC&rsquo;s National Portrait Gallery, which is pleasantly photogenic.
Happy working! If you enjoyed this post, there&rsquo;s a lot more where this came from! I write about computing, cybersecurity, and leading great technical teams. You can subscribe to see new articles first.
`,url:"https://victoria.dev/archive/a-remote-sync-solution-for-ios-and-linux-git-and-working-copy/"},"https://victoria.dev/archive/on-doing-great-things/":{title:"On doing great things",tags:["life"],content:`It&rsquo;s International Women&rsquo;s Day, and I&rsquo;m thinking about Grace Hopper.
Grace Hopper was an amazing lady who did great things. She envisioned and helped create programming languages that translate English terms into machine code. She persevered in her intention to join the US Navy from the time she was rejected at 34 years old, to being sworn in to the US Navy Reserve three years later, to retiring with the rank of commander at age 60&hellip; then was recalled (twice) and promoted to the rank of captain at the age of 67. She advocated for distributed networks and developed computer testing standards we use today, among other achievements too numerous to list here.
By my read, throughout her life, she kept her focus on her work. She did great things because she could do them, and felt some duty to do them. Her work speaks for itself.
I recently came across a sizeable rock denoting a rather small, quiet park. It looks like this:
When I first saw this park, I thought it in no way did this great lady justice. But upon some reflection, its lack of assumption and grandeur grew on me. And today, it drew to the forefront something that&rsquo;s been on my mind.
I try and contribute regularly to the wide world of technology, usually through building things, writing, and mentorship. I sometimes get asked to participate in female-focused tech events. I hear things like, &ldquo;too few developers are women,&rdquo; or &ldquo;we need more women in blockchain,&rdquo; or &ldquo;we need more female coders.&rdquo;
For some time I haven&rsquo;t been sure how to respond, because while my answer isn&rsquo;t &ldquo;yes,&rdquo; it&rsquo;s not exactly &ldquo;no,&rdquo; either. It&rsquo;s really, &ldquo;no, because&hellip;&rdquo; and it&rsquo;s because I&rsquo;m afraid. I&rsquo;m afraid of misrepresenting myself, my values, and my goals.
Discrimination and racism are real things. They exist in the minds and attitudes of a very small percentage of very loud people, as they always will. These people aren&rsquo;t, however, the majority. They are small.
I think that on the infrequent occasions when we encounter these people, we should do our best to lead by example. We should have open minds, tell our stories, listen to theirs. Try and learn something. That&rsquo;s all.
When I present myself, I don&rsquo;t point out that I&rsquo;m a woman. I don&rsquo;t align myself with &ldquo;women in tech&rdquo; or seek to represent them. I don&rsquo;t go to women-only meetings or support organizations that discriminate against men, or anyone at all. It&rsquo;s not because I&rsquo;m insecure as a woman, or ashamed that I&rsquo;m a woman, or some other inflammatory adjective that lately shows up in conjunction with being female. It&rsquo;s because I&rsquo;ve no reason to point out my gender, any more than needing to point out that my hair is black, or that I&rsquo;m short. It&rsquo;s obvious and simultaneously irrelevant.
When I identify with a group, I talk about the go-getters who wake up at 0500 every day and go work out - no matter the weather, or whether they feel like it. I tell stories about the people I met in different countries around the world, who left home, struck out on their own, and had an adventure, because they saw value in the experience. I identify with people who constantly build things, try things, design and make things, and then share those things with the world, because they love to do so. This is how I see myself. This is what matters to me.
Like the unassuming park named after an amazing woman, when truly great things are done, they are done relatively quietly. Not done for the fanfare of announcing them to the world, but for the love of the thing itself. So go do great things, please. The world still needs them.
`,url:"https://victoria.dev/archive/on-doing-great-things/"},"https://victoria.dev/posts/git-commit-practices-your-future-self-will-thank-you-for/":{title:"Git commit practices your future self will thank you for",tags:["git","coding","terminal","leadership"],content:`A history of clean commits can be evidence of a lot of things: attention to detail, good work ethic, and genuine investment in the project. What do your Git commits say about you?
Here&rsquo;s how you can create and maintain a clean and orderly Git commit history using message templates, learning how to squash commits, using git stash, and creating annotated commit tags.
What it means to commit responsibly Whether our code will be seen by the entire open source community or just future versions of ourselves, either one will be grateful if we commit responsibly today. Being responsible can mean a lot of things to different people, so I enlisted some of mastodon.technology (instance shut down since) and dev.to to help round out my list. From those (really great) threads, I distilled these main points:
Committing responsibly
Provide and/or use tests to avoid committing bugs or broken builds Write clean code that meets style specifications Use descriptive commit messages that reference related discussion Make only one change per commit and avoid including unrelated changes Some of the above is achieved through maintaining a short feedback loop that helps you improve your code quality while staying accountable to yourself. I wrote another article that discusses this in detail, especially the part about code review. Other items on this list have to do specifically with making commits in Git. There are some features of Git that can benefit us in these areas, as can harnessing tools like Vim. I&rsquo;ll cover those topics here.
If the majority of your Git commits so far have been created with something like git commit -m &quot;Bug fixes&quot; then this is the article for you!
Write great Git commit messages with a template I think Linus would be very happy if we didn&rsquo;t use git commit -m &quot;Fix bug&quot; in a public repository ever again. As very well put in this classic post and the seven rules of a great Git commit message:
A properly formed Git commit subject line should always be able to complete the following sentence:
If applied, this commit will your subject line here
This other classic post also discusses three questions that the body of the commit message should answer:
Why is it necessary? How does it address the issue? What effects does the patch have?
This can be a lot to remember to cover, but there&rsquo;s a slick way to have these prompts at hand right when you need it. You can set up a commit message template by using the commit.template configuration value.
To set it, configure Git to use a template file (for example, .gitmessage in your home directory), then create the template file with Vim:
git config --global commit.template ~/.gitmessage vim ~/.gitmessage When we run git commit without the -m message flag, the editor will open with our helpful template ready to go. Here&rsquo;s my commit message template:
## If applied, this commit will... ## [Add/Fix/Remove/Update/Refactor/Document] [issue #id] [summary] ## Why is it necessary? (Bug fix, feature, improvements?) - ## How does the change address the issue? - ## What side effects does this change have? - I&rsquo;m a fan of this format because commented lines are not included in the final message. I can simply fill in the blank lines with text and bullet points under the prompts, and it comes out looking something like this:
Fix #16 missing CSS variables - Fix for unstyled elements - Add background color, height for code blocks - Only affects highlight class Reference related discussion Issue trackers in GitHub and Bitbucket both recognize the keywords close, fix, and resolve followed immediately by the issue or pull request number. These keywords conveniently help us close the referenced issue or pull request, and this helps maintain a clear trail of changes. GitLab, and issue trackers like Jira offer similar functionalities.
Use helpful Vim settings for git commit messages By adding a few lines to our Vim configuration, we can make writing great git commit messages easy. We can add these lines to ~/.vimrc to turn on syntax highlighting in general, and spell check and text wrapping for commit messages in particular:
&#34; Filetype detection, plugins, and indent rules filetype plugin indent on &#34; Syntax highlighting syntax on &#34; Spell check and line wrap just for git commit messages autocmd Filetype gitcommit setlocal spell textwidth=72 If you&rsquo;re curious, you can find my full ~/.vimrc in my dotfiles.
Other editors have settings that can help us out as well. I came across these for Sublime Text 3 and language specific settings for VS Code.
One change per commit: how to squash Git commits Still life Git
Let&rsquo;s get one thing out of the way first: rewriting Git history just for the sake of having a pretty tree, especially with public repositories, is generally not advisable. It&rsquo;s kind of like going back in time, where changes you make to your version of the project cause it to look completely different from a version that someone else forked from a point in history that you&rsquo;ve now erased - I mean, haven&rsquo;t you seen Back to the Future Part II? (If you&rsquo;d rather maintain that only one Back to the Future movie was ever made, thus sparing your future self from having to watch the sequels, I get it.)
Here&rsquo;s the main point. If you&rsquo;ve pushed messy commits to a public repository, I say go right ahead and leave them be, instead of complicating things further. (We all learn from our embarrassments, especially the public ones - I&rsquo;m looking at you, past-Vicky.) If your messy commits currently only exist on your local version, great! We can tidy them up into one clean, well-described commit that we&rsquo;ll be proud to push, and no one will be the wiser.
There are a couple different ways to squash commits, and choosing the appropriate one depends on what we need to achieve.
The following examples are illustrated using git log --graph, with some options for brevity. We can set a handy alias to see this log format in our terminal with:
git config --global alias.plog &#34;log --graph --pretty=format:&#39;%h -%d %s %n&#39; --abbrev-commit --date=relative --branches&#34; Then we just do git plog to see the pretty log.
Method #1: one commit to rule the master branch This is appropriate when:
We&rsquo;re committing directly to master We don&rsquo;t intend to open a pull request to merge a feature We don&rsquo;t want to preserve history of branches or changes we haven&rsquo;t yet pushed This method takes a Git tree that looks like this:
* 3e8fd79 - (HEAD -&gt; master) Fix a thing | * 4f0d387 - Tweak something | * 0a6b8b3 - Merge branch &#39;new-article&#39; |\\ | * 33b5509 - (new-article) Update article again again | | | * 1782e63 - Update article again | | | * 3c5b6a8 - Update article | | * | f790737 - (master) Tweak unrelated article |/ | * 65af7e7 Add social media link | * 0e3fa32 (origin/master, origin/HEAD) Update theme And makes it look like this:
* 7f9a127 - (HEAD -&gt; master) Add new article | * 0e3fa32 - (origin/master, origin/HEAD) Update theme Here&rsquo;s how to do it - hold on to your hoverboards, it&rsquo;s super complicated:
git reset --soft origin/master git commit Yup that&rsquo;s all. We can delete the unwanted branch with git branch -D new-article.
Method #2: not that much This is appropriate when:
We want to squash the last x commits but not all commits since origin/master We want to open a pull request to merge a branch This method takes a Git tree that looks like this:
* 13a070f - (HEAD -&gt; new-article) Finish new article | * 78e728a - Edit article draft | * d62603c - Add example | * 1aeb20e - Update draft | * 5a8442a - Add new article draft | | * 65af7e7 - (master) Add social media link |/ | * 0e3fa32 - (origin/master, origin/HEAD) Update theme And makes it look like this:
* 90da69a - (HEAD -&gt; new-article) Add new article | | * 65af7e7 - (master) Add social media link |/ | * 0e3fa32 - (origin/master, origin/HEAD) Update theme To squash the last five commits on branch new-article into one, we use:
git reset --soft HEAD~5 git commit -m &#34;New message for the combined commit&#34; Where --soft leaves our files untouched and staged, and 5 can be thought of as &ldquo;the number of previous commits I want to combine.&rdquo;
We can then do git merge master and create our pull request.
Method #3: getting picky Say we had a really confusing afternoon and our Git tree looks like this:
* dc89918 - (HEAD -&gt; master) Add link | * 9b6780f - Update image asset | * 6379956 - Fix CSS bug | * 16ee1f3 - Merge master into branch |\\ | | | * ccec365 - Update list page | | * | 033dee7 - Fix typo | | * | 90da69a - Add new article |/ | * 0e3fa32 - (origin/master, origin/HEAD) Update theme We want to retain some of this history, but clean up the commits. We also want to change the messages for some of the commits. To achieve this, we&rsquo;ll use git rebase.
This is appropriate when:
We want to squash only some commits We want to edit previous commit messages We want to delete or reorder specific commits Git rebase is a powerful tool, and handy once we&rsquo;ve got the hang of it. To change all the commits since origin/master, we do:
git rebase -i origin/master Or, we can do:
git rebase -i 0e3fa32 Where the commit hash is the last commit we want to retain as-is.
The -i option lets us run the interactive rebase tool, which launches our editor with, essentially, a script for us to modify. We&rsquo;ll see a list of our commits in reverse order to the git log, with the oldest at the top:
pick 90da69a Add new article pick 033dee7 Fix typo pick ccec365 Update list page pick 6379956 Fix CSS bug pick 9b6780f Update image asset pick dc89918 Add link # Rebase 0e3fa32..dc89918 onto 0e3fa32 (6 commands) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like &#34;squash&#34;, but discard this commit&#39;s log message # x, exec = run command (the rest of the line) using shell # d, drop = remove commit # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out # ~ The comments give us a handy guide as to what we&rsquo;re able to do. For now, let&rsquo;s squash the commits with small changes into the more significant commits. In our editor, we change the script to look like this:
pick 90da69a Add new article squash 033dee7 Fix typo pick ccec365 Update list page squash 6379956 Fix CSS bug squash 9b6780f Update image asset squash dc89918 Add link Once we save the changes, the interactive tool continues to run. It will execute our instructions in sequence. In this case, we see the editor again with the following:
# This is a combination of 2 commits. # This is the 1st commit message: Add new article # This is the commit message #2: Fix typo # Please enter the commit message for your changes. Lines starting # with &#39;#&#39; will be ignored, and an empty message aborts the commit. # # interactive rebase in progress; onto 0e3fa32 # Last commands done (2 commands done): # pick 90da69a Add new article # squash 033dee7 Fix typo # Next commands to do (4 remaining commands): # pick ccec365 Update list page # squash 6379956 Fix CSS bug # You are currently rebasing branch &#39;master&#39; on &#39;0e3fa32&#39;. # # Changes to be committed: # modified: ... # ~ Here&rsquo;s our chance to create a new commit message for this first squash, if we want to. Once we save it, the interactive tool will go on to the next instructions. Unless&hellip;
[detached HEAD 3cbad01] Add new article 1 file changed, 129 insertions(+), 19 deletions(-) Auto-merging content/dir/file.md CONFLICT (content): Merge conflict in content/dir/file.md error: could not apply ccec365... Update list page Resolve all conflicts manually, mark them as resolved with &#34;git add/rm &lt;conflicted_files&gt;&#34;, then run &#34;git rebase --continue&#34;. You can instead skip this commit: run &#34;git rebase --skip&#34;. To abort and get back to the state before &#34;git rebase&#34;, run &#34;git rebase --abort&#34;. Could not apply ccec365... Update list page Again, the tool offers some very helpful instructions. Once we fix the merge conflict, we can resume the process with git rebase --continue. Our interactive rebase picks up where it left off.
Once all the squashing is done, our Git tree looks like this:
* 3564b8c - (HEAD -&gt; master) Update list page | * 3cbad01 - Add new article | * 0e3fa32 - (origin/master, origin/HEAD) Update theme Phew, much better.
Git stash If we&rsquo;re in the middle of some work and it&rsquo;s not a good time to commit, but we need to switch branches, stashing can be a good option. Stashing lets us save our unfinished work without needing to create a half-assed commit. It&rsquo;s like that pile of paper on your desk representing all the stuff you&rsquo;ve been in the middle of doing since two weeks ago. Yup, that one.
It&rsquo;s as easy as typing git stash:
Saved working directory and index state WIP on master: 3564b8c Update list page The dirty work we&rsquo;re in the midst of is safely tucked away, and our working directory is clean - just as it was after our last commit. To see what&rsquo;s in our stash stack, we do git stash list:
stash@{0}: WIP on master: 3564b8c Update list page stash@{1}: WIP on master: 90da69a Add new article stash@{2}: WIP on cleanup: 0e3fa32 Update theme To restore our work in progress, we use git stash apply. Git will try and apply our most recent stashed work. To apply an older stash, we use git stash apply stash@{1} where 1 is the stash to apply. If changes since stashing our work prevent the stash from reapplying cleanly, Git will give us a merge conflict to resolve.
Applying a stash doesn&rsquo;t remove it from our list. To remove a stash from our stack, we do git stash drop stash@{0} where 0 is the one we want to remove.
We can also use git stash pop to apply the most recent stash and then immediately remove it from the stack.
Tag release versions using annotated Git tags In the spirit of having a beautiful, clean Git history, there&rsquo;s one more thing we can do to help make our commit log inspire infinite joy in its viewers. If you&rsquo;ve never heard of git tag, your master branch history might look like this&hellip;
* 0377782 - Update theme | * ecf8128 - Add about page (#25) | * 33e432f - Fix #23 navigation bug | * 08b853b - Create blog section | * 63d18b4 - Add theme (#12) | * 233e23f - Add main content (#6) Wouldn&rsquo;t it be nice if it looked like this instead?
* 0377782 - (tag: v2.1.0) Update theme | * ecf8128 - Add about page (#25) | * 33e432f - Fix #23 navigation bug | * 08b853b - (tag: v2.0.0) Create blog section | * 63d18b4 - Add theme (#12) | * 233e23f - (tag: v1.1.0) Add main content (#6) We can tag Git commits with anything, but tags are especially helpful for semantic versioning of releases. Sites like GitHub and GitLab have pages for repositories that list tags, letting viewers of our project browse the release versions. This can be helpful for public projects to differentiate major releases, updates with bug fixes, or beta versions.
There are two types of Git tags: lightweight and annotated. For adding a version tag to commits, we use annotated Git tags.
The Git tag documentation explains it this way:
Tag objects (created with -a, -s, or -u) are called &ldquo;annotated&rdquo; tags; they contain a creation date, the tagger name and e-mail, a tagging message, and an optional GnuPG signature. Whereas a &ldquo;lightweight&rdquo; tag is simply a name for an object (usually a commit object).
Annotated tags are meant for release while lightweight tags are meant for private or temporary object labels. For this reason, some git commands for naming objects (like git describe) will ignore lightweight tags by default.
We can think of lightweight tags as bookmarks, and annotated tags as signed releases.
For public repositories, annotated tags allow us to:
See who tagged the commit, which may differ from the commit author See all the tags with git describe Avoid conflicting tag names To create an annotated Git tag and attach it to our current (last) commit, we do:
git tag -a v1.2.0 -m &#34;Clever release title&#34; This tags the commit on our local repository. To push all annotated tags to the remote, we do:
git push --follow-tags We can also set our Git configuration to push our annotated tags by default:
git config --global push.followTags true If we then want to skip pushing tags this time, we pass --no-follow-tags.
Practice responsible commits A little time invested in getting familiar with these tools and practices can make your commits even more useful and well-crafted. With a little practice, these processes will become second nature. You can make it even easier by creating a personal commit checklist on paper to keep handy while you work - or if that isn&rsquo;t fun enough, make it an interactive pre-commit hook.
Creating clean, useful, and responsible Git commits says a lot about you. Especially in remote work, Git commits may be a primary way that people interact with you over projects. With a little practice and effort, you can make your commit habits an even better reflection of your best work - work that is evidently created with care and pride.
If you enjoyed this post, there&rsquo;s a lot more where it came from! I write about computing, cybersecurity, and leading great technical teams. Subscribe on victoria.dev to see new articles first, and check out the ones below!
`,url:"https://victoria.dev/posts/git-commit-practices-your-future-self-will-thank-you-for/"},"https://victoria.dev/archive/an-automatic-interactive-pre-commit-checklist-in-the-style-of-infomercials/":{title:"An automatic interactive pre-commit checklist, in the style of infomercials",tags:["git","coding","terminal"],content:`What&rsquo;s that, you say? You&rsquo;ve become tired of regular old boring paper checklists? Well, my friend, today is your lucky day! You, yes, you, can become the proud owner of a brand-spanking-new automatic interactive pre-commit hook checklist! You&rsquo;re gonna love this! Your life will be so much easier! Just wait until your friends see you.
What&rsquo;s a pre-commit hook Did you know that nearly 1 out of 5 coders are too embarrassed to ask this question? Don&rsquo;t worry, it&rsquo;s perfectly normal. In the next 60 seconds we&rsquo;ll tell you all you need to know to pre-commit with confidence.
A Git hook is a feature of Git that triggers custom scripts at useful moments. They can be used for all kinds of reasons to help you automate your work, and best of all, you already have them! In every repository that you initialize with git init, you&rsquo;ll have a set of example scripts living in .git/hooks. They all end with .sample and activating them is as easy as renaming the file to remove the .sample part.
Git hooks are not copied when a repository is cloned, so you can make them as personal as you like.
The useful moment in particular that we&rsquo;ll talk about today is the pre-commit. This hook is run after you do git commit, and before you write a commit message. Exiting this hook with a non-zero status will abort the commit, which makes it extremely useful for last-minute quality checks. Or, a bit of fun. Why not both!
How do I get a pre-commit checklist I only want the best for my family and my commits, and that&rsquo;s why I choose an interactive pre-commit checklist. Not only is it fun to use, it helps to keep my projects safe from unexpected off-spec mistakes!
It&rsquo;s so easy! I just write a bash script that can read user input, and plop it into .git/hooks as a file named pre-commit. Then I do chmod +x .git/hooks/pre-commit to make it executable, and I&rsquo;m done!
Oh look, here comes an example bash script now!
#!/bin/sh echo &#34;Would you like to play a game?&#34; # Read user input, assign stdin to keyboard exec &lt; /dev/tty while read -p &#34;Have you double checked that only relevant files were added? (Y/n) &#34; yn; do case $yn in [Yy] ) break;; [Nn] ) echo &#34;Please ensure the right files were added!&#34;; exit 1;; * ) echo &#34;Please answer y (yes) or n (no):&#34; &amp;&amp; continue; esac done while read -p &#34;Has the documentation been updated? (Y/n) &#34; yn; do case $yn in [Yy] ) break;; [Nn] ) echo &#34;Please add or update the docs!&#34;; exit 1;; * ) echo &#34;Please answer y (yes) or n (no):&#34; &amp;&amp; continue; esac done while read -p &#34;Do you know which issue or PR numbers to reference? (Y/n) &#34; yn; do case $yn in [Yy] ) break;; [Nn] ) echo &#34;Better go check those tracking numbers!&#34;; exit 1;; * ) echo &#34;Please answer y (yes) or n (no):&#34; &amp;&amp; continue; esac done exec &lt;&amp;- Take my money Don&rsquo;t delay! Take advantage right now of this generous one-time offer! An interactive pre-commit hook checklist can be yours, today, for the low, low price of&hellip; free? Wait, who wrote this script?
`,url:"https://victoria.dev/archive/an-automatic-interactive-pre-commit-checklist-in-the-style-of-infomercials/"},"https://victoria.dev/archive/how-to-set-up-a-short-feedback-loop-as-a-solo-coder/":{title:"How to set up a short feedback loop as a solo coder",tags:["coding","life"],content:`I&rsquo;ve spent the last couple years as a solo freelance developer. Comparing this experience to previously working in companies, I&rsquo;ve noticed that those of us who work alone can have fewer iterative opportunities for improvement than developers who work on teams. Integral to having opportunities to improve is the concept of a short feedback loop: a process of incorporating new learning from observation and previous experience continuously over a short period of time. This process has to be manufactured by people working mostly alone, instead of, as is often the case, adopted when you join a team.
In this post I hope to share what I&rsquo;ve learned about setting yourself up to improve quickly and continuously as a solo coder.
About feedback loops United States Air Force Colonel John Boyd developed the concept of the OODA loop, OODA being an acronym for observe, orient, decide, act. In military operations, this illustrates a process of decision-making based on the constant ingestion of new information:
Observe: Obtain raw information about unfolding circumstances and the current environment. Orient: Put raw observations in context. Consider such things as relevancy to the current situation and previously gained knowledge and expertise. Decide: Make a plan for moving towards your goal. Act: Execute the plan.
Since it&rsquo;s a loop, the act stage leads directly back into the observe stage. This is the critical &ldquo;feed back&rdquo; concept that enables increasingly successful iterations. It&rsquo;s widely applicable beyond military operations - you may recognize it as the origin of the PDCA (plan-do-check-act) method.
I like the OODA loop for being a succinct illustration of a general feedback loop. Many concepts and working methods build on the idea of feedback loops, including DevOps and Agile software development methods.
Development team feedback loop Let&rsquo;s look at what some components of a feedback loop for a developer on a team might look like:
Direction from product owners or reviews from users Daily scrum/standup with whole team Prioritization with developer team Individual coding and testing Peer code review Deployment and performance monitoring Implicit in these steps is the support of co-workers and management - in other words, someone to answer to. How can a solo freelance developer create a similar environment of accountability?
Solo developer feedback loop Here are some possible steps that an individual freelance developer can implement to create a short feedback loop:
Build discipline Clarify concrete top-level goals Prioritize and plan mid-level and low-level goals Automate your work Block out time for code review Block out time for process review Update your goals and processes with the results of your reviews I&rsquo;ll cover each of these stages in detail below.
Build discipline More of a prerequisite than a stage in itself, building discipline is what enables our short feedback loop to work. Nothing else in this article will be helpful unless we have the skill to do something we don&rsquo;t want to do. Discipline is most certainly a skill. It can be learned, trained, and improved just like any other.
Why is discipline so important? Because when we&rsquo;re crunching to get a project completed this Friday evening, we&rsquo;re not going to want to write a good commit message. We&rsquo;re not going to want to clean up the code comments. We just want to see the darn thing go, Hello, git push -f. It&rsquo;s in those moments that discipline enables us to not miss an opportunity to practice, learn, and improve our work process. Discipline helps us avoid Friday night commits that turn into Monday morning git reset --hards.
Clarify concrete top-level goals Whether working for a client or bootstrapping our own best-new-app-ever, we won&rsquo;t be able to measure any progress or improvements without something to measure them against.
When I&rsquo;m discussing a new project with a client, I always speak in terms of concrete achievements. This could take the form of accomplishing a specific feature by a certain date, or deciding what the MVP looks like to a user. This is as much to my benefit as my client&rsquo;s. By agreeing, in writing, what will be achieved and when, my client and I have clearly defined top-level goals and can both assess how the project is progressing. When I&rsquo;m working for myself, I treat myself as I would a client. I make a commitment, in writing, describing what will be achieved, and when. This can be something as simple as a goals list for the week, or as detailed as a kanban board.
The point of having a concrete goal, however, is not to stick to it at all costs. It&rsquo;s important to set an expectation, with ourselves and with our clients, that the goals will be revisited at mutually-agreeable dates over the course of the project. This enables the all-important &ldquo;feed back&rdquo; part of the loop.
Prioritize and plan mid-level and low-level goals Few goals are achieved all in one step. Even the simple process of making a peanut butter and jelly sandwich (a favourite computer programming teaching example) can be broken down into successively smaller, more precise instructions. While we humans may not require the granularity that a computer program does, goals that are chunked into time-boxed, achievable steps are much more easily digested. 🥪
Start with the mid-level goals, and make each step concrete. If the goal is to release a new open source web app, for example, the steps might look like this:
Complete app JavaScript Create front end and stylesheet Do local tests Set up cloud server Deploy app to cloud Do tests Add repository to GitHub Post on Hacker News Profit!!! Each of the above examples encapsulates many smaller, low-level goals - we can think of these as our to-do list items. For example, &ldquo;Set up cloud server&rdquo; might involve:
Research cloud providers Decide on service and sign up Set up server/instance Add integrations Test deployment Our parameters for chunk sizes and what constitutes a &ldquo;step&rdquo; may be different from one another, and will likely change from project to project. If your mid-level and low-level steps clearly define a concrete path for achieving the top-level goals you set, then you&rsquo;re in good shape. Later on, evaluating the decision process that brought us to these mid-level and low-level goals enables us to bring our feedback loop full circle.
Automate your work I recently read a great article entitled Manual Work is a Bug. It discusses a process by which successful developers document and eventually automate their work. The beauty of this idea is in its simplicity. By writing down the things we do manually, we&rsquo;re able to correct and refine our processes. By refining our processes, we can more easily translate them into code snippets and scripts. With a collection of scripts that we can string together, we can automate our work.
Automating work isn&rsquo;t only about saving time. It reduces haven&rsquo;t-had-my-coffee-yet errors, minimizes cognitive load allowing more room for creativity, and allows our processes to be repeatable across collaborators and projects. It help shorten our feedback loop by ensuring we aren&rsquo;t doing the same thing three times in three different ways.
We can begin to automate by starting our own personal wiki. If we build a habit of writing down every manual thing we do, no matter how basic it may seem at the time, we give ourselves more opportunities to spot patterns, and thus possible integrations and improvements.
The first time we do something manually, we write out the steps. The second time, we follow the steps. This gives us the opportunity to correct and refine them based on what we&rsquo;ve learned since the first time. Over successive iterations, we might replace parts of manual commands with variables; we might find handy snippets of bash scripts that automate just a part of our task. As long as we keep revising and improving our personal wiki, we&rsquo;re moving towards automation.
Block out time for code review It&rsquo;s all too easy to commit messy code when we work alone. We think, who&rsquo;s going to see it? I&rsquo;ll fix it later. Each time that happens, though, we&rsquo;re building a habit. It&rsquo;s a bad one.
Working alone means there&rsquo;s no one likely to give feedback on our commits when we&rsquo;re doing something that doesn&rsquo;t make sense, or that could be improved. Instead, we have to actively seek out opportunities to improve. Open source communities are amazing for this. There&rsquo;s a wealth of information available to us in terms of coding styles, examples of refactored code, and a smorgasbord of snippets that achieve that-thing-we-were-trying-to-do but in fewer lines. We can learn all we please, if we just block out the time to do it.
Schedule your own code review at a time that makes sense for you and the project you&rsquo;re working on. This might be each time you finish a fix or feature, or at regular intervals daily or weekly. If you have someone who can help, book them. There are also a great many organizations that host chatrooms full of people happy to lend a hand: freeCodeCamp and the OWASP Web Security Testing Guide, to name a couple.
Do some research on basic best practices for what you&rsquo;re working on. Set yourself a time limit though, and take what you read with a grain of salt. There&rsquo;s a lot of rabbit holes in that field. As a starting point, I&rsquo;d recommend learning about DRY code, and watching Uncle Bob demand professionalism in software development.
Code review checklist Here&rsquo;s my personal code review checklist, based off some general best practices. Feel free to use it as a starting point for your own!
Victoria&rsquo;s Code Review Extravaganza!
This solves a high-priority item. This is a complete implementation that follows the specification. Off-topic changes were not included and have been added to backlog. Variable names are meaningful and there are no magic numbers. Correct and useful error messages are returned at every opportunity. No debugging print statements were left in. This code is DRY and modular. This code is secure. Private and public code are well separated. This code is its own documentation, or the documentation is up to date. A five-year-old could follow this, seriously it&rsquo;s that readable. Unit tests successfully pass. Master was merged into the branch and tested. Formatting follows style guidelines. I cannot find any further edge cases or known defects. I would be happy if this code was publicly attributed to me. I fully understand what the code does and the impact of the changes I made. I actually verified that it actually does what I said it does. Here is an excellent example of cleaning up code with some of the above points in mind.
Block out time for process review Just as we learn from reviewing our code, we refine our processes by reviewing them as well. Process review is most beneficial when visited at regular intervals throughout the project, not just after the project&rsquo;s completion. For short-term projects, a good starting point for scheduling process reviews is at each half-mark - once midway through, and again after completion. Long-term projects may have reviews at each quarter-mark.
Process review questions Process review can be as simple as a short list of questions:
What were my top-level goals for this period? Did I meet them? What were my mid-level and low-level goals for this period? Did I meet them? Would I have been better served with different or more specific goals? Why? Did I successfully remove or automate obstacles? Did I stick to my code review schedule? Why or why not? How might I remove obstacles next time? Setting aside dedicated time for our process review can help us to answer questions like these thoughtfully and honestly. This allows us to squeeze out every bit of learning we can from our review, helping to shorten our feedback loop.
Update your goals and processes with the results of your reviews All the performance data in the world is no good to us if we don&rsquo;t put it into practice. With each successive code review, we can refine and add to our checklist. With what we learn from each process review, we can fine tune and improve our processes. The more we can invent concrete and observable ways to implement our learning, the more success we&rsquo;ll have.
Making a conscious effort to utilize and practice the things we&rsquo;ve learned is the final, vital, component of our feedback loop. The more often we incorporate new learning, the shorter our loop becomes, allowing us to improve that much faster.
`,url:"https://victoria.dev/archive/how-to-set-up-a-short-feedback-loop-as-a-solo-coder/"},"https://victoria.dev/archive/adorable-bookmarklets-want-to-help-delete-your-social-media-data/":{title:"Adorable bookmarklets want to help delete your social media data",tags:["javascript"],content:`A little while ago I wrote about a Lambda function I called ephemeral for deleting my old tweets. While it&rsquo;s a great project for someone familiar with or wanting to learn to use Lambda, it isn&rsquo;t simple for a non-technical person to set up. There are services out there that will delete your tweets for you, but require your access credentials. There didn&rsquo;t seem to be anything that provided convenience without also requiring authentication.
So, I went oldschool and created the ephemeral bookmarklet.
If that didn&rsquo;t make you instantly nostalgic, a bookmarklet is a little application that lives as a bookmark in your web browser. You &ldquo;install&rdquo; it by dragging the link to your bookmarks toolbar, or right-clicking on the link and choosing &ldquo;Bookmark this link&rdquo; (Firefox). You click it to execute the program on the current page.
Here&rsquo;s what the ephemeral bookmarklet will do:
The ephemeral bookmarklet is part of a new suite of tools for personal data management that I&rsquo;m co-creating with Adam Drake. You can get all the bookmarklets on this page, and they&rsquo;re also open source on GitHub.
There are currently bookmarklets for managing your data on LinkedIn and Twitter. We&rsquo;re looking for testers and contributors to help make this a comprehensive toolset for your social media data management. If you write code, I invite you to contribute and help this toolset grow.
∩{｡◕‿◕｡}∩ &ndash; Bookmarklet says hi!
`,url:"https://victoria.dev/archive/adorable-bookmarklets-want-to-help-delete-your-social-media-data/"},"https://victoria.dev/posts/a-coffee-break-introduction-to-time-complexity-of-algorithms/":{title:"A coffee-break introduction to time complexity of algorithms",tags:["algorithms","computing","go"],content:`Just like writing your very first for loop, understanding time complexity is an integral milestone to learning how to write efficient complex programs. Think of it as having a superpower that allows you to know exactly what type of program might be the most efficient in a particular situation - before even running a single line of code.
The fundamental concepts of complexity analysis are well worth studying. You&rsquo;ll be able to better understand how the code you&rsquo;re writing will interact with the program&rsquo;s input, and as a result, you&rsquo;ll spend a lot less wasted time writing slow and problematic code. It won&rsquo;t take long to go over all you need to know in order to start writing more efficient programs - in fact, we can do it in about fifteen minutes. You can go grab a coffee right now (or tea, if that&rsquo;s your thing) and I&rsquo;ll take you through it before your coffee break is over. Go ahead, I&rsquo;ll wait.
All set? Let&rsquo;s do it!
What is &ldquo;time complexity&rdquo; anyway The time complexity of an algorithm is an approximation of how long that algorithm will take to process some input. It describes the efficiency of the algorithm by the magnitude of its operations. This is different than the number of times an operation repeats; I&rsquo;ll expand on that later. Generally, the fewer operations the algorithm has, the faster it will be.
We write about time complexity using Big O notation, which looks something like O(n). There&rsquo;s rather a lot of math involved in its formal definition, but informally we can say that Big O notation gives us our algorithm&rsquo;s approximate run time in the worst case, or in other words, its upper bound.[2] It is inherently relative and comparative.[3] We&rsquo;re describing the algorithm&rsquo;s efficiency relative to the increasing size of its input data, n. If the input is a string, then n is the length of the string. If it&rsquo;s a list of integers, n is the length of the list.
It&rsquo;s easiest to picture what Big O notation represents with a graph:
Lines made with the very excellent Desmos graph calculator. You can play with this graph here.
Here are the main important points to remember as you read the rest of this article:
Time complexity is an approximation An algorithm&rsquo;s time complexity approximates its worst case run time Determining time complexity There are different classes of complexity that we can use to quickly understand an algorithm. I&rsquo;ll illustrate some of these classes using nested loops and other examples.
Polynomial time complexity A polynomial, from the Greek poly meaning &ldquo;many,&rdquo; and Latin nomen meaning &ldquo;name,&rdquo; describes an expression comprised of constant variables, and addition, multiplication, and exponentiation to a non-negative integer power.[4] That&rsquo;s a super math-y way to say that it contains variables usually denoted by letters and symbols that look like these:
The below classes describe polynomial algorithms. Some have food examples.
Constant A constant time algorithm doesn&rsquo;t change its running time in response to the input data. No matter the size of the data it receives, the algorithm takes the same amount of time to run. We denote this as a time complexity of O(1).
Here&rsquo;s one example of a constant algorithm that takes the first item in a slice.
func takeCupcake(cupcakes []int) int { return cupcakes[0] } Choice of flavours are: vanilla cupcake, strawberry cupcake, mint chocolate cupcake, lemon cupcake, and wibbly wobbly, timey wimey cupcake.
With this constant-time algorithm, no matter how many cupcakes are on offer, you just get the first one. Oh well. Flavours are overrated anyway.
Linear The running duration of a linear algorithm is constant. It will process the input in n number of operations. This is often the best possible (most efficient) case for time complexity where all the data must be examined.
Here&rsquo;s an example of code with time complexity of O(n):
func eatChips(bowlOfChips int) { for chip := 0; chip &lt;= bowlOfChips; chip++ { // dip chip } } Here&rsquo;s another example of code with time complexity of O(n):
func eatChips(bowlOfChips int) { for chip := 0; chip &lt;= bowlOfChips; chip++ { // double dip chip } } It doesn&rsquo;t matter whether the code inside the loop executes once, twice, or any number of times. Both these loops process the input by a constant factor of n, and thus can be described as linear.
Don&rsquo;t double dip in a shared bowl.
Quadratic Now here&rsquo;s an example of code with time complexity of O(n2):
func pizzaDelivery(pizzas int) { for pizza := 0; pizza &lt;= pizzas; pizza++ { // slice pizza for slice := 0; slice &lt;= pizza; slice++ { // eat slice of pizza } } } Because there are two nested loops, or nested linear operations, the algorithm process the input n2 times.
Cubic Extending on the previous example, this code with three nested loops has time complexity of O(n3):
func pizzaDelivery(boxesDelivered int) { for pizzaBox := 0; pizzaBox &lt;= boxesDelivered; pizzaBox++ { // open box for pizza := 0; pizza &lt;= pizzaBox; pizza++ { // slice pizza for slice := 0; slice &lt;= pizza; slice++ { // eat slice of pizza } } } } Seriously though, who delivers unsliced pizza??
Logarithmic A logarithmic algorithm is one that reduces the size of the input at every step. We denote this time complexity as O(log n), where log, the logarithm function, is this shape:
One example of this is a binary search algorithm that finds the position of an element within a sorted array. Here&rsquo;s how it would work, assuming we&rsquo;re trying to find the element x:
If x matches the middle element m of the array, return the position of m If x doesn&rsquo;t match m, see if m is larger or smaller than x If larger, discard all array items greater than m If smaller, discard all array items smaller than m Continue by repeating steps 1 and 2 on the remaining array until x is found I find the clearest analogy for understanding binary search is imagining the process of locating a book in a bookstore aisle. If the books are organized by author&rsquo;s last name and you want to find &ldquo;Terry Pratchett,&rdquo; you know you need to look for the &ldquo;P&rdquo; section.
You can approach the shelf at any point along the aisle and look at the author&rsquo;s last name there. If you&rsquo;re looking at a book by Neil Gaiman, you know you can ignore all the rest of the books to your left, since no letters that come before &ldquo;G&rdquo; in the alphabet happen to be &ldquo;P.&rdquo; You would then move down the aisle to the right any amount, and repeat this process until you&rsquo;ve found the Terry Pratchett section, which should be rather sizable if you&rsquo;re at any decent bookstore because wow did he write a lot of books.
Quasilinear Often seen with sorting algorithms, the time complexity O(n log n) can describe a data structure where each operation takes O(log n) time. One example of this is quick sort, a divide-and-conquer algorithm.
Quick sort works by dividing up an unsorted array into smaller chunks that are easier to process. It sorts the sub-arrays, and thus the whole array. Think about it like trying to put a deck of cards in order. It&rsquo;s faster if you split up the cards and get five friends to help you.
Non-polynomial time complexity The below classes of algorithms are non-polynomial.
Factorial An algorithm with time complexity O(n!) often iterates through all permutations of the input elements. One common example is a brute-force search seen in the travelling salesman problem. It tries to find the least costly path between a number of points by enumerating all possible permutations and finding the ones with the lowest cost.
Exponential An exponential algorithm often also iterates through all subsets of the input elements. It is denoted O(2n) and is often seen in brute-force algorithms. It is similar to factorial time except in its rate of growth, which as you may not be surprised to hear, is exponential. The larger the data set, the more steep the curve becomes.
In cryptography, a brute-force attack may systematically check all possible elements of a password by iterating through subsets. Using an exponential algorithm to do this, it becomes incredibly resource-expensive to brute-force crack a long password versus a shorter one. This is one reason that a long password is considered more secure than a shorter one.
There are further time complexity classes less commonly seen that I won&rsquo;t cover here, but you can read about these and find examples in this handy table.
Recursion time complexity As I described in my article explaining recursion using apple pie, a recursive function calls itself under specified conditions. Its time complexity depends on how many times the function is called and the time complexity of a single function call. In other words, it&rsquo;s the product of the number of times the function runs and a single execution&rsquo;s time complexity.
Here&rsquo;s a recursive function that eats pies until no pies are left:
func eatPies(pies int) int { if pies == 0 { return pies } return eatPies(pies - 1) } The time complexity of a single execution is constant. No matter how many pies are input, the program will do the same thing: check to see if the input is 0. If so, return, and if not, call itself with one fewer pie.
The initial number of pies could be any number, and we need to process all of them, so we can describe the input as n. Thus, the time complexity of this recursive function is the product O(n).
This function&rsquo;s return value is zero, plus some indigestion.
Worst case time complexity So far, we&rsquo;ve talked about the time complexity of a few nested loops and some code examples. Most algorithms, however, are built from many combinations of these. How do we determine the time complexity of an algorithm containing many of these elements strung together?
Easy. We can describe the total time complexity of the algorithm by finding the largest complexity among all of its parts. This is because the slowest part of the code is the bottleneck, and time complexity is concerned with describing the worst case for the algorithm&rsquo;s run time.
Say we have a program for an office party. If our program looks like this:
package main import &#34;fmt&#34; func takeCupcake(cupcakes []int) int { fmt.Println(&#34;Have cupcake number&#34;,cupcakes[0]) return cupcakes[0] } func eatChips(bowlOfChips int) { fmt.Println(&#34;Have some chips!&#34;) for chip := 0; chip &lt;= bowlOfChips; chip++ { // dip chip } fmt.Println(&#34;No more chips.&#34;) } func pizzaDelivery(boxesDelivered int) { fmt.Println(&#34;Pizza is here!&#34;) for pizzaBox := 0; pizzaBox &lt;= boxesDelivered; pizzaBox++ { // open box for pizza := 0; pizza &lt;= pizzaBox; pizza++ { // slice pizza for slice := 0; slice &lt;= pizza; slice++ { // eat slice of pizza } } } fmt.Println(&#34;Pizza is gone.&#34;) } func eatPies(pies int) int { if pies == 0 { fmt.Println(&#34;Someone ate all the pies!&#34;) return pies } fmt.Println(&#34;Eating pie...&#34;) return eatPies(pies - 1) } func main() { takeCupcake([]int{1, 2, 3}) eatChips(23) pizzaDelivery(3) eatPies(3) fmt.Println(&#34;Food gone. Back to work!&#34;) } We can describe the time complexity of all the code by the complexity of its most complex part. This program is made up of functions we&rsquo;ve already seen, with the following time complexity classes:
Function Class Big O takeCupcake constant O(1) eatChips linear O(n) pizzaDelivery cubic O(n3) eatPies linear (recursive) O(n) To describe the time complexity of the entire office party program, we choose the worst case. This program would have the time complexity O(n3).
Here&rsquo;s the office party soundtrack, just for fun.
Have cupcake number 1 Have some chips! No more chips. Pizza is here! Pizza is gone. Eating pie... Eating pie... Eating pie... Someone ate all the pies! Food gone. Back to work! P vs NP, NP-complete, and NP-hard You may come across these terms in your explorations of time complexity. Informally, P (for Polynomial time), is a class of problems that is quick to solve. NP, for Nondeterministic Polynomial time, is a class of problems where the answer can be quickly verified in polynomial time. NP encompasses P, but also another class of problems called NP-complete, for which no fast solution is known.[5] Outside of NP but still including NP-complete is yet another class called NP-hard, which includes problems that no one has been able to verifiably solve with polynomial algorithms.[6]
P vs NP Euler diagram, by Behnam Esfahbod, CC BY-SA 3.0
P versus NP is an unsolved, open question in computer science.
Anyway, you don&rsquo;t generally need to know about NP and NP-hard problems to begin taking advantage of understanding time complexity. They&rsquo;re a whole other Pandora&rsquo;s box.
Approximate the efficiency of an algorithm before you write the code So far, we&rsquo;ve identified some different time complexity classes and how we might determine which one an algorithm falls into. So how does this help us before we&rsquo;ve written any code to evaluate?
By combining a little knowledge of time complexity with an awareness of the size of our input data, we can take a guess at an efficient algorithm for processing our data within a given time constraint. We can base our estimation on the fact that a modern computer can perform some hundreds of millions of operations in a second.[1] The following table from the Competitive Programmer&rsquo;s Handbook offers some estimates on required time complexity to process the respective input size in a time limit of one second.
Input size Required time complexity for 1s processing time n ≤ 10 O(n!) n ≤ 20 O(2n) n ≤ 500 O(n3) n ≤ 5000 O(n2) n ≤ 106 O(n log n) or O(n) n is large O(1) or O(log n) Keep in mind that time complexity is an approximation, and not a guarantee. We can save a lot of time and effort by immediately ruling out algorithm designs that are unlikely to suit our constraints, but we must also consider that Big O notation doesn&rsquo;t account for constant factors. Here&rsquo;s some code to illustrate.
The following two algorithms both have O(n) time complexity.
func makeCoffee(scoops int) { for scoop := 0; scoop &lt;= scoops; scoop++ { // add instant coffee } } func makeStrongCoffee(scoops int) { for scoop := 0; scoop &lt;= 3*scoops; scoop++ { // add instant coffee } } The first function makes a cup of coffee with the number of scoops we ask for. The second function also makes a cup of coffee, but it triples the number of scoops we ask for. To see an illustrative example, let&rsquo;s ask both these functions for a cup of coffee with a million scoops.
Here&rsquo;s the output of the Go test:
Benchmark_makeCoffee-4 1000000000 0.29 ns/op Benchmark_makeStrongCoffee-4 1000000000 0.86 ns/op Our first function, makeCoffee, completed in an average 0.29 nanoseconds. Our second function, makeStrongCoffee, completed in an average of 0.86 nanoseconds. While those may both seem like pretty small numbers, consider that the stronger coffee took near three times longer to make. This should make sense intuitively, since we asked it to triple the scoops. Big O notation alone wouldn&rsquo;t tell you this, since the constant factor of the tripled scoops isn&rsquo;t accounted for.
Improve time complexity of existing code Becoming familiar with time complexity gives us the opportunity to write code, or refactor code, to be more efficient. To illustrate, I&rsquo;ll give a concrete example of one way we can refactor a bit of code to improve its time complexity.
Let&rsquo;s say a bunch of people at the office want some pie. Some people want pie more than others. The amount that everyone wants some pie is represented by an int &gt; 0:
diners := []int{2, 88, 87, 16, 42, 10, 34, 1, 43, 56} Unfortunately, we&rsquo;re bootstrapped and there are only three forks to go around. Since we&rsquo;re a cooperative bunch, the three people who want pie the most will receive the forks to eat it with. Even though they&rsquo;ve all agreed on this, no one seems to want to sort themselves out and line up in an orderly fashion, so we&rsquo;ll have to make do with everybody jumbled about.
Without sorting the list of diners, return the three largest integers in the slice.
Here&rsquo;s a function that solves this problem and has O(n2) time complexity:
func giveForks(diners []int) []int { // make a slice to store diners who will receive forks var withForks []int // loop over three forks for i := 1; i &lt;= 3; i++ { // variables to keep track of the highest integer and where it is var max, maxIndex int // loop over the diners slice for n := range diners { // if this integer is higher than max, update max and maxIndex if diners[n] &gt; max { max = diners[n] maxIndex = n } } // remove the highest integer from the diners slice for the next loop diners = append(diners[:maxIndex], diners[maxIndex+1:]...) // keep track of who gets a fork withForks = append(withForks, max) } return withForks } This program works, and eventually returns diners [88 87 56]. Everyone gets a little impatient while it&rsquo;s running though, since it takes rather a long time (about 120 nanoseconds) just to hand out three forks, and the pie&rsquo;s getting cold. How could we improve it?
By thinking about our approach in a slightly different way, we can refactor this program to have O(n) time complexity:
func giveForks(diners []int) []int { // make a slice to store diners who will receive forks var withForks []int // create variables for each fork var first, second, third int // loop over the diners for i := range diners { // assign the forks if diners[i] &gt; first { third = second second = first first = diners[i] } else if diners[i] &gt; second { third = second second = diners[i] } else if diners[i] &gt; third { third = diners[i] } } // list the final result of who gets a fork withForks = append(withForks, first, second, third) return withForks } Here&rsquo;s how the new program works:
Initially, diner 2 (the first in the list) is assigned the first fork. The other forks remain unassigned.
Then, diner 88 is assigned the first fork instead. Diner 2 gets the second one.
Diner 87 isn&rsquo;t greater than first which is currently 88, but it is greater than 2 who has the second fork. So, the second fork goes to 87. Diner 2 gets the third fork.
Continuing in this violent and rapid fork exchange, diner 16 is then assigned the third fork instead of 2, and so on.
We can add a print statement in the loop to see how the fork assignments play out:
0 0 0 2 0 0 88 2 0 88 87 2 88 87 16 88 87 42 88 87 42 88 87 42 88 87 42 88 87 43 [88 87 56] This program is much faster, and the whole epic struggle for fork domination is over in 47 nanoseconds.
As you can see, with a little change in perspective and some refactoring, we&rsquo;ve made this simple bit of code faster and more efficient.
Well, it looks like our fifteen minute coffee break is up! I hope I&rsquo;ve given you a comprehensive introduction to calculating time complexity. Time to get back to work, hopefully applying your new knowledge to write more effective code! Or maybe just sound smart at your next office party. :)
References &ldquo;If I have seen further it is by standing on the shoulders of Giants.&rdquo; &ndash;Isaac Newton, 1675
Antti Laaksonen. Competitive Programmer&rsquo;s Handbook (pdf), 2017 Wikipedia: Big O notation StackOverflow: What is a plain English explanation of “Big O” notation? Wikipedia: Polynomial Wikipedia: NP-completeness Wikipedia: NP-hardness Desmos graph calculator `,url:"https://victoria.dev/posts/a-coffee-break-introduction-to-time-complexity-of-algorithms/"},"https://victoria.dev/posts/knapsack-problem-algorithms-for-my-real-life-carry-on-knapsack/":{title:"Knapsack problem algorithms for my real-life carry-on knapsack",tags:["algorithms","data","go"],content:`The knapsack problem I&rsquo;m a nomad and live out of one carry-on bag. This means that the total weight of all my worldly possessions must fall under airline cabin baggage weight limits - usually 10kg. On some smaller airlines, however, this weight limit drops to 7kg. Occasionally, I have to decide not to bring something with me to adjust to the smaller weight limit.
As a practical exercise, deciding what to leave behind (or get rid of altogether) entails laying out all my things and choosing which ones to keep. That decision is based on the item&rsquo;s usefulness to me (its worth) and its weight.
This is all my stuff, and my Minaal Carry-on bag.
Being a programmer, I&rsquo;m aware that decisions like this could be made more efficiently by a computer. It&rsquo;s done so frequently and so ubiquitously, in fact, that many will recognize this scenario as the classic packing problem or knapsack problem. How do I go about telling a computer to put as many important items in my bag as possible while coming in at or under a weight limit of 7kg? With algorithms! Yay!
I&rsquo;ll discuss two common approaches to solving the knapsack problem: one called a greedy algorithm, and another called dynamic programming (a little harder, but better, faster, stronger&hellip;).
Let&rsquo;s get to it.
The set up I prepared my data in the form of a CSV file with three columns: the item&rsquo;s name (a string), a representation of its worth (an integer), and its weight in grams (an integer). There are 40 items in total. I represented worth by ranking each item from 40 to 1, with 40 being the most important and 1 equating with something like &ldquo;why do I even have this again?&rdquo; (If you&rsquo;ve never listed out all your possessions and ranked them by order of how useful they are to you, I highly recommend you try it. It can be a very revealing exercise.)
Total weight of all items and bag: 9003g
Bag weight: 1415g
Airline limit: 7000g
Maximum weight of items I can pack: 5585g
Total possible worth of items: 820
The challenge: Pack as many items as the limit allows while maximizing the total worth.
Data structures Reading in a file Before we can begin thinking about how to solve the knapsack problem, we have to solve the problem of reading in and storing our data. Thankfully, the Go standard library&rsquo;s io/ioutil package makes the first part straightforward.
package main import ( &#34;fmt&#34; &#34;io/ioutil&#34; ) func check(e error) { if e != nil { panic(e) } } func readItems(path string) { dat, err := ioutil.ReadFile(path) check(err) fmt.Print(string(dat)) } The ReadFile() function takes a file path and returns the file&rsquo;s contents and an error (nil if the call is successful) so we&rsquo;ve also created a check() function to handle any errors that might be returned. In a real-world application we probably would want to do something more sophisticated than panic, but that&rsquo;s not important right now.
Creating a struct Now that we&rsquo;ve got our data, we should probably do something with it. Since we&rsquo;re working with real-life items and a real-life bag, let&rsquo;s create some types to represent them and make it easier to conceptualize our program. A struct in Go is a typed collection of fields. Here are our two types:
type item struct { name string worth, weight int } type bag struct { bagWeight, currItemsWeight, maxItemsWeight, totalWeight int items []item } It is helpful to use field names that are very descriptive. You can see that the structs are set up just as we&rsquo;ve described the things they represent. An item has a name (string), and a worth and weight (integers). A bag has several fields of type int representing its attributes, and also has the ability to hold items, represented in the struct as a slice of item type thingamabobbers.
Parsing and storing our data Several comprehensive Go packages exist that we could use to parse our CSV data&hellip; but where&rsquo;s the fun in that? Let&rsquo;s go basic with some string splitting and a for loop. Here&rsquo;s our updated readItems() function:
func readItems(path string) []item { dat, err := ioutil.ReadFile(path) check(err) lines := strings.Split(string(dat), &#34;\\n&#34;) itemList := make([]item, 0) for i, v := range lines { if i == 0 { continue } s := strings.Split(v, &#34;,&#34;) newItemWorth, _ := strconv.Atoi(s[1]) newItemWeight, _ := strconv.Atoi(s[2]) newItem := item{name: s[0], worth: newItemWorth, weight: newItemWeight} itemList = append(itemList, newItem) } return itemList } Using strings.Split, we split our dat on newlines. We then create an empty itemList to hold our items.
In our for loop, we skip the first line of our CSV file (the headers) then iterate over each line. We use strconv.Atoi (read &ldquo;A to i&rdquo;) to convert the values for each item&rsquo;s worth and weight into integers. We then create a newItem with these field values and append it to the itemList. Finally, we return itemList.
Here&rsquo;s what our set up looks like so far:
package main import ( &#34;io/ioutil&#34; &#34;strconv&#34; &#34;strings&#34; ) type item struct { name string worth, weight int } type bag struct { bagWeight, currItemsWeight, maxItemsWeight, totalWeight, totalWorth int items []item } func check(e error) { if e != nil { panic(e) } } func readItems(path string) []item { dat, err := ioutil.ReadFile(path) check(err) lines := strings.Split(string(dat), &#34;\\n&#34;) itemList := make([]item, 0) for i, v := range lines { if i == 0 { continue // skip the headers on the first line } s := strings.Split(v, &#34;,&#34;) newItemWorth, _ := strconv.Atoi(s[1]) newItemWeight, _ := strconv.Atoi(s[2]) newItem := item{name: s[0], worth: newItemWorth, weight: newItemWeight} itemList = append(itemList, newItem) } return itemList } Now that we&rsquo;ve got our data structures set up, let&rsquo;s get packing (🥁) on the first approach.
Greedy algorithm A greedy algorithm is the most straightforward approach to solving the knapsack problem, in that it is a one-pass algorithm that constructs a single final solution. At each stage of the problem, the greedy algorithm picks the option that is locally optimal, meaning it looks like the most suitable option right now. It does not revise its previous choices as it progresses through our data set.
Building our greedy algorithm The steps of the algorithm we&rsquo;ll use to solve our knapsack problem are:
Sort items by worth, in descending order. Start with the highest worth item. Put items into the bag until the next item on the list cannot fit. Try to fill any remaining capacity with the next item on the list that can fit. If you read my article about solving problems and making paella, you&rsquo;ll know that I always start by figuring out what the next most important question is. In this case, there are three main operations we need to figure out how to do:
Sort items by worth. Put an item in the bag. Check to see if the bag is full. The first one is just a docs lookup away. Here&rsquo;s how we sort a slice in Go:
sort.Slice(is, func(i, j int) bool { return is[i].worth &gt; is[j].worth }) The sort.Slice() function orders our items according to the less function we provide. In this case, it will order the highest worth items before the lowest worth items.
Given that we don&rsquo;t want to put an item in the bag if it doesn&rsquo;t fit, we&rsquo;ll complete the last two tasks in reverse. First, we&rsquo;ll check to see if the item fits. If so, it goes in the bag.
func (b *bag) addItem(i item) error { if b.currItemsWeight+i.weight &lt;= b.maxItemsWeight { b.currItemsWeight += i.weight b.items = append(b.items, i) return nil } return errors.New(&#34;could not fit item&#34;) } Notice the * in our first line there. That indicates that bag is a pointer receiver (as opposed to a value receiver). It&rsquo;s a concept that can be slightly confusing if you&rsquo;re new to Go. Here are some things to consider that might help you decide when to use a value receiver and when to use a pointer receiver. For the purposes of our addItem() function, this case applies:
If the method needs to mutate the receiver, the receiver must be a pointer.
Our use of a pointer receiver tells our function we want to operate on this specific bag in particular, not a new bag. It&rsquo;s important because without it, every item would always fit in a newly created bag! A little detail like this can make the difference between code that works and code that keeps you up until 4am chugging Red Bull and muttering to yourself. (Go to bed on time even if your code doesn&rsquo;t work - you&rsquo;ll thank me later.)
Now that we&rsquo;ve got our components, let&rsquo;s put together our greedy algorithm:
func greedy(is []item, b bag) { sort.Slice(is, func(i, j int) bool { return is[i].worth &gt; is[j].worth }) for i := range is { b.addItem(is[i]) } b.totalWeight = b.bagWeight + b.currItemsWeight for _, v := range b.items { b.totalWorth += v.worth } } Then in our main() function, we&rsquo;ll create our bag, read in our data, and call our greedy algorithm. Here&rsquo;s what it looks like, all set up and ready to go:
func main() { minaal := bag{bagWeight: 1415, currItemsWeight: 0, maxItemsWeight: 5585} itemList := readItems(&#34;objects.csv&#34;) greedy(itemList, minaal) } Greedy algorithm results So how does this algorithm do when it comes to efficiently packing our bag to maximize its total worth? Here&rsquo;s the result:
Total weight of bag and items: 6987g
Total worth of packed items: 716
Here are the items our greedy algorithm chose, sorted by worth:
Item Worth Weight Lenovo X1 Carbon (5th Gen) 40 112 10 pairs thongs 39 80 5 Underarmour Strappy 38 305 1 pair Uniqlo leggings 37 185 2 Lululemon Cool Racerback 36 174 Chargers and cables in Mini Bomber Travel Kit 35 665 The Roost Stand 34 170 ThinkPad Compact Bluetooth Keyboard with trackpoint 33 460 Seagate Backup PlusSlim 32 159 1 pair black denim shorts 31 197 2 pairs Nike Pro shorts 30 112 2 pairs Lululemon shorts 29 184 Isabella T-Strap Croc sandals 28 200 2 Underarmour HeatGear CoolSwitch tank tops 27 138 5 pairs black socks 26 95 2 pairs Injinji Women&rsquo;s Run Lightweight No-Show Toe Socks 25 54 1 fancy tank top 24 71 1 light and stretchylong-sleeve shirt (Gap Fit) 23 147 Uniqlo Ultralight Down insulating jacket 22 235 Patagonia Torrentshell 21 301 Lightweight Merino Wool Buff 20 50 1 LBD (H&amp;M) 19 174 Field Notes Pitch Black Memo Book Dot-Graph 18 68 Innergie PocketCell USB-C 6000mAh power bank 17 14 JBL Reflect Mini Bluetooth Sport Headphones 13 14 Oakley Latch Sunglasses 11 30 Petzl E+LITE Emergency Headlamp 8 27 It&rsquo;s clear that the greedy algorithm is a straightforward way to quickly find a feasible solution. For small data sets, it will probably be close to the optimal solution. The algorithm packed a total item worth of 716 (104 points less than the maximum possible value), while filling the bag with just 13g left over.
As we learned earlier, the greedy algorithm doesn&rsquo;t improve upon the solution it returns. It simply adds the next highest worth item it can to the bag.
Let&rsquo;s look at another method for solving the knapsack problem that will give us the optimal solution - the highest possible total worth under the weight limit.
Dynamic programming The name &ldquo;dynamic programming&rdquo; can be a bit misleading. It&rsquo;s not a style of programming, as the name might cause you to infer, but simply another approach.
Dynamic programming differs from the straightforward greedy algorithm in a few key ways. Firstly, a dynamic programming bag packing solution enumerates the entire solution space with all possibilities of item combinations that could be used to pack our bag. Where a greedy algorithm chooses the most optimal local solution, dynamic programming algorithms are able to find the most optimal global solution.
Secondly, dynamic programming uses memoization to store the results of previously computed operations and returns the cached result when the operation occurs again. This allows it to &ldquo;remember&rdquo; previous combinations. This takes less time than it would to re-compute the answer again.
Building our dynamic programming algorithm To use dynamic programming to find the optimal recipe for packing our bag, we&rsquo;ll need to:
Create a matrix representing all subsets of the items (the solution space) with rows representing items and columns representing the bag&rsquo;s remaining weight capacity Loop through the matrix and calculate the worth that can be obtained by each combination of items at each stage of the bag&rsquo;s capacity Examine the completed matrix to determine which items to add to the bag in order to produce the maximum possible worth for the bag in total It will be most helpful to visualize our solution space. Here&rsquo;s a representation of what we&rsquo;re building with our code:
The empty knapsackian multiverse.
In Go, we can create this matrix as a slice of slices.
matrix := make([][]int, numItems+1) // rows representing items for i := range matrix { matrix[i] = make([]int, capacity+1) // columns representing grams of weight } We&rsquo;ve padded the rows and columns by 1 so that the indicies match the item and weight numbers.
Now that we&rsquo;ve created our matrix, we&rsquo;ll fill it by looping over the rows and the columns:
// loop through table rows for i := 1; i &lt;= numItems; i++ { // loop through table columns for w := 1; w &lt;= capacity; w++ { // do stuff in each element } } Then for each element, we&rsquo;ll calculate the worth value to ascribe to it. We do this with code that represents the following:
If the item at the index matching the current row fits within the weight capacity represented by the current column, take the maximum of either:
The total worth of the items already in the bag or, The total worth of all the items in the bag except the item at the previous row index, plus the new item&rsquo;s worth In other words, as our algorithm considers one of the items, we&rsquo;re asking it to decide whether this item added to the bag would produce a higher total worth than the last item it added to the bag, at the bag&rsquo;s current total weight. If this current item is a better choice, put it in - if not, leave it out.
Here&rsquo;s the code that accomplishes this:
// if weight of item matching this index can fit at the current capacity column... if is[i-1].weight &lt;= w { // worth of this subset without this item valueOne := float64(matrix[i-1][w]) // worth of this subset without the previous item, and this item instead valueTwo := float64(is[i-1].worth + matrix[i-1][w-is[i-1].weight]) // take maximum of either valueOne or valueTwo matrix[i][w] = int(math.Max(valueOne, valueTwo)) // if the new worth is not more, carry over the previous worth } else { matrix[i][w] = matrix[i-1][w] } This process of comparing item combinations will continue until every item has been considered at every possible stage of the bag&rsquo;s increasing total weight. When all the above have been considered, we&rsquo;ll have enumerated the solution space - filled the matrix - with all possible total worth values.
We&rsquo;ll have a big chart of numbers, and in the last column at the last row we&rsquo;ll have our highest possible value.
A strictly representative representation of the filled matrix.
That&rsquo;s great, but how do we find out which combination of items were put in the bag to achieve that worth?
Getting our optimized item list To see which items combine to create our optimal packing list, we&rsquo;ll need to examine our matrix in reverse to the way we created it. Since we know the highest possible value is in the last row in the last column, we&rsquo;ll start there. To find the items, we:
Get the value of the current cell Compare the value of the current cell to the value in the cell directly above it If the values differ, there was a change to the bag items; find the next cell to examine by moving backwards through the columns according to the current item&rsquo;s weight (find the value of the bag before this current item was added) If the values match, there was no change to the bag items; move up to the cell in the row above and repeat The nature of the action we&rsquo;re trying to achieve lends itself well to a recursive function. If you recall from my previous article about making apple pie, recursive functions are simply functions that call themselves under certain conditions. Here&rsquo;s what it looks like:
func checkItem(b *bag, i int, w int, is []item, matrix [][]int) { if i &lt;= 0 || w &lt;= 0 { return } pick := matrix[i][w] if pick != matrix[i-1][w] { b.addItem(is[i-1]) checkItem(b, i-1, w-is[i-1].weight, is, matrix) } else { checkItem(b, i-1, w, is, matrix) } } Our checkItem() function calls itself if the condition we described in step 4 is true. If step 3 is true, it also calls itself, but with different arguments.
Recursive functions require a base case. In this example, we want the function to stop once we run out of values of worth to compare. Thus our base case is when either i or w are 0.
Here&rsquo;s how the dynamic programming approach looks when it&rsquo;s all put together:
func checkItem(b *bag, i int, w int, is []item, matrix [][]int) { if i &lt;= 0 || w &lt;= 0 { return } pick := matrix[i][w] if pick != matrix[i-1][w] { b.addItem(is[i-1]) checkItem(b, i-1, w-is[i-1].weight, is, matrix) } else { checkItem(b, i-1, w, is, matrix) } } func dynamic(is []item, b *bag) *bag { numItems := len(is) // number of items in knapsack capacity := b.maxItemsWeight // capacity of knapsack // create the empty matrix matrix := make([][]int, numItems+1) // rows representing items for i := range matrix { matrix[i] = make([]int, capacity+1) // columns representing grams of weight } // loop through table rows for i := 1; i &lt;= numItems; i++ { // loop through table columns for w := 1; w &lt;= capacity; w++ { // if weight of item matching this index can fit at the current capacity column... if is[i-1].weight &lt;= w { // worth of this subset without this item valueOne := float64(matrix[i-1][w]) // worth of this subset without the previous item, and this item instead valueTwo := float64(is[i-1].worth + matrix[i-1][w-is[i-1].weight]) // take maximum of either valueOne or valueTwo matrix[i][w] = int(math.Max(valueOne, valueTwo)) // if the new worth is not more, carry over the previous worth } else { matrix[i][w] = matrix[i-1][w] } } } checkItem(b, numItems, capacity, is, matrix) // add other statistics to the bag b.totalWorth = matrix[numItems][capacity] b.totalWeight = b.bagWeight + b.currItemsWeight return b } Dynamic programming results We expect that the dynamic programming approach will give us a more optimized solution than the greedy algorithm. So did it? Here are the results:
Total weight of bag and items: 6982g
Total worth of packed items: 757
Here are the items our dynamic programming algorithm chose, sorted by worth:
Item Worth Weight 10 pairs thongs 39 80 5 Underarmour Strappy 38 305 1 pair Uniqlo leggings 37 185 2 Lululemon Cool Racerback 36 174 Chargers and cables in Mini Bomber Travel Kit 35 665 The Roost Stand 34 170 ThinkPad Compact Bluetooth Keyboard with trackpoint 33 460 Seagate Backup Plus Slim 32 159 1 pair black denim shorts 31 197 2 pairs Nike Pro shorts 30 112 2 pairs Lululemon shorts 29 184 Isabella T-Strap Croc sandals 28 200 2 Underarmour HeatGear CoolSwitch tank tops 27 138 5 pairs black socks 26 95 2 pairs Injinji Women&rsquo;s Run Lightweight No-Show Toe Socks 25 54 1 fancy tank top 24 71 1 light and stretchy long-sleeve shirt (Gap Fit) 23 147 Uniqlo Ultralight Down insulating jacket 22 235 Patagonia Torrentshell 21 301 Lightweight Merino Wool Buff 20 50 1 LBD (H&amp;M) 19 174 Field Notes Pitch Black Memo Book Dot-Graph 18 68 Innergie PocketCell USB-C 6000mAh power bank 17 148 Important papers 16 228 Deuter First Aid Kit Active 15 144 Stanley Classic Vacuum Camp Mug 16oz 14 454 JBL Reflect Mini Bluetooth Sport Headphones 13 14 Anker SoundCore nano Bluetooth Speaker 12 80 Oakley Latch Sunglasses 11 30 Ray Ban Wayfarer Classic 10 45 Petzl E+LITE Emergency Headlamp 8 27 Peak Design Cuff Camera Wrist Strap 6 26 Travelon Micro Scale 5 125 Humangear GoBites Duo 3 22 There&rsquo;s an obvious improvement to our dynamic programming solution over what the greedy algorithm gave us. Our total worth of 757 is 41 points greater than the greedy algorithm&rsquo;s solution of 716, and for a few grams less weight too!
Input sort order While testing my dynamic programming solution, I implemented the Fisher-Yates shuffle algorithm on the input before passing it into my function, just to ensure that the answer wasn&rsquo;t somehow dependent on the sort order of the input. Here&rsquo;s what the shuffle looks like in Go:
rand.Seed(time.Now().UnixNano()) for i := range itemList { j := rand.Intn(i + 1) itemList[i], itemList[j] = itemList[j], itemList[i] } Of course I then realized that Go 1.10 now has a built-in shuffle&hellip; it works precisely the same way and looks like this:
rand.Shuffle(len(itemList), func(i, j int) { itemList[i], itemList[j] = itemList[j], itemList[i] }) So did the order in which the items were processed affect the outcome? Well&hellip;
Suddenly&hellip; a rogue weight appears! As it turns out, in a way, the answer did depend on the order of the input. When I ran my dynamic programming algorithm several times, I sometimes saw a different total weight for the bag, though the total worth remained at 757. I initially thought this was a bug before examining the two sets of items that accompanied the two different total weight values. Everything was the same except for a few changes that collectively added up to a different item subset accounting for 14 of the 757 worth points.
In this case, there were two equally optimal solutions based only on the success metric of the highest total possible worth. Shuffling the input seemed to affect the placement of the items in the matrix and thus, the path that the checkItem() function took as it went through the matrix to find the chosen items. Since the success metric of having the highest possible worth was the same in both item sets, we don&rsquo;t have a single unique solution - there&rsquo;s two!
As an academic exercise, both these sets of items are correct answers. We may choose to optimize further by another metric, say, the total weight of all the items. The highest possible worth at the least possible weight could be seen as an ideal solution.
Here&rsquo;s the second, lighter, dynamic programming result:
Total weight of bag and items: 6955g
Total worth of packed items: 757
Item Worth Weight 10 pairs thongs 39 80 5 Underarmour Strappy 38 305 1 pair Uniqlo leggings 37 185 2 Lululemon Cool Racerback 36 174 Chargers and cables in Mini Bomber Travel Kit 35 665 The Roost Stand 34 170 ThinkPad Compact Bluetooth Keyboard with trackpoint 33 460 Seagate Backup Plus Slim 32 159 1 pair black denim shorts 31 197 2 pairs Nike Pro shorts 30 112 2 pairs Lululemon shorts 29 184 Isabella T-Strap Croc sandals 28 200 2 Underarmour HeatGear CoolSwitch tank tops 27 138 5 pairs black socks 26 95 2 pairs Injinji Women&rsquo;s Run Lightweight No-Show Toe Socks 25 54 1 fancy tank top 24 71 1 light and stretchy long-sleeve shirt (Gap Fit) 23 147 Uniqlo Ultralight Down insulating jacket 22 235 Patagonia Torrentshell 21 301 Lightweight Merino Wool Buff 20 50 1 LBD (H&amp;M) 19 174 Field Notes Pitch Black Memo Book Dot-Graph 18 68 Innergie PocketCell USB-C 6000mAh power bank 17 148 Important papers 16 228 Deuter First Aid Kit Active 15 144 JBL Reflect Mini Bluetooth Sport Headphones 13 14 Anker SoundCore nano Bluetooth Speaker 12 80 Oakley Latch Sunglasses 11 30 Ray Ban Wayfarer Classic 10 45 Zip bag of toiletries 9 236 Petzl E+LITE Emergency Headlamp 8 27 Peak Design Cuff Camera Wrist Strap 6 26 Travelon Micro Scale 5 125 BlitzWolf Bluetooth Tripod/Monopod 4 150 Humangear GoBites Duo 3 22 Vapur Bottle 1L 1 41 Which approach is better? Go benchmarking The Go standard library&rsquo;s testing package makes it straightforward for us to benchmark these two approaches. We can find out how long it takes each algorithm to run, and how much memory each uses. Here&rsquo;s a simple main_test.go file:
package main import ( &#34;testing&#34; ) func Benchmark_greedy(b *testing.B) { itemList := readItems(&#34;objects.csv&#34;) for i := 0; i &lt; b.N; i++ { minaal := bag{bagWeight: 1415, currItemsWeight: 0, maxItemsWeight: 5585} greedy(itemList, minaal) } } func Benchmark_dynamic(b *testing.B) { itemList := readItems(&#34;objects.csv&#34;) for i := 0; i &lt; b.N; i++ { minaal := bag{bagWeight: 1415, currItemsWeight: 0, maxItemsWeight: 5585} dynamic(itemList, &amp;minaal) } } We can run go test -bench=. -benchmem to see these results:
Benchmark_greedy-4 1000000 1619 ns/op 2128 B/op 9 allocs/op Benchmark_dynamic-4 1000 1545322 ns/op 2020332 B/op 49 allocs/op Greedy algorithm performance After running the greedy algorithm 1,000,000 times, the speed of the algorithm was reliably measured to be 0.001619 milliseconds (translation: very fast). It required 2128 Bytes or 2-ish kilobytes of memory and 9 distinct memory allocations per iteration.
Dynamic programming performance The dynamic programming algorithm was run 1,000 times. Its speed was measured to be 1.545322 milliseconds or 0.001545322 seconds (translation: still pretty fast). It required 2,020,332 Bytes or 2-ish Megabytes, and 49 distinct memory allocations per iteration.
The verdict Part of choosing the right approach to solving any programming problem is taking into account the size of the input data set. In this case, it&rsquo;s a small one. In this scenario, a one-pass greedy algorithm will always be faster and less resource-needy than dynamic programming, simply because it has fewer steps. Our greedy algorithm was almost two orders of magnitude faster and less memory-hungry than our dynamic programming algorithm.
Not having those extra steps, however, means that getting the best possible solution from the greedy algorithm is unlikely.
It&rsquo;s clear that the dynamic programming algorithm gave us better numbers: a lower weight, and higher overall worth.
Greedy algorithm Dynamic programming Total weight: 6987g 6955g Total worth: 716 757 Where dynamic programming on small data sets lacks in performance, it makes up in optimization. The question then becomes whether that additional optimization is worth the performance cost.
&ldquo;Better,&rdquo; of course, is a subjective judgement. If speed and low resource usage is our success metric, then the greedy algorithm is clearly better. If the total worth of items in the bag is our success metric, then dynamic programming is clearly better. However, our scenario is a practical one, and only one of these algorithm designs returned an answer I&rsquo;d choose. In optimizing for the overall greatest possible total worth of the items in the bag, the dynamic programming algorithm left out my highest-worth, but also heaviest, item: my laptop. The chargers and cables, Roost stand, and keyboard that were included aren&rsquo;t much use without it.
Better algorithm design There&rsquo;s a simple way to alter the dynamic programming approach so that the laptop is always included: we can modify the data so that the worth of the laptop is greater than the sum of the worth of all the other items. (Try it out!)
Perhaps in re-designing the dynamic programming algorithm to be more practical, we might choose another success metric that better reflects an item&rsquo;s importance, instead of a subjective worth value. There are many possible metrics we can use to represent the value of an item. Here are a few examples of a good proxy:
Amount of time spent using the item Initial cost of purchasing the item Cost of replacement if the item were lost today Dollar value of the product of using the item By the same token, the greedy algorithm&rsquo;s results might be improved with the use of one of these alternate metrics.
On top of choosing an appropriate approach to solving the knapsack problem in general, it is helpful to design our algorithm in a way that translates the practicalities of a scenario into code.
There are many considerations for better algorithm design beyond the scope of this introductory post. One of these is time complexity, and I&rsquo;ve written about it here. A future algorithm may very well decide my bag&rsquo;s contents on the next trip, but we&rsquo;re not quite there yet. Stay tuned!
`,url:"https://victoria.dev/posts/knapsack-problem-algorithms-for-my-real-life-carry-on-knapsack/"},"https://victoria.dev/archive/running-a-free-twitter-bot-on-aws-lambda/":{title:"Running a free Twitter bot on AWS Lambda",tags:["aws","go"],content:`If you read About time, you&rsquo;ll know that I&rsquo;m a big believer in spending time now on building things that save time in the future. To this end I built a simple Twitter bot in Go that would occasionally post links to my articles and keep my account interesting even when I&rsquo;m too busy to use it. The tweets help drive traffic to my sites, and I don&rsquo;t have to lift a finger.
I ran the bot on an Amazon EC2 instance for about a month. My AWS usage has historically been pretty inexpensive (less than the price of a coffee in most of North America), so I was surprised when the little instance I was using racked up a bill 90% bigger than the month before. I don&rsquo;t think AWS is expensive, to be clear, but still&hellip; I&rsquo;m cheap. I want my Twitter bot, and I want it for less.
I&rsquo;d been meaning to explore AWS Lamda, and figured this was a good opportunity. Unlike an EC2 instance that is constantly running (and charging you for it), Lambda charges you per request and according to the duration of time your function takes to run. There&rsquo;s a free tier, too, and the first 1 million requests, plus a certain amount of compute time, are free. Roughly translated to running a Twitter bot that posts for you, say, twice a day, your monthly cost for using Lambda would total&hellip; carry the one&hellip; nothing. I&rsquo;ve been running my Lambda function for a couple weeks now, completely free.
When recently it came to me to take the reigns of the @freeCodeCampTO Twitter, I decided to employ a similar strategy, and also use this opportunity to document the process for you, dear reader.
So if you&rsquo;re currently using a full-time running instance for a task that could be served by a cron job, this is the article for you. I&rsquo;ll cover how to write your function for Lambda, how to get it set up to run automatically, and as a sweet little bonus, a handy bash script that updates your function from the command line whenever you need to make a change. Let&rsquo;s do it!
Is Lambda right for you When I wrote the code for my Twitter bot in Go, I intended to have it run on an AWS instance and borrowed heavily from Francesc&rsquo;s awesome Just for Func episode. Some time later I modified it to randomly choose an article from my RSS feeds and tweet the link, twice a day. I wanted to do something similar for the @freeCodeCampTO bot, and have it tweet an inspiring quote about programming every morning.
This is a good use case for Lambda because:
The program should execute once It runs on a regular schedule, using time as a trigger It doesn&rsquo;t need to run constantly The important thing to keep in mind is that Lambda runs a function once in response to an event that you define. The most widely applicable trigger is a simple cron expression, but there are many other trigger events you can hook up. You can get an overview here.
Write a Lambda function I found this really straightforward to do in Go. First, grab the aws-lambda-go library:
go get github.com/aws/aws-lambda-go/lambda Then make this your func main():
func main() { lambda.Start(tweetFeed) } Where tweetFeed is the name of the function that makes everything happen. While I won&rsquo;t go into writing the whole Twitter bot here, you can view my code on GitHub.
Setting up AWS Lambda I&rsquo;m assuming you already have an AWS account. If not, first things first here: https://aws.amazon.com/free
1. Create your function Find AWS Lambda in the list of services, then look for this shiny button:
We&rsquo;re going to author a function from scratch. Name your function, then under Runtime choose &ldquo;Go 1.x&rdquo;.
Under Role name write any name you like. It&rsquo;s a required field but irrelevant for this use case.
Click Create function.
2. Configure your function You&rsquo;ll see a screen for configuring your new function. Under Handler enter the name of your Go program.
If you scroll down, you&rsquo;ll see a spot to enter environment variables. This is a great place to enter the Twitter API tokens and secrets, using the variable names that your program expects. The AWS Lambda function will create the environment for you using the variables you provide here.
No further settings are necessary for this use case. Click Save at the top of the page.
3. Upload your code You can upload your function code as a zip file on the configuration screen. Since we&rsquo;re using Go, you&rsquo;ll want to go build first, then zip the resulting executable before uploading that to Lambda.
&hellip;Of course I&rsquo;m not going to do that manually every time I want to tweak my function. That&rsquo;s what awscli and this bash script is for!
update.sh
go build &amp;&amp; \\ zip fcc-tweet.zip fcc-tweet &amp;&amp; \\ rm fcc-tweet &amp;&amp; \\ aws lambda update-function-code --function-name fcc-tweet --zip-file fileb://fcc-tweet.zip &amp;&amp; \\ rm fcc-tweet.zip Now whenever I make a tweak, I just run bash update.sh.
If you&rsquo;re not already using AWS Command Line Interface, do pip install awscli and thank me later. Find instructions for getting set up and configured in a few minutes here under Quick Configuration.
4. Test your function Wanna see it go? Of course you do! Click &ldquo;Configure test events&rdquo; in the dropdown at the top.
Since you&rsquo;ll use a time-based trigger for this function, you don&rsquo;t need to enter any code to define test events in the popup window. Simply write any name under Event name and empty the JSON in the field below. Then click Create.
Click Test at the top of the page, and if everything is working correctly you should see&hellip;
5. Set up CloudWatch Events To run our function as we would a cron job - as a regularly scheduled time-based event - we&rsquo;ll use CloudWatch. Click CloudWatch Events in the Designer sidebar.
Under Configure triggers, you&rsquo;ll create a new rule. Choose a descriptive name for your rule without spaces or punctuation, and ensure Schedule expression is selected. Then input the time you want your program to run as a rate expression, or cron expression.
A cron expression looks like this: cron(0 12 * * ? *)
Minutes Hours Month Day of week Year In English 0 12 * ? * Run at noon (UTC) every day For more on how to write your cron expressions, read this.
If you want your program to run twice a day, say once at 10am and again at 3pm, you&rsquo;ll need to set two separate CloudWatch Events triggers and cron expression rules.
Click Add.
Watch it go That&rsquo;s all you need to get your Lambda function up and running! Now you can sit back, relax, and do more important things than share your RSS links on Twitter.
`,url:"https://victoria.dev/archive/running-a-free-twitter-bot-on-aws-lambda/"},"https://victoria.dev/archive/moving-to-a-new-domain-without-breaking-old-links-with-aws-disqus/":{title:"Moving to a new domain without breaking old links with AWS & Disqus",tags:["aws","websites"],content:`I started blogging about my nomadic travels last year, and so far the habit has stuck. Like all side projects, I won&rsquo;t typically invest heavily in setting up web properties before I can be reasonably certain that such an investment is worth my time or enjoyment. In other words: don&rsquo;t buy the domain until you&rsquo;ve proven to yourself that you&rsquo;ll stick with it!
After some months of regular posting I felt I was ready to commit (short courtship, I know, but we&rsquo;re all adults here) and I bought a dedicated domain, herOneBag.com.
Up until recently, my #NomadLyfe blog was just a subdirectory of my main personal site. Now it&rsquo;s all grown up and ready to strike out into the world alone! Here&rsquo;s the setup for the site:
Static site in Amazon Web Services S3 bucket Route 53 handling the DNS CloudFront for distribution and a custom SSL certificate Disqus for comments If you&rsquo;d like a walk-through for how to set up a new domain with this structure, it&rsquo;s over here: Hosting your static site with AWS S3, Route 53, and CloudFront. In this post, I&rsquo;ll just detail how I managed to move my blog to the new site without breaking the old links or losing any comments.
Preserve old links with redirection rules I wanted to avoid breaking links that have been posted around the web by forwarding visitors to the new URL. The change looks like this:
Old URL: https://victoria.dev/meta/5-bag-lessons/
New URL: https://heronebag.com/blog/5-bag-lessons/
You can see that the domain name as well as the subdirectory have changed, but the slug for the blog post remains the same. (I love static sites.)
To redirect links from the old site, we&rsquo;ll need to set redirection rules in the old site&rsquo;s S3 bucket. AWS provides a way to set up a conditional redirect. This is set in the &ldquo;Redirection rules&rdquo; section of your S3 bucket&rsquo;s properties, under &ldquo;Static website hosting.&rdquo; You can find the documentation here.
There are a few examples given, but none that represent the redirect I want. In addition to changing the prefix of the object key, we&rsquo;re also changing the domain. The latter is achieved with the &lt;HostName&gt; tag.
To redirect requests for the old blog URL to the new top level domain, we&rsquo;ll use the code below.
&lt;RoutingRules&gt; &lt;RoutingRule&gt; &lt;Condition&gt; &lt;KeyPrefixEquals&gt;oldblog/&lt;/KeyPrefixEquals&gt; &lt;/Condition&gt; &lt;Redirect&gt; &lt;HostName&gt;newdomain.com&lt;/HostName&gt; &lt;ReplaceKeyPrefixWith&gt;newblog/&lt;/ReplaceKeyPrefixWith&gt; &lt;/Redirect&gt; &lt;/RoutingRule&gt; &lt;/RoutingRules&gt; This rule ensures that requests for olddomain.com/oldblog/specific-blog-post will redirect to newdomain.com/newblog/specific-blog-post.
Migrate Disqus comments Disqus provides a tool for migrating the comment threads from your old blog site to the new one. You can find it in your Disqus admin tools at your-short-name.disqus.com/admin/discussions/migrate/.
To migrate posts from the old blog address to the new one, we&rsquo;ll use the URL mapper tool. Click &ldquo;Start URL mapper,&rdquo; then &ldquo;you can download a CSV here.&rdquo;
Disqus has decent instructions for how this tool works, and you can read them here. Basically, you&rsquo;ll input the new blog URLs into the second column of the CSV file you downloaded, then pass it back to Disqus to process. If you&rsquo;re using a program to edit the CSV, be sure to save the resulting file in CSV format.
Unless you have a bazillion URLs, the tool works pretty quickly, and you&rsquo;ll get an email when it&rsquo;s finished. Don&rsquo;t forget to update the name of your site in the Disqus admin, too.
Transfer other settings Update links in your social profiles and any other sites you may have around the web. If you&rsquo;re using other services attached to your website like Google Analytics or IFTTT, don&rsquo;t forget to update those details too!
`,url:"https://victoria.dev/archive/moving-to-a-new-domain-without-breaking-old-links-with-aws-disqus/"},"https://victoria.dev/archive/a-unicode-substitution-cipher-algorithm/":{title:"A Unicode substitution cipher algorithm",tags:["algorithms","javascript"],content:`Full transparency: I occasionally waste time messing around on Twitter. (Gasp! Shock!) One of the ways I waste time messing around on Twitter is by writing my name in my profile with different Unicode character &ldquo;fonts,&rdquo; 𝖑𝖎𝖐𝖊 𝖙𝖍𝖎𝖘 𝖔𝖓𝖊. I previously did this by searching for different Unicode characters on Google, then one-by-one copying and pasting them into the &ldquo;Name&rdquo; field on my Twitter profile. Since this method of wasting time was a bit of a time waster, I decided (in true programmer fashion) to write a tool that would help me save some time while wasting it.
I originally dubbed the tool &ldquo;uni-pretty,&rdquo; (based on LEGO&rsquo;s Unikitty from a movie &ndash; a pun that absolutely no one got) but have since renamed it fancy unicode. It builds from this GitHub repo. It lets you type any characters into a field and then converts them into Unicode characters that also represent letters, giving you fancy &ldquo;fonts&rdquo; that override a website&rsquo;s CSS, like in your Twitter profile. (Sorry, Internet.)
The tool&rsquo;s first naive iteration existed for about twenty minutes while I copy-pasted Unicode characters into a data structure. This approach of storing the characters in the JavaScript file, called hard-coding, is fraught with issues. Besides having to store every character from every font style, it&rsquo;s painstaking to build, hard to update, and more code means it&rsquo;s susceptible to more possible errors.
Fortunately, working with Unicode means that there&rsquo;s a way to avoid the whole mess of having to store all the font characters: Unicode numbers are sequential. More importantly, the special characters in Unicode that could be used as fonts (meaning that there&rsquo;s a matching character for most or all of the letters of the alphabet) are always in the following sequence: capital A-Z, lowercase a-z.
For example, in the fancy Unicode above, the lowercase letter &ldquo;L&rdquo; character has the Unicode number U+1D591 and HTML code &amp;#120209;. The next letter in the sequence, a lowercase letter &ldquo;M,&rdquo; has the Unicode number U+1D592 and HTML code &amp;#120210;. Notice how the numbers in those codes increment by one.
Why&rsquo;s this relevant? Since each special character can be referenced by a number, and we know that the order of the sequence is always the same (capital A-Z, lowercase a-z), we&rsquo;re able to produce any character simply by knowing the first number of its font sequence (the capital &ldquo;A&rdquo;). If this reminds you of anything, you can borrow my decoder pin.
In cryptography, the Caesar cipher (or shift cipher) is a simple method of encryption that utilizes substitution of one character for another in order to encode a message. This is typically done using the alphabet and a shift &ldquo;key&rdquo; that tells you which letter to substitute for the original one. For example, if I were trying to encode the word &ldquo;cat&rdquo; with a right shift of 3, it would look like this:
c a t f d w With this concept, encoding our plain text letters as a Unicode &ldquo;font&rdquo; is a simple process. All we need is an array to reference our plain text letters with, and the first index of our Unicode capital &ldquo;A&rdquo; representation. Since some Unicode numbers also include letters (which are sequential, but an unnecessary complication) and since the intent is to display the page in HTML, we&rsquo;ll use the HTML code number &amp;#120172;, with the extra bits removed for brevity.
var plain = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;, &#39;H&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;M&#39;, &#39;N&#39;, &#39;O&#39;, &#39;P&#39;, &#39;Q&#39;, &#39;R&#39;, &#39;S&#39;, &#39;T&#39;, &#39;U&#39;, &#39;V&#39;, &#39;W&#39;, &#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;l&#39;, &#39;m&#39;, &#39;n&#39;, &#39;o&#39;, &#39;p&#39;, &#39;q&#39;, &#39;r&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;v&#39;, &#39;w&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;]; var fancyA = 120172; Since we know that the letter sequence of the fancy Unicode is the same as our plain text array, any letter can be found by using its index in the plain text array as an offset from the fancy capital &ldquo;A&rdquo; number. For example, capital &ldquo;B&rdquo; in fancy Unicode is the capital &ldquo;A&rdquo; number, 120172 plus B&rsquo;s index, which is 1: 120173.
Here&rsquo;s our conversion function:
function convert(string) { // Create a variable to store our converted letters let converted = []; // Break string into substrings (letters) let arr = string.split(&#39;&#39;); // Search plain array for indexes of letters arr.forEach(element =&gt; { let i = plain.indexOf(element); // If the letter isn&#39;t a letter (not found in the plain array) if (i == -1) { // Return as a whitespace converted.push(&#39; &#39;); } else { // Get relevant character from fancy number + index let unicode = fancyA + i; // Return as HTML code converted.push(&#39;&amp;#&#39; + unicode + &#39;;&#39;); } }); // Print the converted letters as a string console.log(converted.join(&#39;&#39;)); } A neat possibility for this method of encoding requires a departure from my original purpose, which was to create a human-readable representation of the original string. If the purpose was instead to produce a cipher, this could be done by using any Unicode index in place of fancyA as long as the character indexed isn&rsquo;t a representation of a capital &ldquo;A.&rdquo;
Here&rsquo;s the same code set up with a simplified plain text array, and a non-letter-representation Unicode key:
var plain = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;l&#39;, &#39;m&#39;, &#39;n&#39;, &#39;o&#39;, &#39;p&#39;, &#39;q&#39;, &#39;r&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;v&#39;, &#39;w&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;]; var key = 9016; You might be able to imagine that decoding a cipher produced by this method would be relatively straightforward, once you knew the encoding secret. You&rsquo;d simply need to subtract the key from the HTML code numbers of the encoded characters, then find the relevant plain text letters at the remaining indexes.
Well, that&rsquo;s it for today. Be sure to drink your Ovaltine and we&rsquo;ll see you right here next Monday at 5:45!
Oh, and&hellip; ⍔⍠⍟⍘⍣⍒⍥⍦⍝⍒⍥⍚⍠⍟⍤ ⍒⍟⍕ ⍨⍖⍝⍔⍠⍞⍖ ⍥⍠ ⍥⍙⍖ ⍔⍣⍪⍡⍥⍚⍔ ⍦⍟⍚⍔⍠⍕⍖ ⍤⍖⍔⍣⍖⍥ ⍤⍠⍔⍚⍖⍥⍪
:)
`,url:"https://victoria.dev/archive/a-unicode-substitution-cipher-algorithm/"},"https://victoria.dev/archive/hosting-your-static-site-with-aws-s3-route-53-and-cloudfront/":{title:"Hosting your static site with AWS S3, Route 53, and CloudFront",tags:["aws","websites"],content:`Some time ago I decided to stop freeloading on GitHub pages and move one of my sites to Amazon Web Services (AWS). It turns out that I&rsquo;m still mostly freeloading (yay free tier) so it amounted to a learning experience. Here are the components that let me host and serve the site at my custom domain with HTTPS.
Static site in Amazon Web Services S3 bucket Route 53 handling the DNS CloudFront for distribution and a custom SSL certificate I set all that up most of a year ago. At the time, I found the AWS documentation to be rather fragmented and inconvenient to follow - it was hard to find what you were looking for without knowing what a specific setting might be called, or where it was, or if it existed at all. When I recently set up a new site and stumbled through this process again, I didn&rsquo;t find it any easier. Hopefully this post can help to collect the relevant information into a more easily followed process and serve as an accompanying guide to save future me (and you) some time.
Rather than replace existing documentation, this post is meant to supplement it. Think of me as your cool tech-savvy friend on the phone with you at 4am, troubleshooting your website. (Please don&rsquo;t actually call me at 4am.) I&rsquo;ll walk through the set up while providing links for the documentation that was ultimately helpful (mostly so I can find it again later&hellip;).
Hosting a static site with Amazon S3 and a custom domain If you&rsquo;re starting from scratch, you&rsquo;ll need an AWS account. It behooves you to get one, even if you don&rsquo;t like paying for services - there&rsquo;s a free tier that will cover most of the experimental stuff you&rsquo;re going to want to do in the first year, and even the things I do pay for cost me less than a dollar a month. You can sign up at https://aws.amazon.com/free.
Getting your static site hosted and available at your custom domain is your first mission, should you choose to accept it. Your instructions are here.
Creating the buckets for site hosting on S3 is the most straightforward part of this process in my opinion, and the AWS documentation walkthrough covers what you&rsquo;ll need to do quite well. It gets a little unclear around Step 3: Create and Configure Amazon Route 53 Hosted Zone, so come back and read on once you&rsquo;ve reached that point. I&rsquo;ll make some tea in the meantime.
&hellip; 🎶 🎵
Ready? Cool. See, I&rsquo;m here for you.
Set up Route 53 The majority of the work in this section amounts to creating the correct record sets for your custom domain. If you&rsquo;re already familiar with how record sets work, the documentation is a bit of a slog. Here&rsquo;s how it should look when you&rsquo;re finished:
The &ldquo;NS&rdquo; and &ldquo;SOA&rdquo; records are created automatically for you. The only records you need to create are the &ldquo;A&rdquo; records.
Hop over to Route 53 and follow this walkthrough to create a &ldquo;hosted zone.&rdquo; The value of the NS (Name Servers) records are what you&rsquo;ll have to provide to your domain name registrar. Your registrar is wherever you bought your custom domain, such as this super subtle Namecheap.com affiliate link right here. (Thanks for your support! 😊)
If you created two buckets in the first section (one for yourdomain.com and one for www.yourdomain.com), you&rsquo;ll need two separate A records in Route 53. Initially, these have the value of the endpoints for your matching S3 buckets (looks like s3-website.us-east-2.amazonaws.com). Later, you&rsquo;ll change them to your CloudFront domain name.
If you went with Namecheap as your registrar, Step 4 looks like this:
Waiting is the hardest part&hellip; I&rsquo;ve gotten into the habit of working on another project or setting up the DNS change before going to bed so that changes have time to propagate without me feeling like I need to fiddle with it. ^^;
When the transfer&rsquo;s ready, you&rsquo;ll see your site at http://yourdomain.com. Next, you&rsquo;ll want to set up CloudFront so that becomes https://yourdomain.com.
Set up CloudFront and SSL Here are the instructions for setting up CloudFront. There are a few important points to make sure you don&rsquo;t miss on the &ldquo;Create Distribution&rdquo; page:
Origin Domain Name: Make sure to use your S3 bucket endpoint, and not select the bucket from the dropdown menu that appears. Viewer Protocol Policy: If you want requests for http://yourdomain.com to always result in https://yourdomain.com, choose &ldquo;Redirect HTTP to HTTPS.&rdquo; Alternate Domain Names: Enter yourdomain.com and www.yourdomain.com on separate lines. SSL Certificate: See below. Default Root Object: Enter the name of the html file that should be returned when your users go to https://yourdomain.com. This is usually &ldquo;index.html&rdquo;. SSL Certificate To show your content with HTTPS at your custom domain, you&rsquo;ll need to choose &ldquo;Custom SSL Certificate.&rdquo; You can easily get an SSL Certificate with AWS Certificate Manager. Click on &ldquo;Request or Import a Certificate with ACM&rdquo; to get started in a new window.
Here are instructions for setting up a certificate. I don&rsquo;t think they&rsquo;re very good, personally. Don&rsquo;t worry, I got you.
To account for &ldquo;www.yourdomain.com&rdquo; as well as any subdomains, you&rsquo;ll want to add two domain names to the certificate, like so:
Click &ldquo;Next.&rdquo; You&rsquo;ll be asked to choose a validation method. Choose &ldquo;DNS validation&rdquo; and click &ldquo;Review.&rdquo; If everything is as it should be, click &ldquo;Confirm and request.&rdquo;
You&rsquo;ll see a page, &ldquo;Validation&rdquo; that looks like this. You&rsquo;ll have to click the little arrow next to both domain names to get the important information to show:
Under both domain names, click the button for &ldquo;Create record in Route 53.&rdquo; This will automatically create a CNAME record set in Route 53 with the given values, which ACM will then check in order to validate that you own those domains. You could create the records manually, if you wanted to for some reason. I don&rsquo;t know, maybe you&rsquo;re killing time. ¯\\_(ツ)_/¯
Click &ldquo;Continue.&rdquo; You&rsquo;ll see a console that looks like this:
It may take some time for the validation to complete, at which point the &ldquo;Pending validation&rdquo; status will change to &ldquo;Issued.&rdquo; Again with the waiting. You can close this window to return to the CloudFront set up. Once the certificate is validated, you&rsquo;ll see it in the dropdown menu under &ldquo;Custom SSL Certificate.&rdquo; You can click &ldquo;Create Distribution&rdquo; to finish setting up CloudFront.
In your CloudFront Distributions console, you&rsquo;ll see &ldquo;In Progress&rdquo; until AWS has done its thing. Once it&rsquo;s done, it&rsquo;ll change to &ldquo;Deployed.&rdquo;
One last thing Return to your Route 53 console and click on &ldquo;Hosted zones&rdquo; in the sidebar, then your domain name from the list. For both A records, change the &ldquo;Alias Target&rdquo; from the S3 endpoint to your CloudFront distribution domain, which should look something like dj4p1rv6mvubz.cloudfront.net. It appears in the dropdown after you clear the field.
You&rsquo;re done Well, usually. If you navigate to your new HTTPS domain and don&rsquo;t see your beautiful new site where it should be, here are some things you can do:
Check S3 bucket policy - ensure that the bucket for yourdomain.com in the S3 console shows &ldquo;Public&rdquo; in the &ldquo;Access&rdquo; column. Check S3 bucket index document - In the &ldquo;metadata&rdquo; tab for the bucket, then &ldquo;Static website hosting&rdquo;. Usually &ldquo;index.html&rdquo;. Check CloudFront Origin - the &ldquo;Origin&rdquo; column in the CloudFront Console should show the S3 bucket&rsquo;s endpoint (s3-website.us-east-2.amazonaws.com), not the bucket name (yourdomain.com.s3.amazonaws.com). Check CloudFront Default Root Object - clicking on the distribution name should take you to a details page that shows &ldquo;Default Root Object&rdquo; in the list with the value that you set, usually &ldquo;index.html&rdquo;. Wait. Sometimes changes take up to 48hrs to propagate. ¯\\_(ツ)_/¯ I hope that helps you get set up with your new static site on AWS! If you found this post helpful, there&rsquo;s a lot more where this came from. You can subscribe to victoria.dev to see new posts first!
`,url:"https://victoria.dev/archive/hosting-your-static-site-with-aws-s3-route-53-and-cloudfront/"},"https://victoria.dev/archive/about-time/":{title:"About time",tags:["life"],content:`This morning I read an article that&rsquo;s been making the rounds lately: Modern Media Is a DoS Attack on Your Free Will.
It&rsquo;s made me think, which I must admit, I at first didn&rsquo;t like. See, when I wake up in the morning (and subsequently wake up my computer) the first thing I do is go on Twitter to catch up on everything I missed while I was asleep. All this before my first coffee, mind you. Links on Twitter usually lead to stories on Medium, newly released apps on ProductHunt, and enticing sales on a new gadget or two on Amazon. Wherever it goes, in those blissfully half-awake mental recesses, the last thing I&rsquo;m trying to do is think.
However, yesterday, I also happened to listen to a podcast from freeCodeCamp. It was #7: The code I&rsquo;m still ashamed of. This lead to thoughts on the responsibilities of programmers - the people tasked with designing and building apps and systems meant to steer the very course of your life.
This morning, the combined swirling mess of notions brought on by these two sources of information had, even before my first coffee, the unfortunate effect of making me think.
Mostly, I thought about intention, and time.
I don&rsquo;t believe it&rsquo;s wildly inaccurate to say that when you go about doing something in your daily life, you have a general awareness of your reason for doing it. If you leave your building and go down the street to Starbucks and buy a coffee, more often than not, it&rsquo;s because you wanted a coffee. If you go to the corner store and buy a litre of milk, you probably intend to drink it. If you find yourself nicely dressed on a Friday night waiting at a well-decorated restaurant to meet another human being with whom you share an apparent mutual attraction, I can risk a guess that you&rsquo;re after some form of pleasant human interaction.
In each of these, and many more examples you can think up, the end goal is clearly defined. There is an expected final step to the process; an expected response; a return value.
What is the return value of opening up the Twitter app? Browsing Facebook? Instagram? In fact, any social media?
The concrete answer is that there isn&rsquo;t one. Perhaps in those of us with resilient self-discipline, there may at least be some sort of time limitation. That&rsquo;s the most we can hope for, however, and no wonder - that&rsquo;s what these and other similar services have been designed for. They&rsquo;re built to be open-ended black-holes for our most precious resource&hellip; time.
In the case of the Analytical Engine we have undoubtedly to lay out a certain capital of analytical labour in one particular line; but this is in order that the engine may bring us in a much larger return in another line.
Ada Augusta (Ada Lovelace) - Notes on Sketch of The Analytical Engine
Okay, so I did some more reading. Specifically, #ThrowbackThursday to the mid 1800&rsquo;s and something my good friend Ada Lovelace once scribbled in a book. Widely considered one of the first computer programmers, she and Charles Babbage pioneered many concepts that programmers today take for granted. The one I&rsquo;m going to hang my point on is, I think, nicely encapsulated in the above quote: the things programmers make are supposed to save you time.
Save it. Not lose it.
I think Ada and Charles would agree that, observing the effects of social media apps, clickbait news sites, and many other forms of attention-hogging interactivity that we haven&rsquo;t even classified yet - something&rsquo;s gone horribly wrong.
What if, as programmers, we actually did something about it?
Consider that collectively - no, even individually - we who design and build the workings of modern technology have an incredible amount of power. The next indie app that goes viral on ProductHunt will consume hundreds of hours of time from its users. Where is all that untapped, pure potential going to? Some open-ended, inoffensive amusement? Another advertising platform thinly veiled as a game? Perhaps another drop of oil to smooth the machinery of The Great Engine of Commerce?
I get it - programmers will build what they&rsquo;re paid to build. That&rsquo;s capitalism, that&rsquo;s feeding your family, survival - life. I&rsquo;m not trying to suggest we all quit our jobs, go live in the woods, and volunteer as humanitarians. That would be nice, but it&rsquo;s impractical.
But we all have side projects. Free time. What are you doing with yours?
Before I&rsquo;m accused of being too hand-wavy and idealistic, I want to offer a concrete suggestion. Build things that save time. Not in the &ldquo;I&rsquo;ve made yet another to-do list app for you to download,&rdquo; kind of way, but in the &ldquo;Here&rsquo;s a one-liner to automate this mundane thing that would have taken you hours,&rdquo; kind of way. Here, have a shameless plug.
I also really like this idea from the first article I mentioned, so hang on tight while I bring this full circle:
What’s one concrete thing companies could do now to stop subverting our attention?
I would just like to know what is the ultimate design goal of that site or that system that’s shaping my behavior or thinking. What are they really designing my experience for? Companies will say that their goal is to make the world open and connected or whatever. These are lofty marketing claims. But if you were to actually look at the dashboards that they’re designing, the high-level metrics they’re designing for, you probably wouldn’t see those things. You’d see other things, like frequency of use, time on site, this type of thing. If there was some way for the app to say, to the user, “Here’s generally what this app wants from you, from an attentional point of view,” that would be huge. It would probably be the primary way I would decide which apps I download and use.
There are so many ways I&rsquo;d love to see this put into practice, from the obvious to the subversive. A little position: sticky; banner? A custom meta tag in the header? Maybe a call to action like this takes more introspection and honesty than a lot of app makers are ready for&hellip; but maybe it just takes a little of our time.
`,url:"https://victoria.dev/archive/about-time/"},"https://victoria.dev/archive/batch-renaming-images-including-image-resolution-with-awk/":{title:"Batch renaming images, including image resolution, with awk",tags:["terminal","linux","ci/cd"],content:`The most recent item on my list of &ldquo;Geeky things I did that made me feel pretty awesome&rdquo; is an hour&rsquo;s adventure that culminated in this code:
$ file IMG* | awk &#39;BEGIN{a=0} {print substr($1, 1, length($1)-5),a++&#34;_&#34;substr($8,1, length($8)-1)}&#39; | while read fn fr; do echo $(rename -v &#34;s/$fn/img_$fr/g&#34; *); done IMG_20170808_172653_425.jpg renamed as img_0_4032x3024.jpg IMG_20170808_173020_267.jpg renamed as img_1_3024x3506.jpg IMG_20170808_173130_616.jpg renamed as img_2_3024x3779.jpg IMG_20170808_173221_425.jpg renamed as img_3_3024x3780.jpg IMG_20170808_173417_059.jpg renamed as img_4_2956x2980.jpg IMG_20170808_173450_971.jpg renamed as img_5_3024x3024.jpg IMG_20170808_173536_034.jpg renamed as img_6_4032x3024.jpg IMG_20170808_173602_732.jpg renamed as img_7_1617x1617.jpg IMG_20170808_173645_339.jpg renamed as img_8_3024x3780.jpg IMG_20170909_170146_585.jpg renamed as img_9_3036x3036.jpg IMG_20170911_211522_543.jpg renamed as img_10_3036x3036.jpg IMG_20170913_071608_288.jpg renamed as img_11_2760x2760.jpg IMG_20170913_073205_522.jpg renamed as img_12_2738x2738.jpg // ... etc etc The last item on the aforementioned list is &ldquo;TODO: come up with a shorter title for this list.&rdquo;
I previously wrote about the power of command line tools like sed. This post expands on how to string all this magical functionality into one big, long, rainbow-coloured, viscous stream of awesome.
Rename files The tool that actually handles the renaming of our files is, appropriately enough, rename. The syntax is: rename -n &quot;s/original_filename/new_filename/g&quot; * where -n does a dry-run, and substituting -v would rename the files. The s indicates our substitution string, and g for &ldquo;global&rdquo; finds all occurrences of the string. The * matches zero or more occurrences of our search-and-replace parameters.
We&rsquo;ll come back to this later.
Get file information When I run $ file IMG_20170808_172653_425.jpg in the image directory, I get this output:
IMG_20170808_172653_425.jpg: JPEG image data, baseline, precision 8, 4032x3024, frames 3 Since we can get the image resolution (&ldquo;4032x3024&rdquo; above), we know that we&rsquo;ll be able to use it in our new filename.
Isolate the information we want I love awk for its simplicity. It takes lines of text and makes individual bits of information available to us with built in variables that we can then refer to as column numbers denoted by $1, $2, etc. By default, awk splits up columns on whitespace. To take the example above:
| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | ------------------------------------------------------------------------------------------------------------- | IMG_20170808_172653_425.jpg: | JPEG | image | data, | baseline, | precision | 8, | 4032x3024, | frames | 3 | We can denote different values to use as a splitter with, for example, -F',' if we wanted to use commas as the column divisions. For our current project, spaces are fine.
There are a couple issues we need to solve before we can plug the information into our new filenames. Column $1 has the original filename we want, but there&rsquo;s an extra &ldquo;:&rdquo; character on the end. We don&rsquo;t need the &ldquo;.jpg&rdquo; either. Column $8 has an extra &ldquo;,&rdquo; that we don&rsquo;t want as well. To get just to information we need, we&rsquo;ll take a substring of the column with substr():
substr($1, 1, length($1)-5) - This gives us the file name from the beginning of the string to the end of the string, minus 5 characters (&ldquo;length minus 5&rdquo;). substr($8,1, length($8)-1) - This gives us the image size, without the extra comma (&ldquo;length minus 1&rdquo;).
Avoid duplicate file names To ensure that two images with the same resolutions don&rsquo;t create identical, competing file names, we&rsquo;ll append a unique incrementing number to the filename.
BEGIN{a=0} - Using BEGIN tells awk to run the following code only once, at the (drumroll) beginning. Here, we&rsquo;re declaring the variable a to be 0. a++ - Later in our code, at the appropriate spot for our file name, we call a and increment it.
When awk prints a string, it concatenates everything that isn&rsquo;t separated by a comma. {print a b c} would create &ldquo;abc&rdquo; and {print a,b,c} would create &ldquo;a b c&rdquo;, for example.
We can add additional characters to our file name, such as an underscore, by inserting it in quotations: &quot;_&quot;.
String it all together To feed the output of one command into another command, we use &ldquo;pipe,&rdquo; written as |.
If we only used pipe in this instance, all our data from file and awk would get fed into rename all at once, making for one very, very long and probably non-compiling file name. To run the rename command line by line, we can use while and read. Similarly to awk, read takes input and splits it into variables we can assign and use. In our code, it takes the first bit of output from awk (the original file name) and assigns that the variable name $fn. It takes the second output (our incrementing number and the image resolution) and assigns that to $fr. The variable names are arbitrary; you can call them whatever you want.
To run our rename commands as if we&rsquo;d manually entered them in the terminal one by one, we can use echo $(some command). Finally, done ends our while loop.
Bonus round: rainbow output I wasn&rsquo;t kidding with that &ldquo;rainbow-coloured&rdquo; bit&hellip;
p install lolcat Here&rsquo;s our full code:
le IMG* | awk &#39;BEGIN{a=0} {print substr($1, 1, length($1)-5),a++&#34;_&#34;substr($8,1, length($8)-1)}&#39; | while read fn fs; do echo $(rename -v &#34;s/$fn/img_$fs/g&#34; *); done | lolcat Enjoy!
`,url:"https://victoria.dev/archive/batch-renaming-images-including-image-resolution-with-awk/"},"https://victoria.dev/archive/how-to-code-a-satellite-algorithm-and-cook-paella-from-scratch/":{title:"How to code a satellite algorithm and cook paella from scratch",tags:["algorithms","coding","javascript"],content:`What if I told you that by the end of this article, you&rsquo;ll be able to calculate the orbital period of satellites around Earth using their average altitudes and&hellip; You tuned out already, didn&rsquo;t you?
Okay, how about this: I&rsquo;m going to teach you how to make paella!
And you&rsquo;ll have written a function that does the stuff I mentioned above, just like I did for a freeCodeCamp challenge.
I promise there&rsquo;s an overarching moral lesson that will benefit you every day for the rest of your life. Or at least, feed you for one night. Let&rsquo;s get started.
The only thing I know about paella is that it&rsquo;s an emoticon Unless you&rsquo;re reading this on a Samsung phone, in which case you&rsquo;re looking at a Korean hotpot.
One of my favorite things about living in the world today is that it&rsquo;s totally fine to know next-to-nothing about something. A hundred years ago you might have gone your whole life not knowing anything more about paella other than that it&rsquo;s an emoticon.* But today? You can simply look it up.
*That was a joke.
As with all things in life, when we are unsure, we turn to the internet - in this case, the entry for paella on Wikipedia, which reads:
Paella &hellip;is a Valencian rice dish. Paella has ancient roots, but its modern form originated in the mid-19th century near the Albufera lagoon on the east coast of Spain adjacent to the city of Valencia. Many non-Spaniards view paella as Spain&rsquo;s national dish, but most Spaniards consider it to be a regional Valencian dish. Valencians, in turn, regard paella as one of their identifying symbols.
At this point, you&rsquo;re probably full of questions. Do I need to talk to a Valencian? Should I take an online course on the history of Spain? What type of paella should I try to make? What is the common opinion of modern chefs when it comes to paella types?
If you set out with the intention of answering all these questions, one thing is certain: you&rsquo;ll never end up actually making paella. You&rsquo;ll spend hours upon hours typing questions into search engines and years later wake up with a Masters in Valencian Cuisine.
The &ldquo;Most Important Question&rdquo; method When I talk to myself out loud in public (doesn&rsquo;t everyone?) I refer to this as &ldquo;MIQ&rdquo; (rhymes with &ldquo;Nick&rdquo;). I also imagine MIQ to be a rather crunchy and quite adorable anthropomorphized tortilla chip. Couldn&rsquo;t tell you why.
MIQ swings his crunchy triangular body around to point me in the right direction, and the right direction always takes the form of the most important question that you need to ask yourself at any stage of problem solving. The first most important question is always this:
What is the scope of the objective I want to achieve?
Well, you want to make paella.
The next MIQ then becomes: how much do I actually need to know about paella in order to start making it?
You&rsquo;ve heard this advice before: any big problem can be broken down into multiple, but more manageable, bite-size problems. In this little constellation of bite-size problems, there&rsquo;s only one that you need to solve in order to get most of the way to a complete solution.
In the case of making paella, we need a recipe. That&rsquo;s a bite-size problem that a search engine can solve for us:
Simple Paella Recipe
In a medium bowl, mix together 2 tablespoons olive oil, paprika, oregano, and salt and pepper. Stir in chicken pieces to coat. Cover, and refrigerate. Heat 2 tablespoons olive oil in a large skillet or paella pan over medium heat. Stir in garlic, red pepper flakes, and rice. Cook, stirring, to coat rice with oil, about 3 minutes. Stir in saffron threads, bay leaf, parsley, chicken stock, and lemon zest. Bring to a boil, cover, and reduce heat to medium low. Simmer 20 minutes. Meanwhile, heat 2 tablespoons olive oil in a separate skillet over medium heat. Stir in marinated chicken and onion; cook 5 minutes. Stir in bell pepper and sausage; cook 5 minutes. Stir in shrimp; cook, turning the shrimp, until both sides are pink. Spread rice mixture onto a serving tray. Top with meat and seafood mixture. (allrecipes.com) And voila! Believe it or not, we&rsquo;re most of the way there already.
Having a set of step-by-step instructions that are easy to understand is really most of the work done. All that&rsquo;s left is to go through the motions of gathering the ingredients and then making paella. From this point on, your MIQs may become fewer and far between, and they may slowly decrease in importance in relation to the overall problem. (Where do I buy paprika? How do I know when sausage is cooked? How do I set the timer on my phone for 20 minutes? How do I stop thinking about this delicious smell? Which Instagram filter best captures the ecstasy of this paella right now?)
The answer to that last one is Nashville
I still know nothing about calculating the orbital periods of satellites Okay. Let&rsquo;s examine the problem:
Return a new array that transforms the element&rsquo;s average altitude into their orbital periods.
The array will contain objects in the format {name: &rsquo;name&rsquo;, avgAlt: avgAlt}.
You can read about orbital periods on wikipedia.
The values should be rounded to the nearest whole number. The body being orbited is Earth.
The radius of the earth is 6367.4447 kilometers, and the GM value of earth is 398600.4418 km3s-2.
orbitalPeriod([{name : &quot;sputnik&quot;, avgAlt : 35873.5553}]) should return [{name: &quot;sputnik&quot;, orbitalPeriod: 86400}].
Well, as it turns out, in order to calculate the orbital period of satellites, we also need a recipe. Amazing, the things you can find on the internet these days.
Courtesy of dummies.com (yup! #noshame), here&rsquo;s our recipe:
It&rsquo;s kind of cute, in a way.
That might look pretty complicated, but as we&rsquo;ve already seen, we just need to answer the next MIQ: how much do I actually need to know about this formula in order to start using it?
In the case of this challenge, not too much. We&rsquo;re already given earthRadius, and avgAlt is part of our arguments object. Together, they form the radius, r. With a couple search queries and some mental time-travel to your elementary math class, we can describe this formula in a smattering of English:
T, the orbital period, equals 2 multiplied by Pi, in turn multiplied by the square root of the radius, r cubed, divided by the gravitational mass, GM.
JavaScript has a Math.PI property, as well as Math.sqrt() function and Math.pow() function. Using those combined with simple calculation, we can represent this equation in a single line assigned to a variable:
var orbitalPeriod = 2 * Math.PI * (Math.sqrt(Math.pow((earthRadius + avgAlt), 3) / GM)); From the inside out:
Add earthRadius and avgAlt Cube the result of step 1 Divide the result of step 2 by GM Take the square root of the result of step 3 Multiply 2 times Pi times the result of step 4 Assign the returned value to orbitalPeriod Believe it or not, we&rsquo;re already most of the way there.
The next MIQ for this challenge is to take the arguments object, extract the information we need, and return the result of our equation in the required format. There are a multitude of ways to do this, but I&rsquo;m happy with a straightforward for loop:
function orbitalPeriod(arr) { var resultArr = []; for (var teapot = 0; teapot &lt; arguments[0].length; teapot++) { var GM = 398600.4418; var earthRadius = 6367.4447; var avgAlt = arguments[0][teapot][&#39;avgAlt&#39;]; var name = arguments[0][teapot][&#39;name&#39;]; var orbitalPeriod = 2 * Math.PI * (Math.sqrt(Math.pow((earthRadius + avgAlt), 3) / GM)); var result = { name: name, orbitalPeriod: Math.round(orbitalPeriod) } resultArr.push(result); } return resultArr; } If you need a refresher on iterating through arrays, have a look at my article on iterating, featuring breakfast arrays! (5 minutes read)
Don&rsquo;t look now, but you just gained the ability to calculate the orbital period of satellites. You could even do it while making paella, if you wanted to. Seriously. Put it on your resume.
Tl;dr: the overarching moral lesson Whether it&rsquo;s cooking, coding, or anything else, problems may at first seem confusing, insurmountable, or downright boring. If you&rsquo;re faced with such a challenge, just remember: they&rsquo;re a lot more digestible with a side of bite-sized MIQ chips.
`,url:"https://victoria.dev/archive/how-to-code-a-satellite-algorithm-and-cook-paella-from-scratch/"},"https://victoria.dev/archive/making-sandwiches-with-closures-in-javascript/":{title:"Making sandwiches with closures in JavaScript",tags:["javascript","coding"],content:`Say you&rsquo;re having a little coding get-together, and you need some sandwiches. You happen to know that everyone prefers a different type of sandwich, like chicken, ham, or peanut butter and mayo. You could make all these sandwiches yourself, but that would be tedious and boring.
Luckily, you know of a nearby sandwich shop that delivers. They have the ability and ingredients to make any kind of sandwich in the world, and all you have to do is order through their app.
The sandwich shop looks like this:
function makeMeASandwich(x) { var ingredients = x.join(&#39; &#39;); return function barry() { return ingredients.concat(&#39; sandwich&#39;); } } Notice that we have an outer function, makeMeASandwich() that takes an argument, x. This outer function has the local variable ingredients, which is just x mushed together.
Barry? Who&rsquo;s Barry? He&rsquo;s the guy who works at the sandwich shop. You&rsquo;ll never talk with Barry directly, but he&rsquo;s the reason your sandwiches are made, and why they&rsquo;re so delicious. Barry takes ingredients and mushes them together with &quot; sandwich&quot;.
The reason Barry is able to access the ingredients is because they&rsquo;re in his outer scope. If you were to take Barry out of the sandwich shop, he&rsquo;d no longer be able to access them. This is an example of lexical scoping: &ldquo;Nested functions have access to variables declared in their outer scope.&rdquo; (MDN)
Barry, happily at work in the sandwich shop, is an example of a closure.
Closures are functions that refer to independent (free) variables (variables that are used locally, but defined in an enclosing scope). In other words, these functions &lsquo;remember&rsquo; the environment in which they were created. (MDN)
When you order, the app submits your sandwich request like so:
var pbm = makeMeASandwich([&#39;peanut butter&#39;, &#39;mayo&#39;]); pbm(); And in thirty-minutes-or-it&rsquo;s-free, you get: peanut butter mayo sandwich.
The nice thing about the sandwich shop app is that it remembers the sandwiches you&rsquo;ve ordered before. Your peanut butter and mayo sandwich is now available to you as pbm() for you to order anytime. It&rsquo;s pretty convenient since, each time you order, there&rsquo;s no need to specify that the sandwich you want is the same one you got before with peanut butter and mayo and it&rsquo;s a sandwich. Using pbm() is much more concise.
Let&rsquo;s order the sandwiches you need for the party:
var pmrp = makeMeASandwich([&#39;prosciutto&#39;, &#39;mozzarella&#39;, &#39;red pepper&#39;]); var pbt = makeMeASandwich([&#39;peanut butter&#39;, &#39;tuna&#39;]); var hm = makeMeASandwich([&#39;ham&#39;]); var pbm = makeMeASandwich([&#39;peanut butter&#39;, &#39;mayo&#39;]); pmrp(); pbt(); hm(); pbm(); Your order confirmation reads:
prosciutto mozzarella red pepper sandwich peanut butter tuna sandwich ham sandwich peanut butter mayo sandwich Plot twist! The guy who wanted a ham sandwich now wants a ham and cheese sandwich. Luckily, the sandwich shop just released a new version of their app that will let you add cheese to any sandwich.
With this added feature, the sandwich shop now looks like this:
function makeMeASandwich(x) { var ingredients = x.join(&#39; &#39;); var slices = 0; function barry() { return ingredients.concat(&#39; sandwich&#39;); } function barryAddCheese() { slices += 2; return ingredients.concat(&#39; sandwich with &#39;, slices, &#39; slices of cheese&#39;); } return { noCheese: function() { return barry(); }, addCheese: function() { return barryAddCheese(); } } } You amend the order to look like this:
pmrp.noCheese(); pbt.noCheese(); hm.addCheese(); pbm.noCheese(); And your order confirmation reads:
prosciutto mozzarella red pepper sandwich peanut butter tuna sandwich ham sandwich with 2 slices of cheese peanut butter mayo sandwich You&rsquo;ll notice that when you order a sandwich with cheese, Barry puts 2 slices of cheese on it. In this way, the sandwich shop controls how much cheese you get. You can&rsquo;t get to Barry to tell him you want more than 2 slices at a time. That&rsquo;s because your only access to the sandwich shop is through the public functions noCheese or addCheese.
Of course, there&rsquo;s a way to cheat the system&hellip;
hm.addCheese(); hm.addCheese(); hm.addCheese(); By ordering the same ham sandwich with cheese three times, you get: ham sandwich with 6 slices of cheese.
This happens because the sandwich shop app recognizes the variable hm as the same sandwich each time, and increases the number of cheese slices it tells Barry to add.
The app could prevent you from adding lots of cheese to the same sandwich, either by adding a maximum or by appending unique order numbers to the variable names&hellip; but this is our fantasy sandwich shop, and we get to pile on as much cheese as we want.
By using closures, we can have JavaScript emulate private methods found in languages like Ruby and Java. Closures are a useful way to extend the functionality of JavaScript, and also order sandwiches.
`,url:"https://victoria.dev/archive/making-sandwiches-with-closures-in-javascript/"},"https://victoria.dev/archive/understanding-array.prototype.reduce-and-recursion-using-apple-pie/":{title:"Understanding Array.prototype.reduce() and recursion using apple pie",tags:["javascript","coding"],content:`I was having trouble understanding reduce() and recursion in JavaScript, so I wrote this article to explain it to myself (hey, look, recursion!). I hope you find my examples both helpful and delicious.
Given an array with nested arrays:
var arr = [1, [2], [3, [[4]]]] We want to produce this:
var flat = [1, 2, 3, 4] Using for loops and if statements Naively, if we know the maximum number of nested arrays we&rsquo;ll encounter (there are 4 in this example), we can use for loops to iterate through each array item, then if statements to check if each item is in itself an array, and so on&hellip;
function flatten() { var flat = []; for (var i=0; i&lt;arr.length; i++) { if (Array.isArray(arr[i])) { for (var ii=0; ii&lt;arr[i].length; ii++) { if (Array.isArray(arr[i][ii])) { for (var iii=0; iii&lt;arr[i][ii].length; iii++) { for (var iiii=0; iiii&lt;arr[i][ii][iii].length; iiii++) { if (Array.isArray(arr[i][ii][iii])) { flat.push(arr[i][ii][iii][iiii]); } else { flat.push(arr[i][ii][iii]); } } } } else { flat.push(arr[i][ii]); } } } else { flat.push(arr[i]); } } } // [1, 2, 3, 4] &hellip;Which works, but of course looks ridiculous. Besides looking ridiculous, a) it only works if we know how many nested arrays we&rsquo;ll process, b) it&rsquo;s hard to read and harder to understand, and c) can you imagine having to debug this mess?! (Gee, I think there&rsquo;s an extra i somewhere.)
Using reduce JavaScript has a couple methods we can use to make our code a little less ridiculous. One of these is reduce() and it looks like this:
var flat = arr.reduce(function(done,curr){ return done.concat(curr); }, []); // [ 1, 2, 3, [ [ 4 ] ] ] It&rsquo;s a lot less code, but we haven&rsquo;t taken care of some of the nested arrays. Let&rsquo;s first walk through reduce() together and examine what it does to see how we&rsquo;ll correct this.
Array.prototype.reduce() The reduce() method applies a function against an accumulator and each element in the array (from left to right) to reduce it to a single value. (MDN)
It&rsquo;s not quite as complicated as it seems. Let&rsquo;s think of reduce() as an out-of-work developer (AI took all the dev jobs) with an empty basket. We&rsquo;ll call him Adam. Adam&rsquo;s main function (ba-dum ching) is now to take apples from a pile, shine them up, and put them one-by-one into the basket. This basket of shiny apples is destined to become delicious apple pies. It&rsquo;s a very important job.
Apples plus human effort equals pie. Not to be confused with apple-human-pie, which is less appetizing.
In our above example, the pile of apples is our array, arr. Our basket is done, the accumulator. The initial value of done is an empty array, which we see as [] at the end of our reduce function. The apple that our out-of-work dev is currently shining, you guessed it, is curr. Once Adam processes the current apple, he places it into the basket (.concat()). When there are no more apples in the pile, he returns the basket of polished apples to us, and then probably goes home to his cat, or something.
Using reduce recursively to address nested arrays So that&rsquo;s all well and good, and now we have a basket of polished apples. But we still have some nested arrays to deal with. Going back to our analogy, let&rsquo;s say that some of the apples in the pile are in boxes. Within each box there could be more apples, and/or more boxes containing smaller, cuter apples.
Adorable, slightly skewed apples just want to be loved/eaten.
Here&rsquo;s what we want our apple-processing-function/Adam to do:
If the pile of apples is a pile of apples, take an apple from the pile. If the apple is an apple, polish it, put it in the basket. If the apple is a box, open the box. If the box contains an apple, go to step 2. If the box contains another box, open this box, and go to step 3. When the pile is no more, give us the basket of shiny apples. If the pile of apples is not a pile of apples, give back whatever it is. A recursive reduce function that accomplishes this is:
function flatten(arr) { if (Array.isArray(arr)) { return arr.reduce(function(done,curr){ return done.concat(flatten(curr)); }, []); } else { return arr; } } // [ 1, 2, 3, 4 ] Bear with me and I&rsquo;ll explain.
An act of a function calling itself. Recursion is used to solve problems that contain smaller sub-problems. A recursive function can receive two inputs: a base case (ends recursion) or a recursive case (continues recursion). (MDN)
If you examine our code above, you&rsquo;ll see that flatten() appears twice. The first time it appears, it tells Adam what to do with the pile of apples. The second time, it tells him what to do with the thing he&rsquo;s currently holding, providing instructions in the case it&rsquo;s an apple, and in the case it&rsquo;s not an apple. The thing to note is that these instructions are a repeat of the original instructions we started with - and that&rsquo;s recursion.
We&rsquo;ll break it down line-by-line for clarity:
function flatten(arr) { - we name our overall function and specify that it will take an argument, arr. \`if (Array.isArray(arr)) {we examine the provided &ldquo;arrgument&rdquo; (I know, I&rsquo;m very funny) to determine if it is an array. \`return arr.reduce(function(done,curr){if the previous line is true and the argument is an array, we want to reduce it. This is our recursive case. We&rsquo;ll apply the following function to each array item&hellip; \`return done.concat(flatten(curr));nexpected plot twist appears! The function we want to apply is the very function we&rsquo;re in. Colloquially: take it from the top. }, []);ell our reduce function to start with an empty accumulator (done\`), and wrap it up. \`} else {this resolves our if statement at line 2. If the provided argument isn&rsquo;t an array&hellip; return arr;rn whatever thearr\` is. (Hopefully a cute apple.) This is our base case that breaks us out of recursion. \`}end the else statement. } - end the overall function. And we&rsquo;re done! We&rsquo;ve gone from our 24 line, 4-layers-deep nested for loop solution to a much more concise, 9 line recursive reduce solution. Reduce and recursion can seem a little impenetrable at first, but they&rsquo;re valuable tools that will save you lots of future effort once you grasp them.
And don&rsquo;t worry about Adam, our out-of-work developer. He got so much press after being featured in this article that he opened up his very own AI-managed apple pie factory. He&rsquo;s very happy.
+1 for you if you saw that one coming.
`,url:"https://victoria.dev/archive/understanding-array.prototype.reduce-and-recursion-using-apple-pie/"},"https://victoria.dev/archive/iterating-over-objects-and-arrays-frequent-errors/":{title:"Iterating over objects and arrays: frequent errors",tags:["coding","computing","javascript"],content:`Here&rsquo;s some complaining a quick overview of some code that has confounded me more than once. I&rsquo;m told even very experienced developers encounter these situations regularly, so if you find yourself on your third cup of coffee scratching your head over why your code is doing exactly what you told it to do (and not what you want it to do), maybe this post can help you.
The example code is JavaScript, since that&rsquo;s what I&rsquo;ve been working in lately, but I believe the concepts to be pretty universal.
Quick reference for equivalent statements This&hellip; &hellip;is the same as this i++; i = i + 1; i--; i = i - 1; apples += 5 apples = apples + 5; apples -= 5 apples = apples - 5; apples *= 5 apples = apples * 5; apples /= 5 apples = apples / 5; Quick reference for logical statements This&hellip; &hellip;gives this 3 == '3' true (type converted) 3 === '3' false (type matters; integer is not a string) 3 != '3' false (type converted, 3: 3) 3 !== '3' true (type matters; integer is not a string) || logical &ldquo;or&rdquo;: either side evaluated &amp;&amp; logical &ldquo;and&rdquo;: both sides evaluated Objects Given a breakfast object that looks like this:
var breakfast = { &#39;eggs&#39;: 2, &#39;waffles&#39;: 2, &#39;fruit&#39;: { &#39;blueberries&#39;: 5, &#39;strawberries&#39;: 1, }, &#39;coffee&#39;: 1 } Or like this:
Iterate over object properties We can iterate through each breakfast item using a for loop as follows:
for (item in breakfast) { console.log(&#39;item: &#39;, item); } This produces:
item: eggs item: waffles item: fruit item: coffee Get object property value We can access the value of the property or nested properties (in this example, the number of items) like this:
console.log(&#39;How many waffles? &#39;, breakfast[&#39;waffles&#39;]) console.log(&#39;How many strawberries? &#39;, breakfast[&#39;fruit&#39;][&#39;strawberries&#39;]) Or equivalent syntax:
console.log(&#39;How many waffles? &#39;, breakfast.waffles) console.log(&#39;How many strawberries? &#39;, breakfast.fruit.strawberries) This produces:
How many waffles? 2 How many strawberries? 1 Get object property from the value If instead I want to access the property via the value, for example, to find out which items are served in twos, I can do so by iterating like this:
for (item in breakfast) { if (breakfast[item] == 2) { console.log(&#39;Two of: &#39;, item); } } Which gives us:
Two of: eggs Two of: waffles Alter nested property values Say I want to increase the number of fruits in breakfast, because sugar is bad for me and I like things that are bad for me. I can do that like this:
var fruits = breakfast[&#39;fruit&#39;]; for (f in fruits) { fruits[f] += 1; } console.log(fruits); Which gives us:
{ blueberries: 6, strawberries: 2 } Arrays Given an array of waffles that looks like this:
var wafflesIAte = [ 1, 3, 2, 0, 5, 2, 11 ]; Or like this:
Iterate through array items We can iterate through each item in the array using a for loop:
for (var i = 0; i &lt; wafflesIAte.length; i++) { console.log(&#39;array index: &#39;, i); console.log(&#39;item from array: &#39;, wafflesIAte[i]); } This produces:
array index: 0 item from array: 1 array index: 1 item from array: 3 array index: 2 item from array: 2 array index: 3 item from array: 0 array index: 4 item from array: 5 array index: 5 item from array: 2 array index: 6 item from array: 11 Some things to remember: i in the above context is a placeholder; we could substitute anything we like (x, n, underpants, etc). It simply denotes each instance of the iteration.
i &lt; wafflesIAte.length tells our for loop to continue as long as i is less than the array&rsquo;s length (in this case, 7).
i++ is equivalent to i+1 and means we&rsquo;re incrementing through our array by one each time. We could also use i+2 to proceed with every other item in the array, for example.
Access array item by index We can specify an item in the array using the array index, written as wafflesIAte[i] where i is any index of the array. This gives the item at that location.
Array index always starts with 0, which is accessed with wafflesIAte[0]. Using wafflesIAte[1] gives us the second item in the array, which is &ldquo;3&rdquo;.
Ways to get mixed up over arrays Remember that wafflesIAte.length and the index of the last item in the array are different. The former is 7, the latter is 6.
When incrementing i, remember that [i+1] and [i]+1 are different:
console.log(&#39;[i+1] gives next array index: &#39;, wafflesIAte[0+1]); console.log(&#39;[i]+1 gives index value + 1: &#39;, wafflesIAte[0]+1); Produces:
[i+1] gives next array index: 3 [i]+1 gives index value + 1: 2 Practice makes&hellip; better The more often you code and correct your errors, the better you&rsquo;ll remember it next time!
That&rsquo;s all for now. If you have a correction, best practice, or another common error for me to add, please let me know!
`,url:"https://victoria.dev/archive/iterating-over-objects-and-arrays-frequent-errors/"},"https://victoria.dev/posts/how-to-replace-a-string-with-sed-in-current-and-recursive-subdirectories/":{title:"How to replace a string with sed in current and recursive subdirectories",tags:["terminal"],content:`Meet your new friend sed. This amazingly powerful terminal tool is here to be totally underused for things like finding and replacing strings in files.
Update a string in multiple files with sed You&rsquo;ve got two levels of intensity to choose from:
Non-recursive: Just the files in my current directory. Recursive: This directory and all the subdirectories it contains, with maximum prejudice. Here&rsquo;s how!
Current directory, non-recursive Non-recursive means sed won&rsquo;t change files in any subdirectories of the current folder.
. ├── index.html # Change this file └── blog ├── list.html # Don&#39;t change └── single.html # these files Run this command to search all the files in your current directory and replace a given string. For example, to replace all occurrences of &ldquo;foo&rdquo; with &ldquo;bar&rdquo;:
sed -i -- &#39;s/foo/bar/g&#39; * Here&rsquo;s what each component of the command does:
-i will change the original, and stands for &ldquo;in-place.&rdquo; s is for substitute, so we can find and replace. foo is the string we&rsquo;ll be taking away, bar is the string we&rsquo;ll use instead today. g as in &ldquo;global&rdquo; means &ldquo;all occurrences, please.&rdquo; * denotes all file types. (No more rhymes. What a tease.) You can limit the operation to one file type, such as txt, by using a matching pattern:
sed -i -- &#39;s/foo/bar/g&#39; *.txt Current directory and subdirectories, recursive You can supplement sed with find to expand your scope to all of the current folder&rsquo;s subdirectories. This will include any hidden files.
find . -type f -exec sed -i &#39;s/foo/bar/g&#39; {} + To ignore hidden files (such as .git) you can pass the negation modifier -not -path '*/\\.*', like this:
find . -type f -not -path &#39;*/\\.*&#39; -exec sed -i &#39;s/foo/bar/g&#39; {} + This will exclude any file that has the string /. in its path.
You can also limit this operation to file names that end in a certain extension, like Markdown:
find . -type f -name &#34;*.md&#34; -exec sed -i &#39;s/foo/bar/g&#39; {} + Replacing URLs: change the separator If you want to update a URL, the / separator in your strings will need escaping. It ends up looking like this&hellip;
find . -type f -exec sed -i \\ &#39;s/https:\\/\\/www.oldurl.com\\/blog/https:\\/\\/www.newurl.com\\/blog/g&#39; {} + You can avoid confusion and mistakes by changing the separator to any non-conflicting character. The character that follows the s will be treated as the separator. In this case, using a , or _ would do. This doesn&rsquo;t require escaping and is much more readable:
find . -type f -exec sed -i \\ &#39;s_https://www.oldurl.com/blog_https://www.newurl.com/blog_g&#39; {} + I write about time-saving terminal tricks and how to improve productivity as a software developer. You can get these tips right in your inbox by signing up below!
`,url:"https://victoria.dev/posts/how-to-replace-a-string-with-sed-in-current-and-recursive-subdirectories/"},"https://victoria.dev/archive/things-you-need-to-know-about-becoming-a-data-scientist/":{title:"Things you need to know about becoming a Data Scientist",tags:["data","life"],content:`I recently attended a panel discussion hosted by General Assembly in Singapore entitled, &ldquo;So you want to be a Data Scientist/Analyst&rdquo;. The panel featured professionals in different stages of their careers and offered a wealth of information to an audience of hopefuls, including tips on how to land a job as a data scientist, and stories debunking myths that color this field.
The panelists Misrab Faizullah-Khan - VP of Data Science, GO_JEK Anthony Ta - Data Scientist, Tech in Asia Leow Guo Jun - Data Scientist, GO_JEK Gabriel Jiang - Data Scientist Adam Drake - Chief Data Officer, Atazzo Here&rsquo;s a rundown of the major points discussed, paraphrased for brevity.
What&rsquo;s a day-in-the-life like We&rsquo;re mostly &ldquo;data janitors.&rdquo; A large part of working with data begins with and consists of data sanitation. Without quality data, you won&rsquo;t get accurate results. Understanding how data should be sanitized largely encompasses skills that aren&rsquo;t directly related to data analytics. To fully understand the problem you&rsquo;re hoping to solve, you need to talk with the people involved. It&rsquo;s important that everyone understands all the elements of a project, and exactly what those elements are being called. &ldquo;Sales,&rdquo; as an example, may be calculated differently depending on who you&rsquo;re talking to.
What&rsquo;s a data &ldquo;scientist&rdquo; vs. data &ldquo;analyst&rdquo; It largely depends on the company you work for. &ldquo;Data [insert modifier]&rdquo; is only a recent distinction for a job field that has historically been called &ldquo;Business Analytics.&rdquo; In a smaller company, as with any other position, one person may handle a variety of data-related tasks under the title of &ldquo;Data Scientist.&rdquo; In a larger company with more staff and finer grain specialization, you may have a &ldquo;Data Analyst&rdquo; that handles less technical aspects, and a &ldquo;Data Scientist&rdquo; whose work is very technical and involves quantitative learning or machine learning.
The field of data science/analytics is fresh enough that standard definitions for job titles really haven&rsquo;t been agreed upon yet. When considering a position, focus on the company rather than the title.
Should I join a startup or large company There&rsquo;s no wrong answer. Being aware of your own working style and preferences will help guide your decision.
Startups generally offer more freedom and less micromanaging. This also means that you&rsquo;ll necessarily receive less guidance, and will need to be able to figure stuff out, learn, and make progress under your own power.
In a big company, you&rsquo;re likely to experience more structure, and be expected to follow very clearly defined pre-existing processes. Your job scope will likely be more focused than it would be at a startup. You&rsquo;ll experience less freedom in general, but also more certainty in what&rsquo;s expected of you.
In the end, especially at the beginning of your career, don&rsquo;t put too much stock in choosing one or the other. If you like the company, big or small, give it a try. If you&rsquo;re not happy there after a few months, then try another one. No career decision is ever permanent.
It&rsquo;s also worthwhile to note that even if you find a company you like the first time around, it&rsquo;s in your best interest to change companies after one or two years. The majority of the salary raises you&rsquo;ll earn in your lifetime will occur in the first ten years of your career. Say you&rsquo;re hired by Company A as a junior data scientist for two years - after two years, you&rsquo;re no longer a junior. You can now earn, say, a 30% higher salary in a data scientist position, but it&rsquo;s unlikely that Company A will give you a 30% raise after two years. At that point it&rsquo;s time to find Company B and put a few more years of experience on your resume, then probably change companies again. You don&rsquo;t earn the big bucks sticking with one company for decades - you&rsquo;ll always be the junior developer.
What do you look for when hiring a candidate Overall, the most important skills for a data science candidate are soft skills. Curiosity, tenacity, and good communication skills are vital. Persistence, especially when it comes to adapting to a quickly changing industry, is important. The most promising candidates are passionate enough about the field to be learning everything they can, even outside of their work scope. Hard skills like coding and algorithms can be taught - it&rsquo;s the soft skills that set good candidates apart.
Hacking skills are also vital. This doesn&rsquo;t necessarily mean you can write code. Someone who has a grasp of overall concepts, knows algorithms, and has curiosity enough to continuously learn is going to go farther than someone who can just write code. It takes creativity to build hacking skills on top of being familiar with the basic navigation points. Having the ability to come up with solutions that use available tools in new ways - that&rsquo;s hacking skill.
Design thinking is another important asset. Being able to understand how systems integrate on both technical and business levels is very valuable. If you&rsquo;re able to see the big picture, you&rsquo;re more likely to find different ways to accomplish the overall objective.
You might think that seeing buzzwords on resumes makes you look more attractive as a candidate - more often, it stands out as a red flag. Putting &ldquo;advanced machine learning&rdquo; on your CV and then demonstrating that you don&rsquo;t know basic algorithms doesn&rsquo;t look good. It&rsquo;s your projects and your interests outside of the job you&rsquo;re applying for that say the most about you. Popular topics in this industry change fast - you&rsquo;re better off having a solid grasp of basic fundamentals as well as a broad array of experience than name-dropping whatever&rsquo;s trending.
Is there a future for humans in the data science field? When will the machines replace us This isn&rsquo;t a question unique to data science, and many historical examples already exist. Financial investment is a good example - where you used to have a human do calculations and make predictions, computers now do a lot of that automatically, making decisions about risk and possible payoff every day.
Where humans won&rsquo;t be replaced, just as in other industries that have embraced automation, is in the human element. You&rsquo;ll still need people to handle communication, be creative, be curious, make interpretations and understand problems&hellip; all those things are fundamentally human aspects of enterprise.
Ultimately, machines and more automation will make human work less of a grind. By automating the mundane stuff, like data sanitization for example, human minds are freed up to develop more interesting things.
What are the future applications for data-driven automation Legal is a good next candidate for automation. There&rsquo;s a lot there that can be handled by programs using data to assess risk.
Medicine is another field ripe for advances through data. Radiologists, your days are numbered: image detection is coming for you. The whole field of diagnostics is about to drastically change.
A particularly interesting recent application for data science is in language translation. By looking at similarities in sentence structure and colloquial speech across different languages, we&rsquo;re able to sort similar words based on the &ldquo;space&rdquo; they occupy within the language structure.
Insurance - the original data science industry - already is and will continue to become very automated. With increased ability to use data to assess risk, we&rsquo;re beginning to see new creative insurance products being introduced. E-commerce companies can now buy insurance on the risk a customer will return a product - hard to do without the accessibility of data that we have today.
How do I push data-driven decisions and get my boss to agree with me It&rsquo;s a loaded question. The bottom line is that it depends on the company&rsquo;s data culture and decision path. We&rsquo;ve experienced working for management who say, &ldquo;We&rsquo;ve already made the decisions, we just need the data to prove it.&rdquo; Obviously, that&rsquo;s a tough position to work from.
Generally, ask yourself, &ldquo;Am I making my boss look good?&rdquo; You might hear that and think, &ldquo;Why would I let my boss get all the credit?&rdquo; - but who cares? Let them take the credit. If you&rsquo;re producing good work, you&rsquo;re making your team look good. If you make your team look good, you&rsquo;re indispensible to your team and your boss. People who are indispensible are listened to.
What&rsquo;s your best advice for a budding data scientist Don&rsquo;t be too keen to define yourself too quickly. If you narrow your focus too much, especially when you&rsquo;re learning, you can get stuck in a situation of having become an expert in &ldquo;Technology A, version 3&rdquo; when companies are looking to hire for experts in version 4. It happens.
A broad understanding of fundamentals will be far more valuable to you on the whole. Maybe you start out writing code, and decide you don&rsquo;t like it, but discover that you&rsquo;re really good at designing big picture stuff and leading teams, and you end up as a technical lead. It could even vary depending on the company you work for - so stay flexible.
Your best bet is to follow what you&rsquo;re passionate about, and try to understand a wide range of overall concepts. Spend the majority of your efforts learning things that are timeless, like the base technologies under hot-topic items like TensorFlow. Arm yourself with a broad understanding of the terrain, different companies, and the products that are out there.
If you focus on learning code specifically, learning one language well makes it easier to learn others. Make sure you understand the basics.
TL;dr it Adam: Talk more and don&rsquo;t give up. Anthony: [Be] courageous, and hands-on. Gabriel: Be creative. Guo Jun: It&rsquo;s worth the pain. Misrab: Evaluate yourself and maintain a feedback loop. `,url:"https://victoria.dev/archive/things-you-need-to-know-about-becoming-a-data-scientist/"},"https://victoria.dev/archive/how-i-created-custom-desktop-notifications-using-terminal-and-cron/":{title:"How I created custom desktop notifications using terminal and cron",tags:["terminal","linux"],content:`In my last post I talked about moving from Windows 10 to running i3 on Linux, built up from Debian Base System. Among other things, this change has taught me about the benefits of using basic tools and running a minimal, lightweight system. You can achieve a lot of functionality with just command line tools and simple utilities. One example I&rsquo;d like to illustrate in this post is setting up desktop notifications.
I use dunst for desktop notifications. It&rsquo;s a simple, lightweight tool that is easy to configure, doesn&rsquo;t have many dependencies, and can be used across various distributions.
Battery status/low battery notification I was looking for a simple, versatile set up to create notifications for my battery status without having to rely on separate, standalone GUI apps or services. In my search I came across a simple one-line cron task that seemed to be the perfect fit. I adapted it to my purpose and it looks like this:
# m h dom mon dow command */5 * * * * acpi --battery | awk -F, &#39;/Discharging/ { if (int($2) &lt; 20) print }&#39; | xargs -ri env DISPLAY=:0 notify-send -u critical -i &#34;/usr/share/icons/Paper/16x16/status/xfce-battery-critical.png&#34; -t 3000 &#34;{}\\nBattery low!&#34; Psst&hellip; here&rsquo;s a great tool for formatting your crontab times.
There&rsquo;s a lot going on here, so let&rsquo;s break it down: */5 * * * * Every five minutes, do the following.
acpi --battery Execute acpi and show battery information, which on its own returns something akin to: Battery 0: Discharging, 65%, 03:01:27 remaining
Pretty straightforward so far. At any point you could input acpi --battery in a terminal and receive the status output. Today&rsquo;s post, however, is about receiving this information passively in a desktop notification. So, moving on:
| awk -F, '/Discharging/ { if (int($2) &lt; 20) print }' Pipe (|) the result of the previous command to awk. (If you don&rsquo;t know what pipe does, here&rsquo;s an answer from superuser.com that explains it pretty well, I think.) awk can do a lot of things, but in this case, we&rsquo;re using it to examine the status of our battery. Let&rsquo;s zoom in on the awk command:
awk -F, '/Discharging/ { if (int($2) &lt; 20) print }' Basically, we&rsquo;re saying, &ldquo;Hey, awk, look at that input you just got and try to find the word &ldquo;discharging,&rdquo; then look to see if the number after the first comma is less than 20. If so, print the whole input.&rdquo;
| xargs -ri Pipe the result of the previous command to xargs, which takes it as its input and does more stuff with it. -ri is equivalent to -r (run the next command only if it receives arguments) and -i (look for &ldquo;{}&rdquo; and replace it with the input). So in this example, xargs serves as our gatekeeper and messenger for the next command.
env DISPLAY=:0 Run the following utility in the specified display, in this case, the first display of the local machine.
notify-send -u critical -i &quot;/usr/share/icons/Paper/16x16/status/xfce-battery-critical.png&quot; -t 3000 &quot;{}\\nLow battery!&quot; Shows a desktop notification with -u critical (critical urgency), -i (the specified icon), -t 3000 (display time/expires after 3000 milliseconds), and finally {} (the output of awk, replaced by xargs).
Not bad for a one-liner! I made a few modifications for different states of my battery. Here they all are in my crontab:
# m h dom mon dow command */5 * * * * acpi --battery | awk -F, &#39;/Discharging/ { if ( (int($2) &lt; 30) &amp;&amp; (int($2) &gt; 15) ) print }&#39; | xargs -ri env DISPLAY=:0 notify-send -a &#34;Battery status&#34; -u normal -i &#34;/usr/share/icons/Paper/16x16/status/xfce-battery-low.png&#34; -t 3000 &#34;{}\\nBattery low!&#34; */5 * * * * acpi --battery | awk -F, &#39;/Discharging/ { if (int($2) &lt; 15) print }&#39; | xargs -ri env DISPLAY=:0 notify-send -a &#34;Battery status&#34; -u critical -i &#34;/usr/share/icons/Paper/16x16/status/xfce-battery-critical.png&#34; -t 3000 &#34;{}\\nSeriously, plug me in.&#34; */60 * * * * acpi --battery | awk -F, &#39;/Discharging/ { if (int($2) &gt; 30) print }&#39; | xargs -ri env DISPLAY=:0 notify-send -a &#34;Battery status&#34; -u normal -i &#34;/usr/share/icons/Paper/16x16/status/xfce-battery-ok.png&#34; &#34;{}&#34; */60 * * * * acpi --battery | awk -F, &#39;/Charging/ { print }&#39; | xargs -ri env DISPLAY=:0 notify-send -a &#34;Battery status&#34; -u normal -i &#34;/usr/share/icons/Paper/16x16/status/xfce-battery-ok-charging.png&#34; &#34;{}&#34; */60 * * * * acpi --battery | awk -F, &#39;/Charging/ { if (int($2) == 100) print }&#39; | xargs -ri env DISPLAY=:0 notify-send -a &#34;Battery status&#34; -u normal -i &#34;/usr/share/icons/Paper/16x16/status/xfce-battery-full-charging.png&#34; &#34;Fully charged.&#34; By the way, you can open your crontab in the editor of your choice by accessing it as root from the /var/spool/cron/crontabs/ directory. It&rsquo;s generally best practice however to make changes to your crontab with the command crontab -e.
You can see that each notification makes use of the {} placeholder that tells xargs to put its input there - except for the last one. This is interesting because in this case, we&rsquo;re only using xargs -ri as a kind of switch to present the notification. The actual information that was the input for xargs is not needed in the output in order to create a notification.
Additional notifications with command line tools With cron and just a few combinations of simple command line tools, you can create interesting and useful notifications. Consider the following:
Periodically check your dhcp address */60 * * * * journalctl | awk -F: &#39;/dhcp/ &amp;&amp; /address/ { print $5 }&#39; | tail -1 | xargs -ri env DISPLAY=:0 notify-send -a &#34;dhcp address&#34; -u normal &#34;{}&#34; Which does the following: */60 * * * * Every 60 minutes.
journalctl Take the contents of your system log.
| tail -1'/dhcp/ &amp;&amp; /address/ { print $5 }' Find logs containing both &ldquo;dhcp&rdquo; and &ldquo;address&rdquo; and output the 5th portion as separated by &ldquo;:&rdquo; (the time field counts).
| tail -1 Take the last line of the output.
| xargs -ri env DISPLAY=:0 notify-send -a &quot;dhcp address&quot; -u normal &quot;{}&quot; Create the desktop notification including the output.
Periodically display the time and date */60 * * * * timedatectl status | awk -F\\n &#39;/Local time/ { print }&#39; | xargs -ri env DISPLAY=:0 notify-send -a &#34;Current Time&#34; -u normal &#34;{}&#34; System log activity You can also search your system logs (try journalctl) for any number of things using awk, enabling you to get periodic notifications of virtually any logged events.
Experiment As with all things, you are only limited by your imagination! I hope this post has given you some idea about the endless possibilities of these simple utilities. Thanks for reading!
`,url:"https://victoria.dev/archive/how-i-created-custom-desktop-notifications-using-terminal-and-cron/"},"https://victoria.dev/archive/how-i-ditched-wordpress-and-set-up-my-custom-domain-https-site-for-almost-free/":{title:"How I ditched WordPress and set up my custom domain HTTPS site for (almost) free",tags:["websites"],content:`I got annoyed with WordPress.com. While using the service has its pros (like https and a mobile responsive website, and being very visual and beginner-friendly) it&rsquo;s limiting. For someone who&rsquo;s comfortable enough to be tweaking CSS but who&rsquo;s not interested in creating their own theme (or paying upwards of $50 for one), I felt I wasn&rsquo;t really the type of consumer WordPress.com was suited to.
To start with, if you want to remove WordPress advertising and use a custom domain name, it&rsquo;s a minimum of $3 per month. If, like me, the free themes provided aren&rsquo;t just what you&rsquo;re looking for, you&rsquo;re stuck with two choices: buy a theme for $50+, or pay $8.25 per month to do some css customization. I don&rsquo;t know about you, but I feel like there should be a hack for this.
How I ditched WordPress and got everything I wanted for free Okay, almost free. You still have to pay at least $0.99 for a domain name.
For those of you technical enough to skip reading a long post, the recipe is this:
Buy a custom domain via this Namecheap affiliate link (Thanks for your support! 😊) Install Hugo, my favorite static site generator Host with GitHub Pages Put your custom domain to work with GitHub Pages Use Cloudflare&rsquo;s free plan Enforce HTTPS for GitHub Pages Let&rsquo;s do the nitty gritty:
1. Buy a custom domain This one&rsquo;s pretty simple. Head on over to Namecheap , Gandi, or if you&rsquo;re rolling in dough, GoDaddy. Find your perfect web address and buy it up.
If it&rsquo;s a personal domain like yourname.com, it&rsquo;s a pretty good idea to pay upfront for five years or even ten years, if you&rsquo;ve got the cash. It&rsquo;ll save you the trouble of remembering to renew, allow you to build your personal brand, and prevent someone else from buying up your URL.
If you&rsquo;re just trying out an idea, you can go with a one-year $0.99 experiment. Namecheap also gives you WhoisGuard (domain registration privacy) free for one year.
2. Install Hugo I&rsquo;m a big fan of Hugo so far. Admittedly, those who feel more comfortable with a visual, WYSIWYG editor may feel like a fish out of water at first. As long as you&rsquo;re not afraid of using command line, though, using Hugo is pretty straightforward. The fact that I have access to all my code is my favorite part. It&rsquo;s only as simple or complicated as I want it to be.
Hugo is open source and free. They&rsquo;ve got great documentation, and following their Quickstart guide line-by-line will get you set up with your new site in minutes.
If you&rsquo;re not used to the idea of your site existing as files and folders, the basic premise is this: Hugo, along with the themes available, helps you to create all the pages and files that your site needs to run.
Blog posts can be written in Markdown and saved in your /content/blog/ folder; preferences for your site and theme can be set in the config.toml file. After that, generating all your site&rsquo;s pages is as quick and easy as typing the command hugo --theme=&lt;your theme&gt;. You&rsquo;ll be able to see a live version of your site in your browser as you&rsquo;re editing it (go to http://localhost:1313/ in your browser, as described in Step 5) so you&rsquo;re not flying blind.
3. Host with GitHub Pages If you read to Step 12 of Hugo&rsquo;s Quickstart Guide, you&rsquo;ll see that they even provided instructions for hosting your files on GitHub pages. If you&rsquo;re new to Git, you&rsquo;ll first need to sign up at GitHub and then set up Git. GitHub is a very friendly resource, and you can find a multitude of code examples and guides in connection with it. The Hello World Guide will take you through all you need to know to use GitHub.com.
Once you&rsquo;re comfortable with the way GitHub works generally, setting up a site by following the guide on GitHub Pages is no big deal. If you followed the Hugo Quickstart Guide up to Step 11, you&rsquo;ll want to jump to Step 12 after creating the repository on GitHub.
In case it&rsquo;s not clear, once you set up your new repository on GitHub called yourusername.github.io, grab the HTTPS link at the top. From there it&rsquo;s just a few simple commands to create the git repository for your site and push it to your new web address:
## from yoursite/public folder: $ git init $ git remote add origin &lt;paste that https url here!&gt; $ git add --all $ git commit -m &#34;Initial commit.&#34; $ git push origin master Have a little celebration - your site is already up at https://yourusername.github.io! Now for the pizza-de-resilience: the custom domain.
4. Point your custom domain to GitHub Pages To set up your site at apex (meaning yourname.com will replace yourusername.github.io), there&rsquo;s just four steps:
Add your domain to your GitHub Pages site repository In your domain registrar&rsquo;s DNS settings, create A records pointing to GitHub&rsquo;s IP addresses In your domain registrar&rsquo;s DNS settings, create a CNAME record pointing to yourusername.github.io Make sure there&rsquo;s a CNAME file in the root directory of your GitHub repository containing yourname.com (your custom domain) 5. Enforce HTTPS for GitHub Pages GitHub Pages supports HTTPS through a partnership with Let&rsquo;s Encrypt! This greatly simplifies the process of serving your site securely. Just look for this clever checkbox in the Settings of your site&rsquo;s GitHub repository.
Why do I need HTTPS anyway? For one, it&rsquo;ll give your site a little boost on Google. More importantly, it&rsquo;s fundamental to your website security. You can learn more about HTTPS and TLS in this post.
That&rsquo;s pretty much it! If you don&rsquo;t see changes right away, give all your services a lunch hour or so to propogate. Soon your site will be up and running at https://yourname.com.
Thanks for reading! If you found this post helpful, there&rsquo;s a lot more where this came from. You can subscribe to victoria.dev to see new posts first!
`,url:"https://victoria.dev/archive/how-i-ditched-wordpress-and-set-up-my-custom-domain-https-site-for-almost-free/"},"https://victoria.dev/archive/iteration-in-python-for-list-and-map/":{title:"Iteration in Python: for, list, and map",tags:["python","coding"],content:`Iteration in Python can be a little hard to understand. Subtle differences in terminology like iteration, iterator, iterating, and iterable aren&rsquo;t the most beginner-friendly.
When tackling new concepts, I find concrete examples to be most useful. I&rsquo;ll share some in this post and discuss appropriate situations for each. (Pun intended.)
For loop First, in pseudocode:
for iterating_variable in iterable: statement(s) I find for loops to be the most readable way to iterate in Python. This is especially nice when you&rsquo;re writing code that someone else needs to read and understand, which is always.
An iterating_variable, loosely speaking, is anything you could put in a group. For example: a letter in a string, an item from a list, or an integer in a range of integers.
An iterable houses the things you iterate on. This can also take different forms: a string with multiple characters, a range of numbers, a list, and so on.
A statement or multiple statements indicates doing something to the iterating variable. This could be anything from mathematical expressions to simply printing a result.
Here are a couple simple examples that print each iterating_variable of an iterable:
for letter in &#34;Hello world&#34;: print(letter) for i in range(10): print(i) breakfast_menu = [&#34;toast&#34;, &#34;eggs&#34;, &#34;waffles&#34;, &#34;coffee&#34;] for choice in breakfast_menu: print(choice) You can even use a for loop in a more compact situation, such as this one-liner:
breakfast_buffet = &#34; &#34;.join(str(item) for item in breakfast_menu) The downside to for loops is that they can be a bit verbose, depending on how much you&rsquo;re trying to achieve. Still, for anyone hoping to make their Python code as easily understood as possible, for loops are the most straightforward choice.
List comprehensions A pseudocode example:
new_list = [statement(s) for iterating_variable in iterable] List comprehensions are a concise and elegant way to create a new list by iterating on variables. Once you have a grasp of how they work, you can perform efficient iterations with very little code.
List comprehensions will always return a list, which may or may not be appropriate for your situation.
For example, you could use a list comprehension to quickly calculate and print tip percentage on a few bar tabs at once:
tabs = [23.60, 42.10, 17.50] tabs_incl_tip = [round(tab*1.15, 2) for tab in tabs] print(tabs_incl_tip) &gt;&gt;&gt; [27.14, 48.41, 20.12] In one concise line, we&rsquo;ve taken each tab amount, added a 15% tip, rounded it to the nearest cent, and made a new list of the tabs plus the tip values.
List comprehensions can be an elegant tool if output to a list is useful to you. Be advised that the more statements you add, the more complicated your list comprehension begins to look, especially once you get into nested list comprehensions. If your code isn&rsquo;t well annotated, it may become difficult for another reader to figure out.
Map How to map, in pseudocode:
map(statement, iterable) Map is pretty compact, for better or worse. It can be harder to read and understand, especially if your line of code has a lot of parentheses.
In terms of efficiency for character count, map is hard to beat. It applies your statement to every instance of your iterable and returns an iterator.
Here&rsquo;s an example casting each element of input() (the iterable) from string representation to integer representation. Since map returns an iterator, you also cast the result to a list representation.
values = list(map(int, input().split())) weights = list(map(int, input().split())) It&rsquo;s worth noting that you can also use for loops, list comprehension, and map all together:
output = sum([x[0] * x[1] for x in zip(values, weights)]) / sum(weights) print(round(output, 1)) Your iteration toolbox Each of these methods of iteration in Python have a special place in the code I write every day. I hope these examples have helped you see how to use for loops, list comprehensions, and map in your own Python code!
If you like this post, there&rsquo;s a lot more where that came from! I write about efficient programming for coders and for leading technical teams. Check out the posts below!
`,url:"https://victoria.dev/archive/iteration-in-python-for-list-and-map/"},"https://victoria.dev/archive/":{title:"Archives",tags:[],content:"",url:"https://victoria.dev/archive/"},"https://victoria.dev/posts/":{title:"Victoria Drake's Blog",tags:[],content:"",url:"https://victoria.dev/posts/"},"https://victoria.dev/404/":{title:"404",tags:[],content:`Why wasn&rsquo;t this page found? We asked some leading software developers.
Agile Development
The page was in the backlog, but we prioritized something else for this sprint.
Waterfall Model
The page wasn&rsquo;t found because it wasn&rsquo;t scheduled in the current phase. Check back after we finish design, implementation, and testing.
Continuous Integration
This page is probably broken because the last merge went straight to production without proper testing.
DevOps
Our infrastructure automated the deployment of this page, but it seems like the server didn&rsquo;t get the memo.
Object-Oriented Programming
The page object was never instantiated, so it doesn&rsquo;t actually exist.
Functional Programming
The function to generate this page was pure, but the side effects of showing it were too risky.
Microservices
One of the 18 services needed to render this page failed. We&rsquo;re not sure which one, but we&rsquo;re working on it.
REST API
The endpoint you&rsquo;re trying to reach returned a 404 because the resource was never created.
Machine Learning
Our model predicted you&rsquo;d want this page, but the accuracy was only 50%. Sorry, wrong prediction.
Scrum
The team discussed this page during the daily stand-up, but then we got sidetracked by a blocker and forgot about it.
Version Control
This page exists, but it&rsquo;s stuck in a branch that was never merged. It’s living its best life in the forgotten recesses of our repo.
Technical Debt
This page might have existed once, but it was removed to pay down some technical debt. We didn’t think anyone would notice.
Unit Testing
We tested every other page, except this one. Guess we missed a case.
Full Stack Developer
This page was halfway done, but then the front-end and back-end teams couldn&rsquo;t agree on the API format, so it never made it.
Legacy Code
This page was written in a language no one on the team knows how to debug, so we just left it alone.
UX Design
We redesigned the website, and now no one can find anything—including this page. But hey, doesn&rsquo;t it look sleek?
Code Review
This page failed the review process. The code was spaghetti, so we just scrapped it.
(I help software teams fix errors that lead to things like this. If you let me know about this one, I&rsquo;ll fix that, too.)
`,url:"https://victoria.dev/404/"},"https://victoria.dev/site/":{title:"About this site",tags:[],content:`Victoria.dev is wholly created and owned by me, Victoria Drake. I research, write, illustrate, design, code, and ship everything you’ll see here.
Static site generator I use a static site generator, Hugo, to build the site, and GitHub Actions to continually deploy to GitHub Pages.
It&rsquo;s all open source, so feel free to see for yourself!
Theme The theme based on my Hugo theme, Quint.
Illustrations I draw the all the illustrations and comics you&rsquo;ll find in my articles using my iPad.
Searching I use Lunr to implement the search feature you see on my blog page. Here&rsquo;s how.
My development tools I run a GitHub Action I wrote, link-snitch, to ensure all the links on my 144 pages are working. It uses Hydra, my multithreaded Python site-crawling link checker.
Besides creating new posts, a static site like this one is low-maintenance. Most of my development needs are met with a self-documenting Makefile.
There may or may not be secret pages and easter eggs.
`,url:"https://victoria.dev/site/"},"https://victoria.dev/contact/":{title:"Hello! Your turn.",tags:[],content:`Email me You can say hello@victoria.dev.
Please note that I do not accept guest blog posts or requests for placing links in posts.
Find me in the &lsquo;verse x.com/victoriadotdev github.com/victoriadrake medium.com/@victoriadotdev dev.to/victoria You can also learn how I built this site or sign up for my newsletter.
`,url:"https://victoria.dev/contact/"},"https://victoria.dev/about/":{title:"I'm Victoria Drake.",tags:[],content:`Here&rsquo;s what I do.
Leadership I eagerly lead tech teams to deliver great software with a focus on security and maintainability. With my background as a principal software engineer, I&rsquo;m able to find improvements in every area of building excellent software, from process to production.
Software and Security I serve as a core maintainer and co-author of the Open Web Application Security Project (OWASP) Web Security Testing Guide.
In my free time, I love building software and giving it away. You can find out more about my open-source work at github.com/victoriadrake.
The Next Generation I&rsquo;m on a mission to help prepare millions of new engineers to take on the next era of technology.
I earned the annual Top Contributor award from freeCodeCamp for three consecutive years. I was also recognized two years in a row as a Distinguished Author on the DEV.to developer platform. And I&rsquo;ve written for multiple business and software engineering publications across the web as well as here on my blog.
`,url:"https://victoria.dev/about/"},"https://victoria.dev/search/":{title:"Search Results",tags:[],content:"",url:"https://victoria.dev/search/"},"https://victoria.dev/series/security-for-developers/":{title:"Security for Developers",tags:[],content:`I&rsquo;m often asked if I have resources for software developers who want to help build secure applications. Here&rsquo;s a list of my articles on the topic!
`,url:"https://victoria.dev/series/security-for-developers/"},"https://victoria.dev/series/":{title:"Series",tags:[],content:"",url:"https://victoria.dev/series/"},"https://victoria.dev/about/cv/":{title:"Victoria Drake",tags:[],content:`Engineering Director and Principal Software Engineer As a seasoned engineering leader, I guide teams to produce secure, industry-leading software for both government and private sector entities. My unique experience equips me to navigate the full spectrum of technology product development, from roadmap conception to final deployment and operations.
I&rsquo;ve excelled as an engineering director, principal software engineer, and mentor. I connect executive decisions with hands-on technical experience. I enjoy sharing my expertise at many levels, whether it&rsquo;s advising C-suite executives on technology strategy or software and security best practices, unblocking development teams, or deep-diving the codebase with developers to tackle complex architectural and technical challenges. I particularly thrive in leading distributed teams and bringing together complementary skills.
I&rsquo;m a co-author of the OWASP Web Security Testing Guide and advocate for cybersecurity training across organizations. Along with contributions to various business and engineering publications, my track record reflects my commitment to understanding the technology and security terrain and ensuring teams can traverse it with confidence.
You can read more about me or send me an email.
Download a PDF version of my resume.
Principal Software Engineer September 2021 — September 2023
Principal Software Engineer for Sophos Factory, a modern DevSecOps automation pipeline builder.
Enhanced platform scalability of a newly-acquired startup, aligning it with the broader business needs of Sophos through strategic technical and operational enhancements Collaborated across teams to seamlessly integrate Sophos Factory with broader Sophos offerings and product strategy Established standards and workflow processes to scale up the delivery of new features Ensured roadmap planning and comprehensive feature requirements aligned with the broader company strategy and roadmap Director Of Engineering March 2020 — September 2021
Directed software development at ZibaSec, a modern cybersecurity awareness training platform that uses realistic phishing simulations to create lasting behavior change and cyber risk reduction.
Led the engineering group to design, implement, and secure a serverless cloud infrastructure while greatly improving application performance and achieving FedRAMP Authorization Achieved 4.5x speedup in serverless application performance using multiple infrastructure components and distributed computing techniques Created and implemented strategies for increasing knowledge transfer and organizational scalability in a growing, remote-first company Reduced onboarding time for new engineers by 75% by leading an overhaul of onboarding processes and documentation Scaled the engineering team size by 3x through improved processes for recruiting, interviewing, and hiring Related:
ZibaSec &amp; GitHub: Would you volunteer your company for a cyber attack? ZibaSec’s PhishTACO Platform achieves FedRAMP Authorization OWASP Core Contributor Team August 2019 — present
Co-author and core maintainer for the OWASP WSTG. The Open Web Application Security Project (OWASP) Web Security Testing Guide (WSTG) is the foremost open source resource for testing web application security.
Built and established modern CI/CD and automation practices Serve as technical editor for submissions from contributors Related:
OWASP Web Security Testing Guide v4.2 released freeCodeCamp Coding Mentor, Contributor 2017 — 2021
Recognized as a Top Contributor for three consecutive years at freeCodeCamp, a global 501(c)(3) non-profit organization that helps millions of people worldwide learn how to code.
Served as organizer for the 2017 inaugural freeCodeConference in Toronto Provided mentorship, code review, and career guidance to motivated technologists worldwide Senior Software Developer, Consultant 2016 — 2021
As a senior technology leader with a background in cybersecurity and full-stack software development, I provided executive leadership insights and technical guidance on product and process improvement.
Focus areas:
Leader mentorship and development Increasing development velocity in engineering teams Application infrastructure and code efficiency, speedup, and cost savings Products and case studies:
ApplyByAPI.com, SaaS that improves the technical hiring process by filtering candidates at the top of the funnel, and reduces human hours spent on screening Modern e-commerce solutions for legacy industries, such as large-scale commercial building construction materials Product design and product management for applications including an audio virtual reality application Related:
GitHub Action Hero: Victoria Drake Head of Product 2012 — 2015
Championed digital transformation at a niche manufacturing company, resulting in a high-end e-commerce experience.
Advised on strategic product-market fit changes Planned and executed long-term digital strategy Migrated business processes to use modern e-commerce solutions Overhauled legacy offline inventory management to a modern software solution Managed recruiting activities to appoint and train team leads Education Master of Science, Computer Science - Georgia Institute of Technology
Contact hello@victoria.dev
`,url:"https://victoria.dev/about/cv/"},"https://victoria.dev/bio/":{title:"Victoria Drake",tags:[],content:`Victoria Drake is a software engineering leader, award-winning technical author, and open source community mentor. She is a co-author for the OWASP Web Security Testing Guide and has held roles as Director of Engineering and Principal Software Engineer.
about • contact • github • x / twitter
`,url:"https://victoria.dev/bio/"},"https://victoria.dev/bookshelf/":{title:"Victoria's bookshelf",tags:[],content:`Books that have measurably contributed to my skill stack are shared here.
Required reading for technology leaders Non-coding books for coders Required reading for technology leaders Extreme Ownership: How U.S. Navy SEALs Lead and Win Jocko Willink, Leif Babin
Foundational mindset and principles of leadership. How taking ownership of your work, project, and yourself helps to make you a better leader.
The Art of Action: How Leaders Close the Gaps between Plans, Actions and Results Stephen Bungay
To understand how empowering your team to make decisions without you provides a significant competitive edge. An extremely worthwhile leadership curriculum. I&#39;d get the hardcover.
Thinking, Fast and Slow Daniel Kahneman
To help gain a foundational understanding of how people think and react. Largely considered transformative in the field of cognitive psychology.
Non-coding books for coders A lot of non-technical knowledge gems can contribute to your programming skills! Here are the most helpful ones I&rsquo;ve read myself.
The Power of Habit Charles Duhigg
To understand how to use your embedded superpowers to trick yourself into getting good at anything, including coding.
Thanks for the Feedback: The Science and Art of Receiving Feedback Well Douglas Stone, Sheila Heen
How to responsibly review other people&#39;s work, and how to gratefully receive reviews of your own work.
The War of Art Steven Pressfield
A guide to building systems that overcome inner resistance and support creative work, like programming.
Grit: The Power of Passion and Perseverance Angela Duckworth
To understand that struggling is an important part of the process, and the how to cultivate a mindset for dealing with it productively.
This page helps to support my work and contains Amazon affiliate links. If you happen to like a recommendation and make a purchase, I'll get a small thank-you commission at no extra cost to you! `,url:"https://victoria.dev/bookshelf/"}}</script><script src=/js/lunr.min.js></script><script src=/js/search.js></script></footer></body></html>